[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cloud-Py ",
    "section": "",
    "text": "Cloud-Py \nWelcome to TileDB’s Cloud Python client. This client offers programmatic access to TileDB Cloud.\n\n\nInstallation\nYou can install the Cloud-Py SDK client as follows:\npip install tiledb-cloud\nWhile the preferred method of running code samples and notebooks in this section is directly within TileDB Cloud (as all dependencies are installed for you), you can run most of the code samples and notebooks in this section locally. To run these code samples and notebooks locally, install the following dependencies:\npip install ipykernel jupyterlab graphviz\npip install tiledb-cloud[all]\nFor Life Science capabilities:\npip install tiledb-cloud[life-sciences]"
  },
  {
    "objectID": "reference/soma.mapper.html",
    "href": "reference/soma.mapper.html",
    "title": "soma.mapper",
    "section": "",
    "text": "cloud.soma.mapper\n\n\n\n\n\nName\nDescription\n\n\n\n\nbuild_collection_mapper_workflow_graph\nThe primary entrypoint for the mapper module. The caller passes in either a\n\n\nexperiment_to_anndata_slice\nThis function is not to be called directly: please use\n\n\nexperiment_to_axis_counts\nReturns a tuple of (obs_counts, var_counts) if counts_only is True.\n\n\nrun_collection_mapper_workflow\nThis is an asynchronous entry point, which launches the task graph and returns\n\n\n\n\n\ncloud.soma.mapper.build_collection_mapper_workflow_graph(\n    soma_collection_uri=None,\n    soma_experiment_uris=None,\n    experiment_names=None,\n    measurement_name,\n    X_layer_name,\n    obs_query_string=None,\n    var_query_string=None,\n    obs_attrs=None,\n    var_attrs=None,\n    callback=lambda x: x,\n    args_dict=None,\n    extra_tiledb_config=None,\n    platform_config=None,\n    namespace=None,\n    task_graph_name='SOMAExperiment Collection Mapper',\n    counts_only=False,\n    use_batch_mode=False,\n    resource_class=None,\n    resources=None,\n    access_credentials_name=None,\n    verbose=False,\n)\nThe primary entrypoint for the mapper module. The caller passes in either a sequence of SOMAExperiment URIs or a SOMACollection, which is simply a collection of SOMAExperiment objects. The caller also passes in query terms and a callback lambda which will be called on the to_anndata output of each experiment’s query. The result will be a dictionary mapping experiment names to the callback lambda’s output for each input experiment.\nFor example, if the lambda maps an anndata object to its .shape, then with SOMA experiments A and B, the task graph would return the dict {\"A\": (56868, 43050), \"B\": (23539, 42044)}.\nParameters for input data:\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsoma_collection_uri\nOptional[str]\nURI of a SOMACollection containing SOMAExperiment objects to be processed. Please specify only one of soma_collection_uri or soma_experiment_uris.\nNone\n\n\nsoma_experiment_uris\nOptional[Sequence[str]]\nList/tuple of URIs of SOMAExperiment objects to be processed.\nNone\n\n\nexperiment_names\nOptional[Sequence[str]]\nOptional list of experiment names. If not provided, all SOMAExperiment objects are processed as specified by soma_collection_uri or soma_experiment_uris. If provided, experiment_names can be used to further subset/restrict which SOMAExperiment objects will be processed.\nNone\n\n\nmeasurement_name\nstr\nWhich SOMAMeasurement to query within the specified SOMAExperiment objects. For example, \"RNA\".\nrequired\n\n\nX_layer_name\nstr\nWhich X layer to query within the specified SOMAMeasurement objects. For example, \"data\", \"raw\", \"normalized\". Query parameters:\nrequired\n\n\nobs_query_string\nOptional[str]\nOptional query string for obs. For example: 'cell_type == \"liver\"'.\nNone\n\n\nvar_query_string\nOptional[str]\nOptional query string for var. For example: 'n_cells &gt; 100'.\nNone\n\n\nobs_attrs\nOptional[Sequence[str]]\nOptional list of obs attributes to return as query output. Default: all.\nNone\n\n\nvar_attrs\nOptional[Sequence[str]]\nOptional list of var attributes to return as query output. Default: all. Parameters for data processing:\nNone\n\n\ncallback\nCallable\nYour code to run on each UDF node, one for each SOMAExperiment. On each node, tiledbsoma.AxisQuery is run, using parameters you specify as above, and then query.to_anndata is run on that query output. Your callback function receives that query-output AnnData object. For example: lambda ad: ad.obs.shape.\nlambda x: x\n\n\nargs_dict\nOptional[Dict[str, Any]]\nOptional additional arguments to be passed to your callback. If provided, this must be a dict from string experiment name, to dict of key-value pairs.\nNone\n\n\ncounts_only\nOptional[bool]\nIf specified, only return obs/var counts, not the result of the provided callback. TileDB configs:\nFalse\n\n\nextra_tiledb_config\nOptional[Dict[str, object]]\nCurrently unused; reserved for future use.\nNone\n\n\nplatform_config\nOptional[Dict[str, object]]\nCurrently unused; reserved for future use. Cloud configs:\nNone\n\n\nnamespace\nOptional[str]\nTileDB namespace in which to run the UDFs.\nNone\n\n\ntask_graph_name\nstr\nOptional name for your task graph, so you can find it more easily among other runs. Real-time vs batch modes:\n'SOMAExperiment Collection Mapper'\n\n\nuse_batch_mode\nbool\nIf false (the default), uses real-time UDFs. These have lower latency but fewer resource options.\nFalse\n\n\nresource_class\nOptional[str]\n\"standard\" or \"large\". Only valid when use_batch_mode is False.\nNone\n\n\nresources\nOptional[Dict[str, object]]\nOnly valid when use_batch_mode is True. Example: resources={\"cpu\": \"2\", \"memory\": \"8Gi\"}.\nNone\n\n\naccess_credentials_name\nOptional[str]\nOnly valid when use_batch_mode is True. Other:\nNone\n\n\nverbose\nbool\nIf True, enable verbose logging. Default: False. Return value: A DAG object. If you’ve named this dag, you’ll need to call dag.compute(), dag.wait(), and dag.end_results().\nFalse\n\n\n\n\n\n\n\ncloud.soma.mapper.experiment_to_anndata_slice(\n    exp,\n    *,\n    measurement_name,\n    X_layer_name,\n    obs_query_string=None,\n    var_query_string=None,\n    obs_attrs=None,\n    var_attrs=None,\n)\nThis function is not to be called directly: please use run_collection_mapper_workflow or build_collection_mapper_workflow_graph. This is the function that runs as a UDF node for each SOMAExperiment you specify.\n\n\n\ncloud.soma.mapper.experiment_to_axis_counts(\n    exp,\n    *,\n    measurement_name,\n    X_layer_name,\n    obs_query_string=None,\n    var_query_string=None,\n    obs_attrs=None,\n    var_attrs=None,\n)\nReturns a tuple of (obs_counts, var_counts) if counts_only is True.\n\n\n\ncloud.soma.mapper.run_collection_mapper_workflow(\n    soma_collection_uri=None,\n    soma_experiment_uris=None,\n    experiment_names=None,\n    measurement_name,\n    X_layer_name,\n    obs_query_string=None,\n    var_query_string=None,\n    obs_attrs=None,\n    var_attrs=None,\n    callback=lambda x: x,\n    args_dict=None,\n    extra_tiledb_config=None,\n    platform_config=None,\n    namespace=None,\n    task_graph_name='SOMAExperiment Collection Mapper',\n    counts_only=False,\n    use_batch_mode=False,\n    resource_class=None,\n    resources=None,\n    access_credentials_name=None,\n    verbose=False,\n)\nThis is an asynchronous entry point, which launches the task graph and returns tracking information. Nominally this is not the primary use-case. Please see build_collection_mapper_workflow_graph for information about arguments and return value.",
    "crumbs": [
      "Get Started",
      "Analyze",
      "soma.mapper"
    ]
  },
  {
    "objectID": "reference/soma.mapper.html#functions",
    "href": "reference/soma.mapper.html#functions",
    "title": "soma.mapper",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nbuild_collection_mapper_workflow_graph\nThe primary entrypoint for the mapper module. The caller passes in either a\n\n\nexperiment_to_anndata_slice\nThis function is not to be called directly: please use\n\n\nexperiment_to_axis_counts\nReturns a tuple of (obs_counts, var_counts) if counts_only is True.\n\n\nrun_collection_mapper_workflow\nThis is an asynchronous entry point, which launches the task graph and returns\n\n\n\n\n\ncloud.soma.mapper.build_collection_mapper_workflow_graph(\n    soma_collection_uri=None,\n    soma_experiment_uris=None,\n    experiment_names=None,\n    measurement_name,\n    X_layer_name,\n    obs_query_string=None,\n    var_query_string=None,\n    obs_attrs=None,\n    var_attrs=None,\n    callback=lambda x: x,\n    args_dict=None,\n    extra_tiledb_config=None,\n    platform_config=None,\n    namespace=None,\n    task_graph_name='SOMAExperiment Collection Mapper',\n    counts_only=False,\n    use_batch_mode=False,\n    resource_class=None,\n    resources=None,\n    access_credentials_name=None,\n    verbose=False,\n)\nThe primary entrypoint for the mapper module. The caller passes in either a sequence of SOMAExperiment URIs or a SOMACollection, which is simply a collection of SOMAExperiment objects. The caller also passes in query terms and a callback lambda which will be called on the to_anndata output of each experiment’s query. The result will be a dictionary mapping experiment names to the callback lambda’s output for each input experiment.\nFor example, if the lambda maps an anndata object to its .shape, then with SOMA experiments A and B, the task graph would return the dict {\"A\": (56868, 43050), \"B\": (23539, 42044)}.\nParameters for input data:\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsoma_collection_uri\nOptional[str]\nURI of a SOMACollection containing SOMAExperiment objects to be processed. Please specify only one of soma_collection_uri or soma_experiment_uris.\nNone\n\n\nsoma_experiment_uris\nOptional[Sequence[str]]\nList/tuple of URIs of SOMAExperiment objects to be processed.\nNone\n\n\nexperiment_names\nOptional[Sequence[str]]\nOptional list of experiment names. If not provided, all SOMAExperiment objects are processed as specified by soma_collection_uri or soma_experiment_uris. If provided, experiment_names can be used to further subset/restrict which SOMAExperiment objects will be processed.\nNone\n\n\nmeasurement_name\nstr\nWhich SOMAMeasurement to query within the specified SOMAExperiment objects. For example, \"RNA\".\nrequired\n\n\nX_layer_name\nstr\nWhich X layer to query within the specified SOMAMeasurement objects. For example, \"data\", \"raw\", \"normalized\". Query parameters:\nrequired\n\n\nobs_query_string\nOptional[str]\nOptional query string for obs. For example: 'cell_type == \"liver\"'.\nNone\n\n\nvar_query_string\nOptional[str]\nOptional query string for var. For example: 'n_cells &gt; 100'.\nNone\n\n\nobs_attrs\nOptional[Sequence[str]]\nOptional list of obs attributes to return as query output. Default: all.\nNone\n\n\nvar_attrs\nOptional[Sequence[str]]\nOptional list of var attributes to return as query output. Default: all. Parameters for data processing:\nNone\n\n\ncallback\nCallable\nYour code to run on each UDF node, one for each SOMAExperiment. On each node, tiledbsoma.AxisQuery is run, using parameters you specify as above, and then query.to_anndata is run on that query output. Your callback function receives that query-output AnnData object. For example: lambda ad: ad.obs.shape.\nlambda x: x\n\n\nargs_dict\nOptional[Dict[str, Any]]\nOptional additional arguments to be passed to your callback. If provided, this must be a dict from string experiment name, to dict of key-value pairs.\nNone\n\n\ncounts_only\nOptional[bool]\nIf specified, only return obs/var counts, not the result of the provided callback. TileDB configs:\nFalse\n\n\nextra_tiledb_config\nOptional[Dict[str, object]]\nCurrently unused; reserved for future use.\nNone\n\n\nplatform_config\nOptional[Dict[str, object]]\nCurrently unused; reserved for future use. Cloud configs:\nNone\n\n\nnamespace\nOptional[str]\nTileDB namespace in which to run the UDFs.\nNone\n\n\ntask_graph_name\nstr\nOptional name for your task graph, so you can find it more easily among other runs. Real-time vs batch modes:\n'SOMAExperiment Collection Mapper'\n\n\nuse_batch_mode\nbool\nIf false (the default), uses real-time UDFs. These have lower latency but fewer resource options.\nFalse\n\n\nresource_class\nOptional[str]\n\"standard\" or \"large\". Only valid when use_batch_mode is False.\nNone\n\n\nresources\nOptional[Dict[str, object]]\nOnly valid when use_batch_mode is True. Example: resources={\"cpu\": \"2\", \"memory\": \"8Gi\"}.\nNone\n\n\naccess_credentials_name\nOptional[str]\nOnly valid when use_batch_mode is True. Other:\nNone\n\n\nverbose\nbool\nIf True, enable verbose logging. Default: False. Return value: A DAG object. If you’ve named this dag, you’ll need to call dag.compute(), dag.wait(), and dag.end_results().\nFalse\n\n\n\n\n\n\n\ncloud.soma.mapper.experiment_to_anndata_slice(\n    exp,\n    *,\n    measurement_name,\n    X_layer_name,\n    obs_query_string=None,\n    var_query_string=None,\n    obs_attrs=None,\n    var_attrs=None,\n)\nThis function is not to be called directly: please use run_collection_mapper_workflow or build_collection_mapper_workflow_graph. This is the function that runs as a UDF node for each SOMAExperiment you specify.\n\n\n\ncloud.soma.mapper.experiment_to_axis_counts(\n    exp,\n    *,\n    measurement_name,\n    X_layer_name,\n    obs_query_string=None,\n    var_query_string=None,\n    obs_attrs=None,\n    var_attrs=None,\n)\nReturns a tuple of (obs_counts, var_counts) if counts_only is True.\n\n\n\ncloud.soma.mapper.run_collection_mapper_workflow(\n    soma_collection_uri=None,\n    soma_experiment_uris=None,\n    experiment_names=None,\n    measurement_name,\n    X_layer_name,\n    obs_query_string=None,\n    var_query_string=None,\n    obs_attrs=None,\n    var_attrs=None,\n    callback=lambda x: x,\n    args_dict=None,\n    extra_tiledb_config=None,\n    platform_config=None,\n    namespace=None,\n    task_graph_name='SOMAExperiment Collection Mapper',\n    counts_only=False,\n    use_batch_mode=False,\n    resource_class=None,\n    resources=None,\n    access_credentials_name=None,\n    verbose=False,\n)\nThis is an asynchronous entry point, which launches the task graph and returns tracking information. Nominally this is not the primary use-case. Please see build_collection_mapper_workflow_graph for information about arguments and return value.",
    "crumbs": [
      "Get Started",
      "Analyze",
      "soma.mapper"
    ]
  },
  {
    "objectID": "reference/bioimg.ingestion.html",
    "href": "reference/bioimg.ingestion.html",
    "title": "bioimg.ingestion",
    "section": "",
    "text": "cloud.bioimg.ingestion\n\n\n\n\n\nName\nDescription\n\n\n\n\ningest\nThe function ingests microscopy images into TileDB arrays\n\n\n\n\n\ncloud.bioimg.ingestion.ingest(\n    source,\n    output,\n    config,\n    *args,\n    acn='',\n    taskgraph_name=None,\n    num_batches=None,\n    threads=0,\n    resources=None,\n    ingest_resources=None,\n    compute=True,\n    register=True,\n    mode=Mode.BATCH,\n    namespace,\n    verbose=False,\n    exclude_metadata=None,\n    converter=None,\n    output_ext='',\n    tile_scale=128,\n    timeout=86400,\n    **kwargs,\n)\nThe function ingests microscopy images into TileDB arrays\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource\nUnion[Sequence[str], str]\nuri / iterable of uris of input files. If the uri points to a directory of files make sure it ends with a trailing ‘/’\nrequired\n\n\noutput\nUnion[Sequence[str], str]\nuri / iterable of uris of output files. If the uri points to a directory of files make sure it ends with a trailing ‘/’\nrequired\n\n\nconfig\nMapping[str, Any]\ndict configuration to pass on tiledb.VFS for the source’s resolution\nrequired\n\n\nacn\nstr\nAccess Credentials Name (ACN) registered in TileDB Cloud (ARN type)\n''\n\n\ntaskgraph_name\nOptional[str]\nOptional name for taskgraph, defaults to None\nNone\n\n\nnum_batches\nOptional[int]\nNumber of graph nodes to spawn. Performs it sequentially if default, defaults to 1\nNone\n\n\nthreads\nOptional[int]\nNumber of threads for node side multiprocessing, defaults to 0\n0\n\n\nresources\nOptional[Mapping[str, Any]]\nconfiguration for node specs e.g. {“cpu”: “8”, “memory”: “4Gi”}, defaults to None\nNone\n\n\ningest_resources\nOptional[Mapping[str, Any]]\nconfiguration for node specs e.g. {“cpu”: “8”, “memory”: “4Gi”}. This parameter is intended to be used with the as_batch() wrapper and with the TileDB UI ingest endpoint. It defaults to None and will be superseded by the resources parameter described above.\nNone\n\n\ncompute\nbool\nWhen True the DAG returned will be computed inside the function otherwise DAG will only be returned.\nTrue\n\n\nregister\nbool\nWhen True the ingested images are also being registered under the namespace in which were ingested. Should be False when tiledb uris are given as destination paths, registration node is merged with the ingestion stage.\nTrue\n\n\nmode\nOptional[Mode]\nBy default runs Mode.Batch\nMode.BATCH\n\n\nnamespace\nOptional[str]\nThe namespace where the DAG will run\nrequired\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\nexclude_metadata\nOptional[Union[bool, Callable[[str], str]]]\nAn optional argument that specifies how to transform the original metadata. It can be one of the following: * A callable (function, method, etc.) that takes an OME-XML string and returns it as a string, while removing some of the original metadata and excluding them from being ingested. * A boolean value: * True: Indicates a specific built-in transformation should be applied * False: Indicates no transformation should be applied * None: Indicates no transformation should be applied (same as False).\nNone\n\n\nconverter\nOptional[str]\nThe converter to be used for the image ingestion, when None the default TIFF converter is used. Available converters are one of the (“tiff”, “zarr”, “osd”).\nNone\n\n\noutput_ext\nstr\nextension for the output images in tiledb\n''\n\n\ntile_scale\nint\nThe scaling factor applied to each tile during I/O. Larger scale factors will result in less I/O operations.\n128\n\n\naccess_credentials_name\n\n[TBDeprecated] Access Credentials Name (ACN) registered in TileDB Cloud (ARN type) if acn is not set.\nrequired\n\n\ndest_config\n\ndict configuration to pass on tiledb.VFS for the destination’s resolution\nrequired\n\n\nreader\n\nThe selected reader backend implementation either “experimental” or “production”. Default[“production”]\nrequired\n\n\ntimeout\nOptional[int]\nDuration (sec) ingestion DAG allowed to execute before timeout. The default is 86400 seconds (24 hours).\n86400",
    "crumbs": [
      "Get Started",
      "Analyze",
      "bioimg.ingestion"
    ]
  },
  {
    "objectID": "reference/bioimg.ingestion.html#functions",
    "href": "reference/bioimg.ingestion.html#functions",
    "title": "bioimg.ingestion",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ningest\nThe function ingests microscopy images into TileDB arrays\n\n\n\n\n\ncloud.bioimg.ingestion.ingest(\n    source,\n    output,\n    config,\n    *args,\n    acn='',\n    taskgraph_name=None,\n    num_batches=None,\n    threads=0,\n    resources=None,\n    ingest_resources=None,\n    compute=True,\n    register=True,\n    mode=Mode.BATCH,\n    namespace,\n    verbose=False,\n    exclude_metadata=None,\n    converter=None,\n    output_ext='',\n    tile_scale=128,\n    timeout=86400,\n    **kwargs,\n)\nThe function ingests microscopy images into TileDB arrays\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource\nUnion[Sequence[str], str]\nuri / iterable of uris of input files. If the uri points to a directory of files make sure it ends with a trailing ‘/’\nrequired\n\n\noutput\nUnion[Sequence[str], str]\nuri / iterable of uris of output files. If the uri points to a directory of files make sure it ends with a trailing ‘/’\nrequired\n\n\nconfig\nMapping[str, Any]\ndict configuration to pass on tiledb.VFS for the source’s resolution\nrequired\n\n\nacn\nstr\nAccess Credentials Name (ACN) registered in TileDB Cloud (ARN type)\n''\n\n\ntaskgraph_name\nOptional[str]\nOptional name for taskgraph, defaults to None\nNone\n\n\nnum_batches\nOptional[int]\nNumber of graph nodes to spawn. Performs it sequentially if default, defaults to 1\nNone\n\n\nthreads\nOptional[int]\nNumber of threads for node side multiprocessing, defaults to 0\n0\n\n\nresources\nOptional[Mapping[str, Any]]\nconfiguration for node specs e.g. {“cpu”: “8”, “memory”: “4Gi”}, defaults to None\nNone\n\n\ningest_resources\nOptional[Mapping[str, Any]]\nconfiguration for node specs e.g. {“cpu”: “8”, “memory”: “4Gi”}. This parameter is intended to be used with the as_batch() wrapper and with the TileDB UI ingest endpoint. It defaults to None and will be superseded by the resources parameter described above.\nNone\n\n\ncompute\nbool\nWhen True the DAG returned will be computed inside the function otherwise DAG will only be returned.\nTrue\n\n\nregister\nbool\nWhen True the ingested images are also being registered under the namespace in which were ingested. Should be False when tiledb uris are given as destination paths, registration node is merged with the ingestion stage.\nTrue\n\n\nmode\nOptional[Mode]\nBy default runs Mode.Batch\nMode.BATCH\n\n\nnamespace\nOptional[str]\nThe namespace where the DAG will run\nrequired\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\nexclude_metadata\nOptional[Union[bool, Callable[[str], str]]]\nAn optional argument that specifies how to transform the original metadata. It can be one of the following: * A callable (function, method, etc.) that takes an OME-XML string and returns it as a string, while removing some of the original metadata and excluding them from being ingested. * A boolean value: * True: Indicates a specific built-in transformation should be applied * False: Indicates no transformation should be applied * None: Indicates no transformation should be applied (same as False).\nNone\n\n\nconverter\nOptional[str]\nThe converter to be used for the image ingestion, when None the default TIFF converter is used. Available converters are one of the (“tiff”, “zarr”, “osd”).\nNone\n\n\noutput_ext\nstr\nextension for the output images in tiledb\n''\n\n\ntile_scale\nint\nThe scaling factor applied to each tile during I/O. Larger scale factors will result in less I/O operations.\n128\n\n\naccess_credentials_name\n\n[TBDeprecated] Access Credentials Name (ACN) registered in TileDB Cloud (ARN type) if acn is not set.\nrequired\n\n\ndest_config\n\ndict configuration to pass on tiledb.VFS for the destination’s resolution\nrequired\n\n\nreader\n\nThe selected reader backend implementation either “experimental” or “production”. Default[“production”]\nrequired\n\n\ntimeout\nOptional[int]\nDuration (sec) ingestion DAG allowed to execute before timeout. The default is 86400 seconds (24 hours).\n86400",
    "crumbs": [
      "Get Started",
      "Analyze",
      "bioimg.ingestion"
    ]
  },
  {
    "objectID": "reference/client.html",
    "href": "reference/client.html",
    "title": "client",
    "section": "",
    "text": "cloud.client\n\n\n\n\n\nName\nDescription\n\n\n\n\nClient\nTileDB Client.\n\n\n\n\n\ncloud.client.Client(pool_threads=None, retry_mode=RetryMode.DEFAULT)\nTileDB Client.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npool_threads\nOptional[int]\nNumber of threads to use for http requests\nNone\n\n\nretry_mode\nRetryOrStr\nRetry mode [“default”, “forceful”, “disabled”]\nRetryMode.DEFAULT\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nbuild\nBuilds an API client with the given config.\n\n\nretry_mode\nSets how we should retry requests and updates API instances.\n\n\nset_threads\nUpdates the number of threads in the async thread pool.\n\n\n\n\n\ncloud.client.Client.build(builder)\nBuilds an API client with the given config.\n\n\n\ncloud.client.Client.retry_mode(mode=RetryMode.DEFAULT)\nSets how we should retry requests and updates API instances.\n\n\n\ncloud.client.Client.set_threads(threads=None)\nUpdates the number of threads in the async thread pool.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nConfig\nBuilds a tiledb config setting the login parameters that exist for the cloud service\n\n\nCtx\nBuilds a TileDB Context that has the tiledb config parameters\n\n\ndefault_charged_namespace\nReturns the namespace :func:default_user charges to by default.\n\n\ndefault_user\nReturns the default user to be used.\n\n\nfind_organization_or_user_for_default_charges\nTakes a user model and finds either the first non public organization\n\n\nlist_arrays\nList arrays in a user account\n\n\nlist_groups\nList groups owned by a user.\n\n\nlist_public_arrays\nList public arrays\n\n\nlist_public_groups\nList public groups owned by a user.\n\n\nlist_shared_arrays\nList shared arrays\n\n\nlist_shared_groups\nList groups shared by/to specified namespaces.\n\n\nlogin\nLogin to cloud service\n\n\norganization\n\n\n\norganizations\n\n\n\nuser_profile\n\n\n\n\n\n\ncloud.client.Config(cfg_dict=None)\nBuilds a tiledb config setting the login parameters that exist for the cloud service\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\ntiledb.Config\n\n\n\n\n\n\n\ncloud.client.Ctx(config=None)\nBuilds a TileDB Context that has the tiledb config parameters for tiledb cloud set from stored login\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\ntiledb.Ctx\n\n\n\n\n\n\n\ncloud.client.default_charged_namespace(required_action=None)\nReturns the namespace :func:default_user charges to by default.\nIf required_action is set then it checks amond the user organizations to find the first one that support this action.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrequired_action\nOptional[str]\na namespace action, must be an enum from rest_api.NamespaceActions\nNone\n\n\n\n\n\n\n\ncloud.client.default_user()\nReturns the default user to be used.\nIf :data:config.user is set, that is the default user. If unset, we fetch the currently logged-in user with :func:user_profile and store that in :data:config.user.\n\n\n\ncloud.client.find_organization_or_user_for_default_charges(\n    user,\n    required_action=None,\n)\nTakes a user model and finds either the first non public organization or the user itself\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuser\nmodels_v1.User\n\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nnamespace name to charge by default (organization or user if not part of any organization)\n\n\n\n\n\n\n\ncloud.client.list_arrays(\n    namespace=None,\n    permissions=None,\n    tag=None,\n    exclude_tag=None,\n    search=None,\n    file_type=None,\n    exclude_file_type=None,\n    page=None,\n    per_page=None,\n    async_req=False,\n    with_metadata=None,\n)\nList arrays in a user account\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnamespace\nstr\nlist arrays in single namespace\nNone\n\n\npermissions\nstr\nfilter arrays for given permissions\nNone\n\n\ntag\nlist\nzero or more tags to filter on\nNone\n\n\nexclude_tag\nlist\nzero or more tags to filter on\nNone\n\n\nsearch\nstr\nsearch string\nNone\n\n\nfile_type\nlist\nzero or more file_types to filter on\nNone\n\n\nexclude_file_type\nlist\nzero or more file_types to filter on\nNone\n\n\npage\nint\noptional page for pagination\nNone\n\n\nper_page\nint\noptional per_page for pagination\nNone\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\nwith_metadata\nbool\ninclude the metadata of the array\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nlist of all array metadata you have access to that meet the filter applied\n\n\n\n\n\n\n\ncloud.client.list_groups(\n    namespace=None,\n    permission=None,\n    group_type=None,\n    tag=None,\n    exclude_tag=None,\n    search=None,\n    flat=True,\n    parent=None,\n    page=None,\n    per_page=None,\n    async_req=False,\n    with_metadata=None,\n)\nList groups owned by a user.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnamespace\nOptional[str]\nThe namespace whose owned groups should be returned.\nNone\n\n\npermissions\n\nFilter arrays for the given permission.\nrequired\n\n\ngroup_type\nOptional[str]\nIf provided, return only groups of the given type.\nNone\n\n\ntag\nUnion[str, Sequence[str], None]\nIf provided, include groups matching the given tags.\nNone\n\n\nexclude_tag\nUnion[str, Sequence[str], None]\nIf provided, exclude groups matching the given tags.\nNone\n\n\nsearch\nOptional[str]\nA search string.\nNone\n\n\nflat\nbool\nIf false (the default), return only “top-level” groups (i.e., no sub-groups within other groups).\nTrue\n\n\nparent\nUnion[None, str, uuid.UUID]\nIf provided, only show the children of the group with the given ID.\nNone\n\n\npage\nOptional[int]\nFor pagination, which page to return (1-based).\nNone\n\n\nper_page\nOptional[int]\nFor pagination, how many elements to return on a page.\nNone\n\n\nasync_req\nbool\nRun this asynchronously; return a Future of results.\nFalse\n\n\nwith_metadata\nbool\ninclude the metadata of the array\nNone\n\n\n\n\n\n\n\ncloud.client.list_public_arrays(\n    namespace=None,\n    permissions=None,\n    tag=None,\n    exclude_tag=None,\n    search=None,\n    file_type=None,\n    exclude_file_type=None,\n    page=None,\n    per_page=None,\n    async_req=False,\n    with_metadata=None,\n)\nList public arrays\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnamespace\nstr\nlist arrays in single namespace\nNone\n\n\npermissions\nstr\nfilter arrays for given permissions\nNone\n\n\ntag\nlist\nzero or more tags to filter on\nNone\n\n\nexclude_tag\nlist\nzero or more tags to filter on\nNone\n\n\nsearch\nstr\nsearch string\nNone\n\n\nfile_type\nlist\nzero or more file_types to filter on\nNone\n\n\nexclude_file_type\nlist\nzero or more file_types to filter on\nNone\n\n\npage\nint\noptional page for pagination\nNone\n\n\nper_page\nint\noptional per_page for pagination\nNone\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\nwith_metadata\nbool\ninclude the metadata of the array\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nlist of all array metadata you have access to that meet the filter applied\n\n\n\n\n\n\n\ncloud.client.list_public_groups(\n    namespace=None,\n    permission=None,\n    group_type=None,\n    tag=None,\n    exclude_tag=None,\n    search=None,\n    flat=True,\n    parent=None,\n    page=None,\n    per_page=None,\n    async_req=False,\n    with_metadata=None,\n)\nList public groups owned by a user.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnamespace\nOptional[str]\nThe namespace whose owned groups should be returned.\nNone\n\n\npermissions\n\nFilter arrays for the given permission.\nrequired\n\n\ngroup_type\nOptional[str]\nIf provided, return only groups of the given type.\nNone\n\n\ntag\nUnion[str, Sequence[str], None]\nIf provided, include groups matching the given tags.\nNone\n\n\nexclude_tag\nUnion[str, Sequence[str], None]\nIf provided, exclude groups matching the given tags.\nNone\n\n\nsearch\nOptional[str]\nA search string.\nNone\n\n\nflat\nbool\nIf false (the default), return only “top-level” groups (i.e., no sub-groups within other groups).\nTrue\n\n\nparent\nUnion[None, str, uuid.UUID]\nIf provided, only show the children of the group with the given ID.\nNone\n\n\npage\nOptional[int]\nFor pagination, which page to return (1-based).\nNone\n\n\nper_page\nOptional[int]\nFor pagination, how many elements to return on a page.\nNone\n\n\nasync_req\nbool\nRun this asynchronously; return a Future of results.\nFalse\n\n\nwith_metadata\nbool\ninclude the metadata of the array\nNone\n\n\n\n\n\n\n\ncloud.client.list_shared_arrays(\n    namespace=None,\n    permissions=None,\n    tag=None,\n    exclude_tag=None,\n    search=None,\n    file_type=None,\n    exclude_file_type=None,\n    page=None,\n    per_page=None,\n    async_req=False,\n    with_metadata=None,\n)\nList shared arrays\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnamespace\nstr\nlist arrays in single namespace\nNone\n\n\npermissions\nstr\nfilter arrays for given permissions\nNone\n\n\ntag\nlist\nzero or more tags to filter on\nNone\n\n\nexclude_tag\nlist\nzero or more tags to filter on\nNone\n\n\nsearch\nstr\nsearch string\nNone\n\n\nfile_type\nlist\nzero or more file_types to filter on\nNone\n\n\nexclude_file_type\nlist\nzero or more file_types to filter on\nNone\n\n\npage\nint\noptional page for pagination\nNone\n\n\nper_page\nint\noptional per_page for pagination\nNone\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\nwith_metadata\nbool\ninclude the metadata of the array\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nlist of all array metadata you have access to that meet the filter applied\n\n\n\n\n\n\n\ncloud.client.list_shared_groups(\n    namespace=None,\n    shared_to=None,\n    permission=None,\n    group_type=None,\n    tag=None,\n    exclude_tag=None,\n    search=None,\n    flat=True,\n    parent=None,\n    page=None,\n    per_page=None,\n    async_req=False,\n    with_metadata=None,\n)\nList groups shared by/to specified namespaces.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnamespace\nOptional[str]\nThe namespace whose owned groups should be returned.\nNone\n\n\nshared_to\nOptional[str]\nA target, to return groups shared to this namespace.\nNone\n\n\npermissions\n\nFilter arrays for the given permission.\nrequired\n\n\ngroup_type\nOptional[str]\nIf provided, return only groups of the given type.\nNone\n\n\ntag\nUnion[str, Sequence[str], None]\nIf provided, include groups matching the given tags.\nNone\n\n\nexclude_tag\nUnion[str, Sequence[str], None]\nIf provided, exclude groups matching the given tags.\nNone\n\n\nsearch\nOptional[str]\nA search string.\nNone\n\n\nflat\nbool\nIf false (the default), return only “top-level” groups (i.e., no sub-groups within other groups).\nTrue\n\n\nparent\nUnion[None, str, uuid.UUID]\nIf provided, only show the children of the group with the given ID.\nNone\n\n\npage\nOptional[int]\nFor pagination, which page to return (1-based).\nNone\n\n\nper_page\nOptional[int]\nFor pagination, how many elements to return on a page.\nNone\n\n\nasync_req\nbool\nRun this asynchronously; return a Future of results.\nFalse\n\n\nwith_metadata\nbool\ninclude the metadata of the array\nNone\n\n\n\n\n\n\n\ncloud.client.login(\n    token=None,\n    username=None,\n    password=None,\n    host=None,\n    verify_ssl=None,\n    no_session=False,\n    threads=None,\n)\nLogin to cloud service\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntoken\n\napi token for login\nNone\n\n\nusername\n\nusername for login\nNone\n\n\npassword\n\npassword for login\nNone\n\n\nhost\n\nhost to login to. the tiledb.cloud.regions module contains region-specific host constants.\nNone\n\n\nverify_ssl\n\nEnable strict SSL verification\nNone\n\n\nno_session\n\ndon’t create a session token on login, store instead username/password\nFalse\n\n\nthreads\n\nnumber of threads to enable for concurrent requests\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncloud.client.organization(organization, async_req=False)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\norganization\nstr\norganization to fetct\nrequired\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\ndetails about organization\n\n\n\n\n\n\n\ncloud.client.organizations(async_req=False)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nlist of all organizations user is part of\n\n\n\n\n\n\n\ncloud.client.user_profile(async_req=False)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nyour user profile",
    "crumbs": [
      "Get Started",
      "Account",
      "client"
    ]
  },
  {
    "objectID": "reference/client.html#classes",
    "href": "reference/client.html#classes",
    "title": "client",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nClient\nTileDB Client.\n\n\n\n\n\ncloud.client.Client(pool_threads=None, retry_mode=RetryMode.DEFAULT)\nTileDB Client.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npool_threads\nOptional[int]\nNumber of threads to use for http requests\nNone\n\n\nretry_mode\nRetryOrStr\nRetry mode [“default”, “forceful”, “disabled”]\nRetryMode.DEFAULT\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nbuild\nBuilds an API client with the given config.\n\n\nretry_mode\nSets how we should retry requests and updates API instances.\n\n\nset_threads\nUpdates the number of threads in the async thread pool.\n\n\n\n\n\ncloud.client.Client.build(builder)\nBuilds an API client with the given config.\n\n\n\ncloud.client.Client.retry_mode(mode=RetryMode.DEFAULT)\nSets how we should retry requests and updates API instances.\n\n\n\ncloud.client.Client.set_threads(threads=None)\nUpdates the number of threads in the async thread pool.",
    "crumbs": [
      "Get Started",
      "Account",
      "client"
    ]
  },
  {
    "objectID": "reference/client.html#functions",
    "href": "reference/client.html#functions",
    "title": "client",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nConfig\nBuilds a tiledb config setting the login parameters that exist for the cloud service\n\n\nCtx\nBuilds a TileDB Context that has the tiledb config parameters\n\n\ndefault_charged_namespace\nReturns the namespace :func:default_user charges to by default.\n\n\ndefault_user\nReturns the default user to be used.\n\n\nfind_organization_or_user_for_default_charges\nTakes a user model and finds either the first non public organization\n\n\nlist_arrays\nList arrays in a user account\n\n\nlist_groups\nList groups owned by a user.\n\n\nlist_public_arrays\nList public arrays\n\n\nlist_public_groups\nList public groups owned by a user.\n\n\nlist_shared_arrays\nList shared arrays\n\n\nlist_shared_groups\nList groups shared by/to specified namespaces.\n\n\nlogin\nLogin to cloud service\n\n\norganization\n\n\n\norganizations\n\n\n\nuser_profile\n\n\n\n\n\n\ncloud.client.Config(cfg_dict=None)\nBuilds a tiledb config setting the login parameters that exist for the cloud service\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\ntiledb.Config\n\n\n\n\n\n\n\ncloud.client.Ctx(config=None)\nBuilds a TileDB Context that has the tiledb config parameters for tiledb cloud set from stored login\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\ntiledb.Ctx\n\n\n\n\n\n\n\ncloud.client.default_charged_namespace(required_action=None)\nReturns the namespace :func:default_user charges to by default.\nIf required_action is set then it checks amond the user organizations to find the first one that support this action.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrequired_action\nOptional[str]\na namespace action, must be an enum from rest_api.NamespaceActions\nNone\n\n\n\n\n\n\n\ncloud.client.default_user()\nReturns the default user to be used.\nIf :data:config.user is set, that is the default user. If unset, we fetch the currently logged-in user with :func:user_profile and store that in :data:config.user.\n\n\n\ncloud.client.find_organization_or_user_for_default_charges(\n    user,\n    required_action=None,\n)\nTakes a user model and finds either the first non public organization or the user itself\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuser\nmodels_v1.User\n\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nnamespace name to charge by default (organization or user if not part of any organization)\n\n\n\n\n\n\n\ncloud.client.list_arrays(\n    namespace=None,\n    permissions=None,\n    tag=None,\n    exclude_tag=None,\n    search=None,\n    file_type=None,\n    exclude_file_type=None,\n    page=None,\n    per_page=None,\n    async_req=False,\n    with_metadata=None,\n)\nList arrays in a user account\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnamespace\nstr\nlist arrays in single namespace\nNone\n\n\npermissions\nstr\nfilter arrays for given permissions\nNone\n\n\ntag\nlist\nzero or more tags to filter on\nNone\n\n\nexclude_tag\nlist\nzero or more tags to filter on\nNone\n\n\nsearch\nstr\nsearch string\nNone\n\n\nfile_type\nlist\nzero or more file_types to filter on\nNone\n\n\nexclude_file_type\nlist\nzero or more file_types to filter on\nNone\n\n\npage\nint\noptional page for pagination\nNone\n\n\nper_page\nint\noptional per_page for pagination\nNone\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\nwith_metadata\nbool\ninclude the metadata of the array\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nlist of all array metadata you have access to that meet the filter applied\n\n\n\n\n\n\n\ncloud.client.list_groups(\n    namespace=None,\n    permission=None,\n    group_type=None,\n    tag=None,\n    exclude_tag=None,\n    search=None,\n    flat=True,\n    parent=None,\n    page=None,\n    per_page=None,\n    async_req=False,\n    with_metadata=None,\n)\nList groups owned by a user.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnamespace\nOptional[str]\nThe namespace whose owned groups should be returned.\nNone\n\n\npermissions\n\nFilter arrays for the given permission.\nrequired\n\n\ngroup_type\nOptional[str]\nIf provided, return only groups of the given type.\nNone\n\n\ntag\nUnion[str, Sequence[str], None]\nIf provided, include groups matching the given tags.\nNone\n\n\nexclude_tag\nUnion[str, Sequence[str], None]\nIf provided, exclude groups matching the given tags.\nNone\n\n\nsearch\nOptional[str]\nA search string.\nNone\n\n\nflat\nbool\nIf false (the default), return only “top-level” groups (i.e., no sub-groups within other groups).\nTrue\n\n\nparent\nUnion[None, str, uuid.UUID]\nIf provided, only show the children of the group with the given ID.\nNone\n\n\npage\nOptional[int]\nFor pagination, which page to return (1-based).\nNone\n\n\nper_page\nOptional[int]\nFor pagination, how many elements to return on a page.\nNone\n\n\nasync_req\nbool\nRun this asynchronously; return a Future of results.\nFalse\n\n\nwith_metadata\nbool\ninclude the metadata of the array\nNone\n\n\n\n\n\n\n\ncloud.client.list_public_arrays(\n    namespace=None,\n    permissions=None,\n    tag=None,\n    exclude_tag=None,\n    search=None,\n    file_type=None,\n    exclude_file_type=None,\n    page=None,\n    per_page=None,\n    async_req=False,\n    with_metadata=None,\n)\nList public arrays\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnamespace\nstr\nlist arrays in single namespace\nNone\n\n\npermissions\nstr\nfilter arrays for given permissions\nNone\n\n\ntag\nlist\nzero or more tags to filter on\nNone\n\n\nexclude_tag\nlist\nzero or more tags to filter on\nNone\n\n\nsearch\nstr\nsearch string\nNone\n\n\nfile_type\nlist\nzero or more file_types to filter on\nNone\n\n\nexclude_file_type\nlist\nzero or more file_types to filter on\nNone\n\n\npage\nint\noptional page for pagination\nNone\n\n\nper_page\nint\noptional per_page for pagination\nNone\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\nwith_metadata\nbool\ninclude the metadata of the array\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nlist of all array metadata you have access to that meet the filter applied\n\n\n\n\n\n\n\ncloud.client.list_public_groups(\n    namespace=None,\n    permission=None,\n    group_type=None,\n    tag=None,\n    exclude_tag=None,\n    search=None,\n    flat=True,\n    parent=None,\n    page=None,\n    per_page=None,\n    async_req=False,\n    with_metadata=None,\n)\nList public groups owned by a user.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnamespace\nOptional[str]\nThe namespace whose owned groups should be returned.\nNone\n\n\npermissions\n\nFilter arrays for the given permission.\nrequired\n\n\ngroup_type\nOptional[str]\nIf provided, return only groups of the given type.\nNone\n\n\ntag\nUnion[str, Sequence[str], None]\nIf provided, include groups matching the given tags.\nNone\n\n\nexclude_tag\nUnion[str, Sequence[str], None]\nIf provided, exclude groups matching the given tags.\nNone\n\n\nsearch\nOptional[str]\nA search string.\nNone\n\n\nflat\nbool\nIf false (the default), return only “top-level” groups (i.e., no sub-groups within other groups).\nTrue\n\n\nparent\nUnion[None, str, uuid.UUID]\nIf provided, only show the children of the group with the given ID.\nNone\n\n\npage\nOptional[int]\nFor pagination, which page to return (1-based).\nNone\n\n\nper_page\nOptional[int]\nFor pagination, how many elements to return on a page.\nNone\n\n\nasync_req\nbool\nRun this asynchronously; return a Future of results.\nFalse\n\n\nwith_metadata\nbool\ninclude the metadata of the array\nNone\n\n\n\n\n\n\n\ncloud.client.list_shared_arrays(\n    namespace=None,\n    permissions=None,\n    tag=None,\n    exclude_tag=None,\n    search=None,\n    file_type=None,\n    exclude_file_type=None,\n    page=None,\n    per_page=None,\n    async_req=False,\n    with_metadata=None,\n)\nList shared arrays\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnamespace\nstr\nlist arrays in single namespace\nNone\n\n\npermissions\nstr\nfilter arrays for given permissions\nNone\n\n\ntag\nlist\nzero or more tags to filter on\nNone\n\n\nexclude_tag\nlist\nzero or more tags to filter on\nNone\n\n\nsearch\nstr\nsearch string\nNone\n\n\nfile_type\nlist\nzero or more file_types to filter on\nNone\n\n\nexclude_file_type\nlist\nzero or more file_types to filter on\nNone\n\n\npage\nint\noptional page for pagination\nNone\n\n\nper_page\nint\noptional per_page for pagination\nNone\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\nwith_metadata\nbool\ninclude the metadata of the array\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nlist of all array metadata you have access to that meet the filter applied\n\n\n\n\n\n\n\ncloud.client.list_shared_groups(\n    namespace=None,\n    shared_to=None,\n    permission=None,\n    group_type=None,\n    tag=None,\n    exclude_tag=None,\n    search=None,\n    flat=True,\n    parent=None,\n    page=None,\n    per_page=None,\n    async_req=False,\n    with_metadata=None,\n)\nList groups shared by/to specified namespaces.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnamespace\nOptional[str]\nThe namespace whose owned groups should be returned.\nNone\n\n\nshared_to\nOptional[str]\nA target, to return groups shared to this namespace.\nNone\n\n\npermissions\n\nFilter arrays for the given permission.\nrequired\n\n\ngroup_type\nOptional[str]\nIf provided, return only groups of the given type.\nNone\n\n\ntag\nUnion[str, Sequence[str], None]\nIf provided, include groups matching the given tags.\nNone\n\n\nexclude_tag\nUnion[str, Sequence[str], None]\nIf provided, exclude groups matching the given tags.\nNone\n\n\nsearch\nOptional[str]\nA search string.\nNone\n\n\nflat\nbool\nIf false (the default), return only “top-level” groups (i.e., no sub-groups within other groups).\nTrue\n\n\nparent\nUnion[None, str, uuid.UUID]\nIf provided, only show the children of the group with the given ID.\nNone\n\n\npage\nOptional[int]\nFor pagination, which page to return (1-based).\nNone\n\n\nper_page\nOptional[int]\nFor pagination, how many elements to return on a page.\nNone\n\n\nasync_req\nbool\nRun this asynchronously; return a Future of results.\nFalse\n\n\nwith_metadata\nbool\ninclude the metadata of the array\nNone\n\n\n\n\n\n\n\ncloud.client.login(\n    token=None,\n    username=None,\n    password=None,\n    host=None,\n    verify_ssl=None,\n    no_session=False,\n    threads=None,\n)\nLogin to cloud service\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntoken\n\napi token for login\nNone\n\n\nusername\n\nusername for login\nNone\n\n\npassword\n\npassword for login\nNone\n\n\nhost\n\nhost to login to. the tiledb.cloud.regions module contains region-specific host constants.\nNone\n\n\nverify_ssl\n\nEnable strict SSL verification\nNone\n\n\nno_session\n\ndon’t create a session token on login, store instead username/password\nFalse\n\n\nthreads\n\nnumber of threads to enable for concurrent requests\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncloud.client.organization(organization, async_req=False)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\norganization\nstr\norganization to fetct\nrequired\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\ndetails about organization\n\n\n\n\n\n\n\ncloud.client.organizations(async_req=False)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nlist of all organizations user is part of\n\n\n\n\n\n\n\ncloud.client.user_profile(async_req=False)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nyour user profile",
    "crumbs": [
      "Get Started",
      "Account",
      "client"
    ]
  },
  {
    "objectID": "reference/compute.delayed.html",
    "href": "reference/compute.delayed.html",
    "title": "compute.delayed",
    "section": "",
    "text": "cloud.compute.delayed\n\n\n\n\n\nName\nDescription\n\n\n\n\nDelayed\nServerless Python function execution as a future.\n\n\nDelayedArrayUDF\nServerless Array UDF called with the\n\n\nDelayedBase\nBase Delayed interface class\n\n\nDelayedMultiArrayUDF\nServerless Multi-Array UDF called with the\n\n\nDelayedSQL\nServerless SQL query on a registered Array called with the\n\n\n\n\n\ncloud.compute.delayed.Delayed(\n    func_exec,\n    *args,\n    local=False,\n    mode=Mode.REALTIME,\n    **kwargs,\n)\nServerless Python function execution as a future.\nExtends DelayedBase\n\n\n\ncloud.compute.delayed.DelayedArrayUDF(uri, func_exec, *args, **kwargs)\nServerless Array UDF called with the Delayed API\nExtends DelayedBase\n\n\n\ncloud.compute.delayed.DelayedBase(\n    func,\n    *args,\n    name=None,\n    dag=None,\n    local_mode=False,\n    mode=Mode.REALTIME,\n    **kwargs,\n)\nBase Delayed interface class\n\n\n\n\n\nName\nDescription\n\n\n\n\nall\nRun a list of Delayed object all in parallel\n\n\ncompute\nStarts execution of all Delayed tasks associated with this node.\n\n\nvisualize\nBuild and render a tree diagram of the DAG.\n\n\n\n\n\ncloud.compute.delayed.DelayedBase.all(futures, namespace=None)\nRun a list of Delayed object all in parallel\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfutures\n\nlist of Delayed objects to run\nrequired\n\n\nnamespace\n\noptional namespace to run all tasks in\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nlist of results in order of futures\n\n\n\n\n\n\n\ncloud.compute.delayed.DelayedBase.compute(\n    namespace=None,\n    name=None,\n    max_workers=None,\n)\nStarts execution of all Delayed tasks associated with this node.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnamespace\nOptional[str]\nThe namespace to execute tasks under, if different than the user’s default.\nNone\n\n\nname\nOptional[str]\nAn optional name to identify the task graph in logs.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nAny\nresults\n\n\n\n\n\n\n\ncloud.compute.delayed.DelayedBase.visualize(\n    notebook=True,\n    auto_update=True,\n    force_plotly=False,\n)\nBuild and render a tree diagram of the DAG.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnotebook\n\nIs the visualization inside a jupyter notebook? If so we’ll use a widget\nTrue\n\n\nauto_update\n\nShould the diagram be auto updated with each status change\nTrue\n\n\nforce_plotly\n\nForce the use of plotly graphs instead of TileDB Plot Widget\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nreturns plotly figure\n\n\n\n\n\n\n\n\n\ncloud.compute.delayed.DelayedMultiArrayUDF(func, array_list, *args, **kwargs)\nServerless Multi-Array UDF called with the Delayed API\nExtends DelayedBase\n\n\n\ncloud.compute.delayed.DelayedSQL(*args, **kwargs)\nServerless SQL query on a registered Array called with the Delayed API\nExtends DelayedBase",
    "crumbs": [
      "Get Started",
      "Scale",
      "compute.delayed"
    ]
  },
  {
    "objectID": "reference/compute.delayed.html#classes",
    "href": "reference/compute.delayed.html#classes",
    "title": "compute.delayed",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nDelayed\nServerless Python function execution as a future.\n\n\nDelayedArrayUDF\nServerless Array UDF called with the\n\n\nDelayedBase\nBase Delayed interface class\n\n\nDelayedMultiArrayUDF\nServerless Multi-Array UDF called with the\n\n\nDelayedSQL\nServerless SQL query on a registered Array called with the\n\n\n\n\n\ncloud.compute.delayed.Delayed(\n    func_exec,\n    *args,\n    local=False,\n    mode=Mode.REALTIME,\n    **kwargs,\n)\nServerless Python function execution as a future.\nExtends DelayedBase\n\n\n\ncloud.compute.delayed.DelayedArrayUDF(uri, func_exec, *args, **kwargs)\nServerless Array UDF called with the Delayed API\nExtends DelayedBase\n\n\n\ncloud.compute.delayed.DelayedBase(\n    func,\n    *args,\n    name=None,\n    dag=None,\n    local_mode=False,\n    mode=Mode.REALTIME,\n    **kwargs,\n)\nBase Delayed interface class\n\n\n\n\n\nName\nDescription\n\n\n\n\nall\nRun a list of Delayed object all in parallel\n\n\ncompute\nStarts execution of all Delayed tasks associated with this node.\n\n\nvisualize\nBuild and render a tree diagram of the DAG.\n\n\n\n\n\ncloud.compute.delayed.DelayedBase.all(futures, namespace=None)\nRun a list of Delayed object all in parallel\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfutures\n\nlist of Delayed objects to run\nrequired\n\n\nnamespace\n\noptional namespace to run all tasks in\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nlist of results in order of futures\n\n\n\n\n\n\n\ncloud.compute.delayed.DelayedBase.compute(\n    namespace=None,\n    name=None,\n    max_workers=None,\n)\nStarts execution of all Delayed tasks associated with this node.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnamespace\nOptional[str]\nThe namespace to execute tasks under, if different than the user’s default.\nNone\n\n\nname\nOptional[str]\nAn optional name to identify the task graph in logs.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nAny\nresults\n\n\n\n\n\n\n\ncloud.compute.delayed.DelayedBase.visualize(\n    notebook=True,\n    auto_update=True,\n    force_plotly=False,\n)\nBuild and render a tree diagram of the DAG.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnotebook\n\nIs the visualization inside a jupyter notebook? If so we’ll use a widget\nTrue\n\n\nauto_update\n\nShould the diagram be auto updated with each status change\nTrue\n\n\nforce_plotly\n\nForce the use of plotly graphs instead of TileDB Plot Widget\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nreturns plotly figure\n\n\n\n\n\n\n\n\n\ncloud.compute.delayed.DelayedMultiArrayUDF(func, array_list, *args, **kwargs)\nServerless Multi-Array UDF called with the Delayed API\nExtends DelayedBase\n\n\n\ncloud.compute.delayed.DelayedSQL(*args, **kwargs)\nServerless SQL query on a registered Array called with the Delayed API\nExtends DelayedBase",
    "crumbs": [
      "Get Started",
      "Scale",
      "compute.delayed"
    ]
  },
  {
    "objectID": "reference/dag.mode.html",
    "href": "reference/dag.mode.html",
    "title": "dag.mode",
    "section": "",
    "text": "cloud.dag.mode\n\n\n\n\n\nName\nDescription\n\n\n\n\nMode\nMode to run a DAG in\n\n\n\n\n\ncloud.dag.mode.Mode()\nMode to run a DAG in\n\n\n\n\n\nName\nDescription\n\n\n\n\nBATCH\nDesigned for large, resource intensive asynchronous workloads.\n\n\nLOCAL\nRun on local machine (testing purposes)\n\n\nREALTIME\nDesigned to return results directly to the client (default).",
    "crumbs": [
      "Get Started",
      "Scale",
      "dag.mode"
    ]
  },
  {
    "objectID": "reference/dag.mode.html#classes",
    "href": "reference/dag.mode.html#classes",
    "title": "dag.mode",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nMode\nMode to run a DAG in\n\n\n\n\n\ncloud.dag.mode.Mode()\nMode to run a DAG in\n\n\n\n\n\nName\nDescription\n\n\n\n\nBATCH\nDesigned for large, resource intensive asynchronous workloads.\n\n\nLOCAL\nRun on local machine (testing purposes)\n\n\nREALTIME\nDesigned to return results directly to the client (default).",
    "crumbs": [
      "Get Started",
      "Scale",
      "dag.mode"
    ]
  },
  {
    "objectID": "reference/array.html",
    "href": "reference/array.html",
    "title": "array",
    "section": "",
    "text": "cloud.array\nRegister, search, and manage arrays with TileDB.\n\n\n\n\n\nName\nDescription\n\n\n\n\nArrayList\nThis class incrementally builds a list of UDFArrayDetails\n\n\n\n\n\ncloud.array.ArrayList()\nThis class incrementally builds a list of UDFArrayDetails for use in multi array UDFs list[UDFArrayDetails]\n\n\n\n\n\nName\nDescription\n\n\n\n\nadd\nAdds an array to list\n\n\nget\nReturns the list of UDFArrayDetails\n\n\n\n\n\ncloud.array.ArrayList.add(uri=None, ranges=None, buffers=None, layout=None)\nAdds an array to list\n\n\n\ncloud.array.ArrayList.get()\nReturns the list of UDFArrayDetails\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\napply\nApply a user defined function to an array, synchronously.\n\n\napply_async\nApply a user-defined function to an array, asynchronously.\n\n\napply_base\nApply a user-defined function to an array, and return data and metadata.\n\n\narray_activity\nFetch array activity\n\n\ndelete_array\nDeregister the array from the tiledb cloud service,\n\n\nderegister_array\nDeregister the from the tiledb cloud service.\n\n\nexec_multi_array_udf\nApply a user-defined function to multiple arrays, synchronously.\n\n\nexec_multi_array_udf_async\nApply a user-defined function to multiple arrays, asynchronously.\n\n\nexec_multi_array_udf_base\nApply a user defined function to multiple arrays.\n\n\ninfo\nReturns the cloud metadata\n\n\nlist_shared_with\nReturn array sharing policies\n\n\nparse_ranges\nTakes a list of the following objects per dimension:\n\n\nregister_array\nRegister this array with the tiledb cloud service\n\n\nshare_array\nShares array with give namespace and permissions\n\n\nunshare_array\nRemoves sharing of an array from given namespace\n\n\nupdate_file_properties\nUpdate an Array to indicate its a file and has given properties.\n\n\nupdate_info\nUpdate an array’s info\n\n\n\n\n\ncloud.array.apply(*args, **kwargs)\nApply a user defined function to an array, synchronously.\nAll arguments are exactly as in :func:apply_base, but this returns the data only.\nExample:\n\n\n\nimport tiledb, tiledb.cloud, numpy def median(df): … return numpy.median(df[“a”]) # Open the array then run the UDF tiledb.cloud.array.apply(“tiledb://TileDB-Inc/quickstart_dense”, median, [(0,5), (0,5)], attrs=[“a”, “b”, “c”]) 2.0\n\n\n\n\n\n\ncloud.array.apply_async(*args, **kwargs)\nApply a user-defined function to an array, asynchronously.\nAll arguments are exactly as in :func:apply_base, but this returns the data as a future-like AsyncResponse.\n\n\n\ncloud.array.apply_base(\n    uri,\n    func=None,\n    ranges=(),\n    name=None,\n    attrs=(),\n    layout=None,\n    image_name='default',\n    http_compressor='deflate',\n    include_source_lines=True,\n    task_name=None,\n    v2=None,\n    result_format=models.ResultFormat.NATIVE,\n    result_format_version=None,\n    store_results=False,\n    stored_param_uuids=(),\n    timeout=None,\n    resource_class=None,\n    _download_results=True,\n    namespace=None,\n    _server_graph_uuid=None,\n    _client_node_uuid=None,\n    **kwargs,\n)\nApply a user-defined function to an array, and return data and metadata.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nThe tiledb://... URI of the array to apply the function to.\nrequired\n\n\nfunc\nUnion[str, Callable, None]\nThe function to run. This can be either a callable function, or the name of a registered user-defined function\nNone\n\n\nranges\nSequence\nranges to issue query on\n()\n\n\nname\nOptional[str]\nDeprecated. If func is None, the name of the registered user-defined function to call.\nNone\n\n\nattrs\nSequence\nlist of attributes or dimensions to fetch in query\n()\n\n\nlayout\nOptional[str]\ntiledb query layout\nNone\n\n\nimage_name\nstr\nudf image name to use, useful for testing beta features\n'default'\n\n\nhttp_compressor\nstr\nset http compressor for results\n'deflate'\n\n\ninclude_source_lines\nbool\nTrue to send the source code of your UDF to the server with your request. (This means it can be shown to you in stack traces if an error occurs.) False to send only compiled Python bytecode.\nTrue\n\n\ntask_name\nstr\noptional name to assign the task for logging and audit purposes\nNone\n\n\nv2\n\nIgnored.\nNone\n\n\nresult_format\nResultFormat\nresult serialization format\nmodels.ResultFormat.NATIVE\n\n\nresult_format_version\n\nDeprecated and ignored.\nNone\n\n\nstore_results\nbool\nTrue to temporarily store results on the server side for later retrieval (in addition to downloading them).\nFalse\n\n\ntimeout\nint\nTimeout for UDF in seconds\nNone\n\n\nresource_class\nOptional[str]\nThe name of the resource class to use. Resource classes define maximum limits for cpu and memory usage.\nNone\n\n\n_download_results\nbool\nTrue to download and parse results eagerly. False to not download results by default and only do so lazily (e.g. for an intermediate node in a graph).\nTrue\n\n\nnamespace\nOptional[str]\nThe namespace to execute the UDF under.\nNone\n\n\n_server_graph_uuid\nOptional[uuid.UUID]\nIf this function is being executed within a DAG, the server-generated ID of the graph’s log. Otherwise, None.\nNone\n\n\n_client_node_uuid\nOptional[uuid.UUID]\nIf this function is being executed within a DAG, the ID of this function’s node within the graph. Otherwise, None.\nNone\n\n\nkwargs\nAny\nnamed arguments to pass to function Example &gt;&gt;&gt; import tiledb, tiledb.cloud, numpy &gt;&gt;&gt; def median(df): … return numpy.median(df[“a”]) &gt;&gt;&gt; # Open the array then run the UDF &gt;&gt;&gt; tiledb.cloud.array.apply_base(“tiledb://TileDB-Inc/quickstart_dense”, median, [(0,5), (0,5)], attrs=[“a”, “b”, “c”]).result 2.0\n{}\n\n\n\n\n\n\n\ncloud.array.array_activity(uri, async_req=False)\nFetch array activity\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\n\n\nrequired\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncloud.array.delete_array(uri, *, async_req=False)\nDeregister the array from the tiledb cloud service, then deletes physical array from disk.\nAll access to the array and cloud metadata will be removed.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\ncloud.array.deregister_array(uri, async_req=False)\nDeregister the from the tiledb cloud service. This does not physically delete the array, it will remain in your bucket. All access to the array and cloud metadata will be removed.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\ncloud.array.exec_multi_array_udf(*args, **kwargs)\nApply a user-defined function to multiple arrays, synchronously.\nAll arguments are exactly as in :func:exec_multi_array_udf_base.\n\n\n\ncloud.array.exec_multi_array_udf_async(*args, **kwargs)\nApply a user-defined function to multiple arrays, asynchronously.\nAll arguments are exactly as in :func:exec_multi_array_udf_base.\n\n\n\ncloud.array.exec_multi_array_udf_base(\n    func=None,\n    array_list=None,\n    namespace=None,\n    name=None,\n    layout=None,\n    image_name='default',\n    http_compressor='deflate',\n    include_source_lines=True,\n    task_name=None,\n    result_format=models.ResultFormat.NATIVE,\n    result_format_version=None,\n    store_results=False,\n    stored_param_uuids=(),\n    resource_class=None,\n    _download_results=True,\n    _server_graph_uuid=None,\n    _client_node_uuid=None,\n    **kwargs,\n)\nApply a user defined function to multiple arrays.\n\n:param func: The function to run. This can be either a callable function,\n    or the name of a registered user-defined function\n:param array_list: The list of arrays to run the function on,\n    as an already-built ArrayList object.\n:param namespace: namespace to run udf under\n:param layout: Ignored.\n:param image_name: udf image name to use, useful for testing beta features\n:param http_compressor: set http compressor for results\n:param str task_name: optional name to assign the task\n    for logging and audit purposes\n:param ResultFormat result_format: result serialization format\n:param str result_format_version: Deprecated and ignored.\n:param store_results: True to temporarily store results on the server side\n    for later retrieval (in addition to downloading them).\n:param _server_graph_uuid: If this function is being executed within a DAG,\n    the server-generated ID of the graph's log. Otherwise, None.\n:param _client_node_uuid: If this function is being executed within a DAG,\n    the ID of this function's node within the graph. Otherwise, None.\n:param resource_class: The name of the resource class to use. Resource classes\n    define maximum limits for cpu and memory usage.\n:param kwargs: named arguments to pass to function\n:return: A future containing the results of the UDF.\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from tiledb.cloud import array\n&gt;&gt;&gt; import tiledb.cloud\n&gt;&gt;&gt; dense_array = \"tiledb://andreas/quickstart_dense_local\"\n&gt;&gt;&gt; sparse_array = \"tiledb://andreas/quickstart_sparse_local\"\n&gt;&gt;&gt; def median(numpy_ordered_dictionary):\n...    return np.median(numpy_ordered_dictionary[0][\"a\"]) + np.median(numpy_ordered_dictionary[1][\"a\"])\n&gt;&gt;&gt; array_list = array.ArrayList()\n&gt;&gt;&gt; array_list.add(dense_array, [(1, 4), (1, 4)], [\"a\"])\n&gt;&gt;&gt; array_list.add(sparse_array, [(1, 2), (1, 4)], [\"a\"])\n&gt;&gt;&gt; namespace = \"namespace\"\n&gt;&gt;&gt; res = array.exec_multi_array_udf(median, array_list, namespace)\n&gt;&gt;&gt; print(\"Median Multi UDF:\n{} “.format(res))\n\n\n\ncloud.array.info(uri, async_req=False)\nReturns the cloud metadata\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nmetadata object\n\n\n\n\n\n\n\ncloud.array.list_shared_with(uri, async_req=False)\nReturn array sharing policies\n\n\n\ncloud.array.parse_ranges(ranges)\nTakes a list of the following objects per dimension:\n\nscalar index\n(start,end) tuple\nlist of either of the above types\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nranges\n\nlist of (scalar, tuple, list)\nrequired\n\n\nbuilder\n\nfunction taking arguments (dim_idx, start, end)\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncloud.array.register_array(\n    uri,\n    namespace=None,\n    array_name=None,\n    description=None,\n    access_credentials_name=None,\n    async_req=False,\n    dest_uri=None,\n)\nRegister this array with the tiledb cloud service\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nstorage URI of array.\nrequired\n\n\nnamespace\nstr\nThe user or organization to register the array under. If unset will default to the user\nNone\n\n\narray_name\nstr\nname of array, optional. Defaults to stem of the storage URI.\nNone\n\n\ndescription\nstr\noptional description\nNone\n\n\naccess_credentials_name\nstr\noptional name of access credentials to use. If left blank default for namespace will be used.\nNone\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\ndest_uri\nOptional[str]\nIf set, the tiledb:// URI of the destination.\nNone\n\n\n\n\n\n\n\ncloud.array.share_array(uri, namespace, permissions, async_req=False)\nShares array with give namespace and permissions\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnamespace\nstr\n\nrequired\n\n\npermissions\nlist(str)\n\nrequired\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncloud.array.unshare_array(uri, namespace, async_req=False)\nRemoves sharing of an array from given namespace\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnamespace\nstr\nnamespace to remove shared access to the array\nrequired\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncloud.array.update_file_properties(\n    uri,\n    file_type=None,\n    file_properties=None,\n    async_req=False,\n)\nUpdate an Array to indicate its a file and has given properties. Any properties set are returned with the array info.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nuri of array to update\nrequired\n\n\nfile_type\nstr\nfile type to set\nNone\n\n\nfile_properties\ndict\ndictionary of properties to set\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncloud.array.update_info(\n    uri,\n    array_name=None,\n    description=None,\n    access_credentials_name=None,\n    tags=None,\n    async_req=False,\n)\nUpdate an array’s info\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnamespace\nstr\nThe username or organization that owns the array. If unset, will use the logged-in user.\nrequired\n\n\narray_name\nstr\nname of array to rename to\nNone\n\n\ndescription\nstr\noptional description\nNone\n\n\naccess_credentials_name\nstr\nThe access credentials to use when accessing the backing array. Leave unset to not change.\nNone\n\n\ntags\nlist\nto update to\nNone\n\n\nfile_type\nstr\narray represents give file type\nrequired\n\n\nfile_properties\nstr\nset file properties on array\nrequired\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse",
    "crumbs": [
      "Get Started",
      "Catalog",
      "array"
    ]
  },
  {
    "objectID": "reference/array.html#classes",
    "href": "reference/array.html#classes",
    "title": "array",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nArrayList\nThis class incrementally builds a list of UDFArrayDetails\n\n\n\n\n\ncloud.array.ArrayList()\nThis class incrementally builds a list of UDFArrayDetails for use in multi array UDFs list[UDFArrayDetails]\n\n\n\n\n\nName\nDescription\n\n\n\n\nadd\nAdds an array to list\n\n\nget\nReturns the list of UDFArrayDetails\n\n\n\n\n\ncloud.array.ArrayList.add(uri=None, ranges=None, buffers=None, layout=None)\nAdds an array to list\n\n\n\ncloud.array.ArrayList.get()\nReturns the list of UDFArrayDetails",
    "crumbs": [
      "Get Started",
      "Catalog",
      "array"
    ]
  },
  {
    "objectID": "reference/array.html#functions",
    "href": "reference/array.html#functions",
    "title": "array",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\napply\nApply a user defined function to an array, synchronously.\n\n\napply_async\nApply a user-defined function to an array, asynchronously.\n\n\napply_base\nApply a user-defined function to an array, and return data and metadata.\n\n\narray_activity\nFetch array activity\n\n\ndelete_array\nDeregister the array from the tiledb cloud service,\n\n\nderegister_array\nDeregister the from the tiledb cloud service.\n\n\nexec_multi_array_udf\nApply a user-defined function to multiple arrays, synchronously.\n\n\nexec_multi_array_udf_async\nApply a user-defined function to multiple arrays, asynchronously.\n\n\nexec_multi_array_udf_base\nApply a user defined function to multiple arrays.\n\n\ninfo\nReturns the cloud metadata\n\n\nlist_shared_with\nReturn array sharing policies\n\n\nparse_ranges\nTakes a list of the following objects per dimension:\n\n\nregister_array\nRegister this array with the tiledb cloud service\n\n\nshare_array\nShares array with give namespace and permissions\n\n\nunshare_array\nRemoves sharing of an array from given namespace\n\n\nupdate_file_properties\nUpdate an Array to indicate its a file and has given properties.\n\n\nupdate_info\nUpdate an array’s info\n\n\n\n\n\ncloud.array.apply(*args, **kwargs)\nApply a user defined function to an array, synchronously.\nAll arguments are exactly as in :func:apply_base, but this returns the data only.\nExample:\n\n\n\nimport tiledb, tiledb.cloud, numpy def median(df): … return numpy.median(df[“a”]) # Open the array then run the UDF tiledb.cloud.array.apply(“tiledb://TileDB-Inc/quickstart_dense”, median, [(0,5), (0,5)], attrs=[“a”, “b”, “c”]) 2.0\n\n\n\n\n\n\ncloud.array.apply_async(*args, **kwargs)\nApply a user-defined function to an array, asynchronously.\nAll arguments are exactly as in :func:apply_base, but this returns the data as a future-like AsyncResponse.\n\n\n\ncloud.array.apply_base(\n    uri,\n    func=None,\n    ranges=(),\n    name=None,\n    attrs=(),\n    layout=None,\n    image_name='default',\n    http_compressor='deflate',\n    include_source_lines=True,\n    task_name=None,\n    v2=None,\n    result_format=models.ResultFormat.NATIVE,\n    result_format_version=None,\n    store_results=False,\n    stored_param_uuids=(),\n    timeout=None,\n    resource_class=None,\n    _download_results=True,\n    namespace=None,\n    _server_graph_uuid=None,\n    _client_node_uuid=None,\n    **kwargs,\n)\nApply a user-defined function to an array, and return data and metadata.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nThe tiledb://... URI of the array to apply the function to.\nrequired\n\n\nfunc\nUnion[str, Callable, None]\nThe function to run. This can be either a callable function, or the name of a registered user-defined function\nNone\n\n\nranges\nSequence\nranges to issue query on\n()\n\n\nname\nOptional[str]\nDeprecated. If func is None, the name of the registered user-defined function to call.\nNone\n\n\nattrs\nSequence\nlist of attributes or dimensions to fetch in query\n()\n\n\nlayout\nOptional[str]\ntiledb query layout\nNone\n\n\nimage_name\nstr\nudf image name to use, useful for testing beta features\n'default'\n\n\nhttp_compressor\nstr\nset http compressor for results\n'deflate'\n\n\ninclude_source_lines\nbool\nTrue to send the source code of your UDF to the server with your request. (This means it can be shown to you in stack traces if an error occurs.) False to send only compiled Python bytecode.\nTrue\n\n\ntask_name\nstr\noptional name to assign the task for logging and audit purposes\nNone\n\n\nv2\n\nIgnored.\nNone\n\n\nresult_format\nResultFormat\nresult serialization format\nmodels.ResultFormat.NATIVE\n\n\nresult_format_version\n\nDeprecated and ignored.\nNone\n\n\nstore_results\nbool\nTrue to temporarily store results on the server side for later retrieval (in addition to downloading them).\nFalse\n\n\ntimeout\nint\nTimeout for UDF in seconds\nNone\n\n\nresource_class\nOptional[str]\nThe name of the resource class to use. Resource classes define maximum limits for cpu and memory usage.\nNone\n\n\n_download_results\nbool\nTrue to download and parse results eagerly. False to not download results by default and only do so lazily (e.g. for an intermediate node in a graph).\nTrue\n\n\nnamespace\nOptional[str]\nThe namespace to execute the UDF under.\nNone\n\n\n_server_graph_uuid\nOptional[uuid.UUID]\nIf this function is being executed within a DAG, the server-generated ID of the graph’s log. Otherwise, None.\nNone\n\n\n_client_node_uuid\nOptional[uuid.UUID]\nIf this function is being executed within a DAG, the ID of this function’s node within the graph. Otherwise, None.\nNone\n\n\nkwargs\nAny\nnamed arguments to pass to function Example &gt;&gt;&gt; import tiledb, tiledb.cloud, numpy &gt;&gt;&gt; def median(df): … return numpy.median(df[“a”]) &gt;&gt;&gt; # Open the array then run the UDF &gt;&gt;&gt; tiledb.cloud.array.apply_base(“tiledb://TileDB-Inc/quickstart_dense”, median, [(0,5), (0,5)], attrs=[“a”, “b”, “c”]).result 2.0\n{}\n\n\n\n\n\n\n\ncloud.array.array_activity(uri, async_req=False)\nFetch array activity\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\n\n\nrequired\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncloud.array.delete_array(uri, *, async_req=False)\nDeregister the array from the tiledb cloud service, then deletes physical array from disk.\nAll access to the array and cloud metadata will be removed.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\ncloud.array.deregister_array(uri, async_req=False)\nDeregister the from the tiledb cloud service. This does not physically delete the array, it will remain in your bucket. All access to the array and cloud metadata will be removed.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\ncloud.array.exec_multi_array_udf(*args, **kwargs)\nApply a user-defined function to multiple arrays, synchronously.\nAll arguments are exactly as in :func:exec_multi_array_udf_base.\n\n\n\ncloud.array.exec_multi_array_udf_async(*args, **kwargs)\nApply a user-defined function to multiple arrays, asynchronously.\nAll arguments are exactly as in :func:exec_multi_array_udf_base.\n\n\n\ncloud.array.exec_multi_array_udf_base(\n    func=None,\n    array_list=None,\n    namespace=None,\n    name=None,\n    layout=None,\n    image_name='default',\n    http_compressor='deflate',\n    include_source_lines=True,\n    task_name=None,\n    result_format=models.ResultFormat.NATIVE,\n    result_format_version=None,\n    store_results=False,\n    stored_param_uuids=(),\n    resource_class=None,\n    _download_results=True,\n    _server_graph_uuid=None,\n    _client_node_uuid=None,\n    **kwargs,\n)\nApply a user defined function to multiple arrays.\n\n:param func: The function to run. This can be either a callable function,\n    or the name of a registered user-defined function\n:param array_list: The list of arrays to run the function on,\n    as an already-built ArrayList object.\n:param namespace: namespace to run udf under\n:param layout: Ignored.\n:param image_name: udf image name to use, useful for testing beta features\n:param http_compressor: set http compressor for results\n:param str task_name: optional name to assign the task\n    for logging and audit purposes\n:param ResultFormat result_format: result serialization format\n:param str result_format_version: Deprecated and ignored.\n:param store_results: True to temporarily store results on the server side\n    for later retrieval (in addition to downloading them).\n:param _server_graph_uuid: If this function is being executed within a DAG,\n    the server-generated ID of the graph's log. Otherwise, None.\n:param _client_node_uuid: If this function is being executed within a DAG,\n    the ID of this function's node within the graph. Otherwise, None.\n:param resource_class: The name of the resource class to use. Resource classes\n    define maximum limits for cpu and memory usage.\n:param kwargs: named arguments to pass to function\n:return: A future containing the results of the UDF.\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from tiledb.cloud import array\n&gt;&gt;&gt; import tiledb.cloud\n&gt;&gt;&gt; dense_array = \"tiledb://andreas/quickstart_dense_local\"\n&gt;&gt;&gt; sparse_array = \"tiledb://andreas/quickstart_sparse_local\"\n&gt;&gt;&gt; def median(numpy_ordered_dictionary):\n...    return np.median(numpy_ordered_dictionary[0][\"a\"]) + np.median(numpy_ordered_dictionary[1][\"a\"])\n&gt;&gt;&gt; array_list = array.ArrayList()\n&gt;&gt;&gt; array_list.add(dense_array, [(1, 4), (1, 4)], [\"a\"])\n&gt;&gt;&gt; array_list.add(sparse_array, [(1, 2), (1, 4)], [\"a\"])\n&gt;&gt;&gt; namespace = \"namespace\"\n&gt;&gt;&gt; res = array.exec_multi_array_udf(median, array_list, namespace)\n&gt;&gt;&gt; print(\"Median Multi UDF:\n{} “.format(res))\n\n\n\ncloud.array.info(uri, async_req=False)\nReturns the cloud metadata\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nmetadata object\n\n\n\n\n\n\n\ncloud.array.list_shared_with(uri, async_req=False)\nReturn array sharing policies\n\n\n\ncloud.array.parse_ranges(ranges)\nTakes a list of the following objects per dimension:\n\nscalar index\n(start,end) tuple\nlist of either of the above types\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nranges\n\nlist of (scalar, tuple, list)\nrequired\n\n\nbuilder\n\nfunction taking arguments (dim_idx, start, end)\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncloud.array.register_array(\n    uri,\n    namespace=None,\n    array_name=None,\n    description=None,\n    access_credentials_name=None,\n    async_req=False,\n    dest_uri=None,\n)\nRegister this array with the tiledb cloud service\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nstorage URI of array.\nrequired\n\n\nnamespace\nstr\nThe user or organization to register the array under. If unset will default to the user\nNone\n\n\narray_name\nstr\nname of array, optional. Defaults to stem of the storage URI.\nNone\n\n\ndescription\nstr\noptional description\nNone\n\n\naccess_credentials_name\nstr\noptional name of access credentials to use. If left blank default for namespace will be used.\nNone\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\ndest_uri\nOptional[str]\nIf set, the tiledb:// URI of the destination.\nNone\n\n\n\n\n\n\n\ncloud.array.share_array(uri, namespace, permissions, async_req=False)\nShares array with give namespace and permissions\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnamespace\nstr\n\nrequired\n\n\npermissions\nlist(str)\n\nrequired\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncloud.array.unshare_array(uri, namespace, async_req=False)\nRemoves sharing of an array from given namespace\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnamespace\nstr\nnamespace to remove shared access to the array\nrequired\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncloud.array.update_file_properties(\n    uri,\n    file_type=None,\n    file_properties=None,\n    async_req=False,\n)\nUpdate an Array to indicate its a file and has given properties. Any properties set are returned with the array info.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nuri of array to update\nrequired\n\n\nfile_type\nstr\nfile type to set\nNone\n\n\nfile_properties\ndict\ndictionary of properties to set\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncloud.array.update_info(\n    uri,\n    array_name=None,\n    description=None,\n    access_credentials_name=None,\n    tags=None,\n    async_req=False,\n)\nUpdate an array’s info\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnamespace\nstr\nThe username or organization that owns the array. If unset, will use the logged-in user.\nrequired\n\n\narray_name\nstr\nname of array to rename to\nNone\n\n\ndescription\nstr\noptional description\nNone\n\n\naccess_credentials_name\nstr\nThe access credentials to use when accessing the backing array. Leave unset to not change.\nNone\n\n\ntags\nlist\nto update to\nNone\n\n\nfile_type\nstr\narray represents give file type\nrequired\n\n\nfile_properties\nstr\nset file properties on array\nrequired\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse",
    "crumbs": [
      "Get Started",
      "Catalog",
      "array"
    ]
  },
  {
    "objectID": "reference/vcf.ingestion.html",
    "href": "reference/vcf.ingestion.html",
    "title": "vcf.ingestion",
    "section": "",
    "text": "cloud.vcf.ingestion\n\n\n\n\n\nName\nDescription\n\n\n\n\nContigs\nThe contigs to ingest.\n\n\n\n\n\ncloud.vcf.ingestion.Contigs()\nThe contigs to ingest.\nALL = all contigs CHROMOSOMES = all human chromosomes OTHER = all contigs other than the human chromosomes ALL_DISABLE_MERGE = all contigs with merging disabled, for non-human datasets\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nconsolidate_dataset_udf\nConsolidate arrays in the dataset.\n\n\ncreate_dataset_udf\nCreate a TileDB-VCF dataset.\n\n\ncreate_manifest\nCreate a manifest array in the dataset.\n\n\nfilter_samples_udf\nReturn URIs for samples not already in the dataset.\n\n\nfilter_uris_udf\nReturn URIs from sample_uris that are not in the manifest.\n\n\nfind_uris_aws_udf\nFind URIs matching a pattern in the search_uri path with an efficient\n\n\nfind_uris_udf\nFind URIs matching a pattern in the search_uri path.\n\n\nget_logger_wrapper\nGet a logger instance and log version information.\n\n\ningest_manifest_dag\nCreate a DAG to load the manifest array.\n\n\ningest_manifest_udf\nIngest sample URIs into the manifest array.\n\n\ningest_samples_dag\nCreate a DAG to ingest samples into the dataset.\n\n\ningest_samples_udf\nIngest samples into the dataset.\n\n\ningest_vcf\nIngest samples into a dataset.\n\n\ningest_vcf_annotations\nIngest annotation VCF into a dataset. For example, a ClinVar or gnomAD VCF.\n\n\nread_metadata_uris_udf\nRead a list of URIs from a TileDB array. The URIs will be read from the\n\n\nread_uris_udf\nRead a list of URIs from a URI.\n\n\nregister_dataset_udf\nRegister the dataset on TileDB Cloud.\n\n\n\n\n\ncloud.vcf.ingestion.consolidate_dataset_udf(\n    dataset_uri,\n    *,\n    config=None,\n    exclude=MANIFEST_ARRAY,\n    include=None,\n    id='consolidate',\n    verbose=False,\n)\nConsolidate arrays in the dataset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\nexclude\nOptional[Union[Sequence[str], str]]\ngroup members to exclude, defaults to MANIFEST_ARRAY\nMANIFEST_ARRAY\n\n\ninclude\nOptional[Union[Sequence[str], str]]\ngroup members to include, defaults to None\nNone\n\n\nid\nstr\nprofiler event id, defaults to “consolidate”\n'consolidate'\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\ncloud.vcf.ingestion.create_dataset_udf(\n    dataset_uri,\n    *,\n    config=None,\n    extra_attrs=None,\n    vcf_attrs=None,\n    anchor_gap=None,\n    compression_level=None,\n    annotation_dataset=False,\n    verbose=False,\n)\nCreate a TileDB-VCF dataset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\nextra_attrs\nOptional[Union[Sequence[str], str]]\nINFO/FORMAT fields to materialize, defaults to None\nNone\n\n\nvcf_attrs\nOptional[str]\nVCF with all INFO/FORMAT fields to materialize, defaults to None\nNone\n\n\nanchor_gap\nOptional[int]\nanchor gap for VCF dataset, defaults to None\nNone\n\n\ncompression_level\nOptional[int]\nzstd compression level for the VCF dataset, defaults to None (uses the default level in TileDB-VCF)\nNone\n\n\nannotation_dataset\nbool\ncreate an annotation dataset, defaults to False\nFalse\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\ndataset URI\n\n\n\n\n\n\n\ncloud.vcf.ingestion.create_manifest(dataset_uri, group)\nCreate a manifest array in the dataset.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\ngroup\ntiledb.Group\ndataset group\nrequired\n\n\n\n\n\n\n\ncloud.vcf.ingestion.filter_samples_udf(\n    dataset_uri,\n    *,\n    config=None,\n    verbose=False,\n)\nReturn URIs for samples not already in the dataset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nSequence[str]\nsample URIs\n\n\n\n\n\n\n\ncloud.vcf.ingestion.filter_uris_udf(\n    dataset_uri,\n    sample_uris,\n    *,\n    config=None,\n    verbose=False,\n)\nReturn URIs from sample_uris that are not in the manifest.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nsample_uris\nSequence[str]\nsample URIs\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nSequence[str]\nfiltered sample URIs\n\n\n\n\n\n\n\ncloud.vcf.ingestion.find_uris_aws_udf(\n    dataset_uri,\n    search_uri,\n    *,\n    config=None,\n    include=None,\n    exclude=None,\n    max_files=None,\n    verbose=False,\n)\nFind URIs matching a pattern in the search_uri path with an efficient implementation for S3.\ninclude and exclude patterns are Unix shell style (see fnmatch module).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nsearch_uri\nstr\nURI to search for VCF files\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\ninclude\nOptional[str]\ninclude pattern used in the search, defaults to None\nNone\n\n\nexclude\nOptional[str]\nexclude pattern applied to the search results, defaults to None\nNone\n\n\nmax_files\nOptional[int]\nmaximum number of URIs returned, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nSequence[str]\nlist of URIs\n\n\n\n\n\n\n\ncloud.vcf.ingestion.find_uris_udf(\n    dataset_uri,\n    search_uri,\n    *,\n    config=None,\n    include=None,\n    exclude=None,\n    max_files=None,\n    verbose=False,\n)\nFind URIs matching a pattern in the search_uri path.\ninclude and exclude patterns are Unix shell style (see fnmatch module).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nsearch_uri\nstr\nURI to search for VCF files\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\ninclude\nOptional[str]\ninclude pattern used in the search, defaults to None\nNone\n\n\nexclude\nOptional[str]\nexclude pattern applied to the search results, defaults to None\nNone\n\n\nmax_files\nOptional[int]\nmaximum number of URIs returned, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nSequence[str]\nlist of URIs\n\n\n\n\n\n\n\ncloud.vcf.ingestion.get_logger_wrapper(verbose=False)\nGet a logger instance and log version information.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlogging.Logger\nlogger instance\n\n\n\n\n\n\n\ncloud.vcf.ingestion.ingest_manifest_dag(\n    dataset_uri,\n    *,\n    acn=None,\n    config=None,\n    namespace=None,\n    search_uri=None,\n    pattern=None,\n    ignore=None,\n    sample_list_uri=None,\n    metadata_uri=None,\n    metadata_attr='uri',\n    max_files=None,\n    batch_size=MANIFEST_BATCH_SIZE,\n    workers=MANIFEST_WORKERS,\n    extra_attrs=None,\n    vcf_attrs=None,\n    anchor_gap=None,\n    compression_level=None,\n    verbose=False,\n    aws_find_mode=False,\n    disable_manifest=False,\n    consolidate_resources=CONSOLIDATE_RESOURCES,\n    manifest_resources=MANIFEST_RESOURCES,\n)\nCreate a DAG to load the manifest array.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nacn\nOptional[str]\nAccess Credentials Name (ACN) registered in TileDB Cloud (ARN type), defaults to None\nNone\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\nnamespace\nOptional[str]\nTileDB-Cloud namespace, defaults to None\nNone\n\n\nsearch_uri\nOptional[str]\nURI to search for VCF files, defaults to None\nNone\n\n\npattern\nOptional[str]\npattern to match when searching for VCF files, defaults to None\nNone\n\n\nignore\nOptional[str]\npattern to ignore when searching for VCF files, defaults to None\nNone\n\n\nsample_list_uri\nOptional[str]\nURI with a list of VCF URIs, defaults to None\nNone\n\n\nmetadata_uri\nOptional[str]\nURI of metadata array holding VCF URIs, defaults to None\nNone\n\n\nmetadata_attr\nstr\nname of metadata attribute containing URIs, defaults to “uri”\n'uri'\n\n\nmax_files\nOptional[int]\nmaximum number of URIs to ingest, defaults to None\nNone\n\n\nbatch_size\nint\nmanifest batch size, defaults to MANIFEST_BATCH_SIZE\nMANIFEST_BATCH_SIZE\n\n\nworkers\nint\nmaximum number of parallel workers, defaults to MANIFEST_WORKERS\nMANIFEST_WORKERS\n\n\nextra_attrs\nOptional[Union[Sequence[str], str]]\nINFO/FORMAT fields to materialize, defaults to None\nNone\n\n\nvcf_attrs\nOptional[str]\nVCF with all INFO/FORMAT fields to materialize, defaults to None\nNone\n\n\nanchor_gap\nOptional[int]\nanchor gap for VCF dataset, defaults to None\nNone\n\n\ncompression_level\nOptional[int]\nzstd compression level for the VCF dataset, defaults to None (uses the default level in TileDB-VCF)\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\naws_find_mode\nbool\nuse AWS CLI to find VCFs, defaults to False\nFalse\n\n\ndisable_manifest\nbool\ndisable manifest creation, defaults to False\nFalse\n\n\nconsolidate_resources\nOptional[Mapping[str, str]]\nmanual override for consolidate UDF resources, defaults to CONSOLIDATE_RESOURCES\nCONSOLIDATE_RESOURCES\n\n\nmanifest_resources\nOptional[Mapping[str, str]]\nmanual override for manifest UDF resources, defaults to MANIFEST_RESOURCES\nMANIFEST_RESOURCES\n\n\n\n\n\n\n\ncloud.vcf.ingestion.ingest_manifest_udf(\n    dataset_uri,\n    sample_uris,\n    *,\n    config=None,\n    id='manifest',\n    verbose=False,\n)\nIngest sample URIs into the manifest array.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nsample_uris\nSequence[str]\nsample URIs\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\nid\nstr\nprofiler event id, defaults to “manifest”\n'manifest'\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\ncloud.vcf.ingestion.ingest_samples_dag(\n    dataset_uri,\n    *,\n    acn=None,\n    config=None,\n    namespace=None,\n    contigs=Contigs.ALL,\n    threads=VCF_THREADS,\n    batch_size=VCF_BATCH_SIZE,\n    workers=VCF_WORKERS,\n    max_samples=None,\n    resume=True,\n    ingest_resources=None,\n    verbose=False,\n    create_index=True,\n    trace_id=None,\n    consolidate_stats=False,\n    use_remote_tmp=False,\n    sample_list_uri=None,\n    consolidate_resources=CONSOLIDATE_RESOURCES,\n)\nCreate a DAG to ingest samples into the dataset.\nNote: If sample_list_uri is provided, the manifest is not checked for existing samples.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nacn\nOptional[str]\nAccess Credentials Name (ACN) registered in TileDB Cloud (ARN type), defaults to None\nNone\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\nnamespace\nOptional[str]\nTileDB-Cloud namespace, defaults to None\nNone\n\n\ncontigs\nOptional[Union[Sequence[str], Contigs]]\ncontig mode (Contigs.ALL | Contigs.CHROMOSOMES | Contigs.OTHER | Contigs.ALL_DISABLE_MERGE) or list of contigs to ingest, defaults to Contigs.ALL\nContigs.ALL\n\n\nthreads\nint\nnumber of threads to use per ingestion task, defaults to VCF_THREADS\nVCF_THREADS\n\n\nbatch_size\nint\nsample batch size, defaults to VCF_BATCH_SIZE\nVCF_BATCH_SIZE\n\n\nworkers\nint\nmaximum number of parallel workers, defaults to VCF_WORKERS\nVCF_WORKERS\n\n\nmax_samples\nOptional[int]\nmaximum number of samples to ingest, defaults to None (no limit)\nNone\n\n\nresume\nbool\nenable resume ingestion mode, defaults to True\nTrue\n\n\ningest_resources\nOptional[Mapping[str, str]]\nmanual override for ingest UDF resources, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\ncreate_index\nbool\nforce creation of a local index file, defaults to True\nTrue\n\n\ntrace_id\nOptional[str]\ntrace ID for logging, defaults to None\nNone\n\n\nconsolidate_stats\nbool\nconsolidate the stats arrays, defaults to False\nFalse\n\n\nuse_remote_tmp\nbool\nuse remote tmp space if VCFs need to be bgzipped, defaults to False (preferred for small VCFs)\nFalse\n\n\nsample_list_uri\nOptional[str]\nURI with a list of VCF URIs, defaults to None\nNone\n\n\nconsolidate_resources\nOptional[Mapping[str, str]]\nmanual override for consolidate UDF resources, defaults to None\nCONSOLIDATE_RESOURCES\n\n\n\n\n\n\n\ncloud.vcf.ingestion.ingest_samples_udf(\n    dataset_uri,\n    sample_uris,\n    *,\n    config=None,\n    threads,\n    memory_mb,\n    sample_batch_size,\n    contig_mode='all',\n    contigs_to_keep_separate=None,\n    contig_fragment_merging=True,\n    resume=True,\n    create_index=True,\n    id='samples',\n    verbose=False,\n    trace_id=None,\n    use_remote_tmp=False,\n)\nIngest samples into the dataset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nsample_uris\nSequence[str]\nsample URIs\nrequired\n\n\nthreads\nint\nnumber of threads to use for ingestion\nrequired\n\n\nmemory_mb\nint\nmemory to use for ingestion in MiB\nrequired\n\n\nsample_batch_size\nint\nsample batch size to use for ingestion\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\ncontig_mode\nstr\ningestion mode, defaults to “all”\n'all'\n\n\ncontigs_to_keep_separate\nOptional[Sequence[str]]\nlist of contigs to keep separate, defaults to None\nNone\n\n\ncontig_fragment_merging\nbool\nenable contig fragment merging, defaults to True\nTrue\n\n\nresume\nbool\nenable resume ingestion mode, defaults to True\nTrue\n\n\ncreate_index\nbool\nforce creation of a local index file, defaults to True\nTrue\n\n\nid\nstr\nprofiler event id, defaults to “samples”\n'samples'\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\ntrace_id\nOptional[str]\ntrace ID for logging, defaults to None\nNone\n\n\nuse_remote_tmp\nbool\nuse remote tmp space if VCFs need to be bgzipped, defaults to False (preferred for small VCFs)\nFalse\n\n\n\n\n\n\n\ncloud.vcf.ingestion.ingest_vcf(\n    dataset_uri,\n    *,\n    acn=None,\n    config=None,\n    namespace=None,\n    register_name=None,\n    search_uri=None,\n    pattern=None,\n    ignore=None,\n    sample_list_uri=None,\n    metadata_uri=None,\n    metadata_attr='uri',\n    max_files=None,\n    max_samples=None,\n    contigs=Contigs.ALL,\n    resume=True,\n    extra_attrs=DEFAULT_ATTRIBUTES,\n    vcf_attrs=None,\n    anchor_gap=None,\n    compression_level=None,\n    manifest_batch_size=MANIFEST_BATCH_SIZE,\n    manifest_workers=MANIFEST_WORKERS,\n    vcf_batch_size=VCF_BATCH_SIZE,\n    vcf_workers=VCF_WORKERS,\n    vcf_threads=VCF_THREADS,\n    ingest_resources=None,\n    verbose=False,\n    create_index=True,\n    trace_id=None,\n    consolidate_stats=True,\n    aws_find_mode=False,\n    use_remote_tmp=False,\n    disable_manifest=False,\n    consolidate_resources=CONSOLIDATE_RESOURCES,\n    manifest_resources=MANIFEST_RESOURCES,\n)\nIngest samples into a dataset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nacn\nOptional[str]\nAccess Credentials Name (ACN) registered in TileDB Cloud (ARN type), defaults to None\nNone\n\n\nconfig\n\nconfig dictionary, defaults to None\nNone\n\n\nnamespace\nOptional[str]\nTileDB-Cloud namespace, defaults to None\nNone\n\n\nregister_name\nOptional[str]\nname to register the dataset with on TileDB Cloud, defaults to None\nNone\n\n\nsearch_uri\nOptional[str]\nURI to search for VCF files, defaults to None\nNone\n\n\npattern\nOptional[str]\nUnix shell style pattern to match when searching for VCF files, defaults to None\nNone\n\n\nignore\nOptional[str]\nUnix shell style pattern to ignore when searching for VCF files, defaults to None\nNone\n\n\nsample_list_uri\nOptional[str]\nURI with a list of VCF URIs, defaults to None\nNone\n\n\nmetadata_uri\nOptional[str]\nURI of metadata array holding VCF URIs, defaults to None\nNone\n\n\nmetadata_attr\nstr\nname of metadata attribute containing URIs, defaults to “uri”\n'uri'\n\n\nmax_files\nOptional[int]\nmaximum number of VCF URIs to read/find, defaults to None (no limit)\nNone\n\n\nmax_samples\nOptional[int]\nmaximum number of samples to ingest, defaults to None (no limit)\nNone\n\n\ncontigs\nOptional[Union[Sequence[str], Contigs]]\ncontig mode (Contigs.ALL | Contigs.CHROMOSOMES | Contigs.OTHER | Contigs.ALL_DISABLE_MERGE) or list of contigs to ingest, defaults to Contigs.ALL\nContigs.ALL\n\n\nresume\nbool\nenable resume ingestion mode, defaults to True\nTrue\n\n\nextra_attrs\nOptional[Union[Sequence[str], str]]\nINFO/FORMAT fields to materialize, defaults to repr(DEFAULT_ATTRIBUTES)\nDEFAULT_ATTRIBUTES\n\n\nvcf_attrs\nOptional[str]\nVCF with all INFO/FORMAT fields to materialize, defaults to None\nNone\n\n\nanchor_gap\nOptional[int]\nanchor gap for VCF dataset, defaults to None\nNone\n\n\ncompression_level\nOptional[int]\nzstd compression level for the VCF dataset, defaults to None (uses the default level in TileDB-VCF)\nNone\n\n\nmanifest_batch_size\nint\nbatch size for manifest ingestion, defaults to MANIFEST_BATCH_SIZE\nMANIFEST_BATCH_SIZE\n\n\nmanifest_workers\nint\nnumber of workers for manifest ingestion, defaults to MANIFEST_WORKERS\nMANIFEST_WORKERS\n\n\nvcf_batch_size\nint\nbatch size for VCF ingestion, defaults to VCF_BATCH_SIZE\nVCF_BATCH_SIZE\n\n\nvcf_workers\nint\nnumber of workers for VCF ingestion, defaults to VCF_WORKERS\nVCF_WORKERS\n\n\nvcf_threads\nint\nnumber of threads for VCF ingestion, defaults to VCF_THREADS\nVCF_THREADS\n\n\ningest_resources\nOptional[Mapping[str, str]]\nmanual override for ingest UDF resources, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\ncreate_index\nbool\nforce creation of a local index file, defaults to True\nTrue\n\n\ntrace_id\nOptional[str]\ntrace ID for logging, defaults to None\nNone\n\n\nconsolidate_stats\nbool\nconsolidate the stats arrays, defaults to True\nTrue\n\n\naws_find_mode\nbool\nuse AWS CLI to find VCFs, defaults to False\nFalse\n\n\nuse_remote_tmp\nbool\nuse remote tmp space if VCFs need to be sorted and bgzipped, defaults to False (preferred for small VCFs)\nFalse\n\n\ndisable_manifest\nbool\ndisable manifest creation, defaults to False\nFalse\n\n\nconsolidate_resources\nOptional[Mapping[str, str]]\nmanual override for consolidate UDF resources, defaults to CONSOLIDATE_RESOURCES\nCONSOLIDATE_RESOURCES\n\n\nmanifest_resources\nOptional[Mapping[str, str]]\nmanual override for manifest UDF resources, defaults to MANIFEST_RESOURCES\nMANIFEST_RESOURCES\n\n\n\n\n\n\n\ncloud.vcf.ingestion.ingest_vcf_annotations(\n    dataset_uri,\n    *,\n    vcf_uri=None,\n    search_uri=None,\n    pattern=None,\n    ignore=None,\n    create_index=True,\n    config=None,\n    acn=None,\n    namespace=None,\n    register_name=None,\n    ingest_resources=None,\n    verbose=False,\n    consolidate_resources=CONSOLIDATE_RESOURCES,\n)\nIngest annotation VCF into a dataset. For example, a ClinVar or gnomAD VCF.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nvcf_uri\nOptional[str]\nVCF URI, defaults to None\nNone\n\n\nsearch_uri\nOptional[str]\nURI to search for VCF files, defaults to None\nNone\n\n\npattern\nOptional[str]\nUnix shell style pattern to match when searching for VCF files, defaults to None\nNone\n\n\nignore\nOptional[str]\nUnix shell style pattern to ignore when searching for VCF files, defaults to None\nNone\n\n\ncreate_index\nbool\nforce creation of a local index file, defaults to True\nTrue\n\n\nconfig\n\nconfig dictionary, defaults to None\nNone\n\n\nacn\nOptional[str]\nAccess Credentials Name (ACN) registered in TileDB Cloud (ARN type), defaults to None\nNone\n\n\nnamespace\nOptional[str]\nTileDB-Cloud namespace, defaults to None\nNone\n\n\nregister_name\nOptional[str]\nname to register the dataset with on TileDB Cloud, defaults to None\nNone\n\n\ningest_resources\nOptional[Mapping[str, str]]\nmanual override for ingest UDF resources, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\nconsolidate_resources\nOptional[Mapping[str, str]]\nmanual override for consolidate UDF resources, defaults to None\nCONSOLIDATE_RESOURCES\n\n\n\n\n\n\n\ncloud.vcf.ingestion.read_metadata_uris_udf(\n    dataset_uri,\n    *,\n    config=None,\n    metadata_uri,\n    metadata_attr='uri',\n    max_files=None,\n    verbose=False,\n)\nRead a list of URIs from a TileDB array. The URIs will be read from the attribute specified in the metadata_attr argument.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nTileDB config, defaults to None\nNone\n\n\nmetadata_uri\nstr\nmetadata array URI\nrequired\n\n\nmetadata_attr\nstr\nname of metadata attribute containing URIs, defaults to “uri”\n'uri'\n\n\nmax_files\nOptional[int]\nmaximum number of URIs returned, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nSequence[str]\nlist of URIs\n\n\n\n\n\n\n\ncloud.vcf.ingestion.read_uris_udf(\n    dataset_uri,\n    list_uri,\n    *,\n    config=None,\n    max_files=None,\n    verbose=False,\n)\nRead a list of URIs from a URI.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nlist_uri\nstr\nURI of the list of URIs\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\nmax_files\nOptional[int]\nmaximum number of URIs returned, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nSequence[str]\nlist of URIs\n\n\n\n\n\n\n\ncloud.vcf.ingestion.register_dataset_udf(\n    dataset_uri,\n    *,\n    register_name,\n    acn,\n    namespace=None,\n    config=None,\n    verbose=False,\n)\nRegister the dataset on TileDB Cloud.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nregister_name\nstr\nname to register the dataset with on TileDB Cloud\nrequired\n\n\nnamespace\nOptional[str]\nTileDB Cloud namespace, defaults to the user’s default namespace\nNone\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse",
    "crumbs": [
      "Get Started",
      "Analyze",
      "vcf.ingestion"
    ]
  },
  {
    "objectID": "reference/vcf.ingestion.html#classes",
    "href": "reference/vcf.ingestion.html#classes",
    "title": "vcf.ingestion",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nContigs\nThe contigs to ingest.\n\n\n\n\n\ncloud.vcf.ingestion.Contigs()\nThe contigs to ingest.\nALL = all contigs CHROMOSOMES = all human chromosomes OTHER = all contigs other than the human chromosomes ALL_DISABLE_MERGE = all contigs with merging disabled, for non-human datasets",
    "crumbs": [
      "Get Started",
      "Analyze",
      "vcf.ingestion"
    ]
  },
  {
    "objectID": "reference/vcf.ingestion.html#functions",
    "href": "reference/vcf.ingestion.html#functions",
    "title": "vcf.ingestion",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nconsolidate_dataset_udf\nConsolidate arrays in the dataset.\n\n\ncreate_dataset_udf\nCreate a TileDB-VCF dataset.\n\n\ncreate_manifest\nCreate a manifest array in the dataset.\n\n\nfilter_samples_udf\nReturn URIs for samples not already in the dataset.\n\n\nfilter_uris_udf\nReturn URIs from sample_uris that are not in the manifest.\n\n\nfind_uris_aws_udf\nFind URIs matching a pattern in the search_uri path with an efficient\n\n\nfind_uris_udf\nFind URIs matching a pattern in the search_uri path.\n\n\nget_logger_wrapper\nGet a logger instance and log version information.\n\n\ningest_manifest_dag\nCreate a DAG to load the manifest array.\n\n\ningest_manifest_udf\nIngest sample URIs into the manifest array.\n\n\ningest_samples_dag\nCreate a DAG to ingest samples into the dataset.\n\n\ningest_samples_udf\nIngest samples into the dataset.\n\n\ningest_vcf\nIngest samples into a dataset.\n\n\ningest_vcf_annotations\nIngest annotation VCF into a dataset. For example, a ClinVar or gnomAD VCF.\n\n\nread_metadata_uris_udf\nRead a list of URIs from a TileDB array. The URIs will be read from the\n\n\nread_uris_udf\nRead a list of URIs from a URI.\n\n\nregister_dataset_udf\nRegister the dataset on TileDB Cloud.\n\n\n\n\n\ncloud.vcf.ingestion.consolidate_dataset_udf(\n    dataset_uri,\n    *,\n    config=None,\n    exclude=MANIFEST_ARRAY,\n    include=None,\n    id='consolidate',\n    verbose=False,\n)\nConsolidate arrays in the dataset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\nexclude\nOptional[Union[Sequence[str], str]]\ngroup members to exclude, defaults to MANIFEST_ARRAY\nMANIFEST_ARRAY\n\n\ninclude\nOptional[Union[Sequence[str], str]]\ngroup members to include, defaults to None\nNone\n\n\nid\nstr\nprofiler event id, defaults to “consolidate”\n'consolidate'\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\ncloud.vcf.ingestion.create_dataset_udf(\n    dataset_uri,\n    *,\n    config=None,\n    extra_attrs=None,\n    vcf_attrs=None,\n    anchor_gap=None,\n    compression_level=None,\n    annotation_dataset=False,\n    verbose=False,\n)\nCreate a TileDB-VCF dataset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\nextra_attrs\nOptional[Union[Sequence[str], str]]\nINFO/FORMAT fields to materialize, defaults to None\nNone\n\n\nvcf_attrs\nOptional[str]\nVCF with all INFO/FORMAT fields to materialize, defaults to None\nNone\n\n\nanchor_gap\nOptional[int]\nanchor gap for VCF dataset, defaults to None\nNone\n\n\ncompression_level\nOptional[int]\nzstd compression level for the VCF dataset, defaults to None (uses the default level in TileDB-VCF)\nNone\n\n\nannotation_dataset\nbool\ncreate an annotation dataset, defaults to False\nFalse\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\ndataset URI\n\n\n\n\n\n\n\ncloud.vcf.ingestion.create_manifest(dataset_uri, group)\nCreate a manifest array in the dataset.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\ngroup\ntiledb.Group\ndataset group\nrequired\n\n\n\n\n\n\n\ncloud.vcf.ingestion.filter_samples_udf(\n    dataset_uri,\n    *,\n    config=None,\n    verbose=False,\n)\nReturn URIs for samples not already in the dataset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nSequence[str]\nsample URIs\n\n\n\n\n\n\n\ncloud.vcf.ingestion.filter_uris_udf(\n    dataset_uri,\n    sample_uris,\n    *,\n    config=None,\n    verbose=False,\n)\nReturn URIs from sample_uris that are not in the manifest.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nsample_uris\nSequence[str]\nsample URIs\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nSequence[str]\nfiltered sample URIs\n\n\n\n\n\n\n\ncloud.vcf.ingestion.find_uris_aws_udf(\n    dataset_uri,\n    search_uri,\n    *,\n    config=None,\n    include=None,\n    exclude=None,\n    max_files=None,\n    verbose=False,\n)\nFind URIs matching a pattern in the search_uri path with an efficient implementation for S3.\ninclude and exclude patterns are Unix shell style (see fnmatch module).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nsearch_uri\nstr\nURI to search for VCF files\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\ninclude\nOptional[str]\ninclude pattern used in the search, defaults to None\nNone\n\n\nexclude\nOptional[str]\nexclude pattern applied to the search results, defaults to None\nNone\n\n\nmax_files\nOptional[int]\nmaximum number of URIs returned, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nSequence[str]\nlist of URIs\n\n\n\n\n\n\n\ncloud.vcf.ingestion.find_uris_udf(\n    dataset_uri,\n    search_uri,\n    *,\n    config=None,\n    include=None,\n    exclude=None,\n    max_files=None,\n    verbose=False,\n)\nFind URIs matching a pattern in the search_uri path.\ninclude and exclude patterns are Unix shell style (see fnmatch module).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nsearch_uri\nstr\nURI to search for VCF files\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\ninclude\nOptional[str]\ninclude pattern used in the search, defaults to None\nNone\n\n\nexclude\nOptional[str]\nexclude pattern applied to the search results, defaults to None\nNone\n\n\nmax_files\nOptional[int]\nmaximum number of URIs returned, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nSequence[str]\nlist of URIs\n\n\n\n\n\n\n\ncloud.vcf.ingestion.get_logger_wrapper(verbose=False)\nGet a logger instance and log version information.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlogging.Logger\nlogger instance\n\n\n\n\n\n\n\ncloud.vcf.ingestion.ingest_manifest_dag(\n    dataset_uri,\n    *,\n    acn=None,\n    config=None,\n    namespace=None,\n    search_uri=None,\n    pattern=None,\n    ignore=None,\n    sample_list_uri=None,\n    metadata_uri=None,\n    metadata_attr='uri',\n    max_files=None,\n    batch_size=MANIFEST_BATCH_SIZE,\n    workers=MANIFEST_WORKERS,\n    extra_attrs=None,\n    vcf_attrs=None,\n    anchor_gap=None,\n    compression_level=None,\n    verbose=False,\n    aws_find_mode=False,\n    disable_manifest=False,\n    consolidate_resources=CONSOLIDATE_RESOURCES,\n    manifest_resources=MANIFEST_RESOURCES,\n)\nCreate a DAG to load the manifest array.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nacn\nOptional[str]\nAccess Credentials Name (ACN) registered in TileDB Cloud (ARN type), defaults to None\nNone\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\nnamespace\nOptional[str]\nTileDB-Cloud namespace, defaults to None\nNone\n\n\nsearch_uri\nOptional[str]\nURI to search for VCF files, defaults to None\nNone\n\n\npattern\nOptional[str]\npattern to match when searching for VCF files, defaults to None\nNone\n\n\nignore\nOptional[str]\npattern to ignore when searching for VCF files, defaults to None\nNone\n\n\nsample_list_uri\nOptional[str]\nURI with a list of VCF URIs, defaults to None\nNone\n\n\nmetadata_uri\nOptional[str]\nURI of metadata array holding VCF URIs, defaults to None\nNone\n\n\nmetadata_attr\nstr\nname of metadata attribute containing URIs, defaults to “uri”\n'uri'\n\n\nmax_files\nOptional[int]\nmaximum number of URIs to ingest, defaults to None\nNone\n\n\nbatch_size\nint\nmanifest batch size, defaults to MANIFEST_BATCH_SIZE\nMANIFEST_BATCH_SIZE\n\n\nworkers\nint\nmaximum number of parallel workers, defaults to MANIFEST_WORKERS\nMANIFEST_WORKERS\n\n\nextra_attrs\nOptional[Union[Sequence[str], str]]\nINFO/FORMAT fields to materialize, defaults to None\nNone\n\n\nvcf_attrs\nOptional[str]\nVCF with all INFO/FORMAT fields to materialize, defaults to None\nNone\n\n\nanchor_gap\nOptional[int]\nanchor gap for VCF dataset, defaults to None\nNone\n\n\ncompression_level\nOptional[int]\nzstd compression level for the VCF dataset, defaults to None (uses the default level in TileDB-VCF)\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\naws_find_mode\nbool\nuse AWS CLI to find VCFs, defaults to False\nFalse\n\n\ndisable_manifest\nbool\ndisable manifest creation, defaults to False\nFalse\n\n\nconsolidate_resources\nOptional[Mapping[str, str]]\nmanual override for consolidate UDF resources, defaults to CONSOLIDATE_RESOURCES\nCONSOLIDATE_RESOURCES\n\n\nmanifest_resources\nOptional[Mapping[str, str]]\nmanual override for manifest UDF resources, defaults to MANIFEST_RESOURCES\nMANIFEST_RESOURCES\n\n\n\n\n\n\n\ncloud.vcf.ingestion.ingest_manifest_udf(\n    dataset_uri,\n    sample_uris,\n    *,\n    config=None,\n    id='manifest',\n    verbose=False,\n)\nIngest sample URIs into the manifest array.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nsample_uris\nSequence[str]\nsample URIs\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\nid\nstr\nprofiler event id, defaults to “manifest”\n'manifest'\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\ncloud.vcf.ingestion.ingest_samples_dag(\n    dataset_uri,\n    *,\n    acn=None,\n    config=None,\n    namespace=None,\n    contigs=Contigs.ALL,\n    threads=VCF_THREADS,\n    batch_size=VCF_BATCH_SIZE,\n    workers=VCF_WORKERS,\n    max_samples=None,\n    resume=True,\n    ingest_resources=None,\n    verbose=False,\n    create_index=True,\n    trace_id=None,\n    consolidate_stats=False,\n    use_remote_tmp=False,\n    sample_list_uri=None,\n    consolidate_resources=CONSOLIDATE_RESOURCES,\n)\nCreate a DAG to ingest samples into the dataset.\nNote: If sample_list_uri is provided, the manifest is not checked for existing samples.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nacn\nOptional[str]\nAccess Credentials Name (ACN) registered in TileDB Cloud (ARN type), defaults to None\nNone\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\nnamespace\nOptional[str]\nTileDB-Cloud namespace, defaults to None\nNone\n\n\ncontigs\nOptional[Union[Sequence[str], Contigs]]\ncontig mode (Contigs.ALL | Contigs.CHROMOSOMES | Contigs.OTHER | Contigs.ALL_DISABLE_MERGE) or list of contigs to ingest, defaults to Contigs.ALL\nContigs.ALL\n\n\nthreads\nint\nnumber of threads to use per ingestion task, defaults to VCF_THREADS\nVCF_THREADS\n\n\nbatch_size\nint\nsample batch size, defaults to VCF_BATCH_SIZE\nVCF_BATCH_SIZE\n\n\nworkers\nint\nmaximum number of parallel workers, defaults to VCF_WORKERS\nVCF_WORKERS\n\n\nmax_samples\nOptional[int]\nmaximum number of samples to ingest, defaults to None (no limit)\nNone\n\n\nresume\nbool\nenable resume ingestion mode, defaults to True\nTrue\n\n\ningest_resources\nOptional[Mapping[str, str]]\nmanual override for ingest UDF resources, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\ncreate_index\nbool\nforce creation of a local index file, defaults to True\nTrue\n\n\ntrace_id\nOptional[str]\ntrace ID for logging, defaults to None\nNone\n\n\nconsolidate_stats\nbool\nconsolidate the stats arrays, defaults to False\nFalse\n\n\nuse_remote_tmp\nbool\nuse remote tmp space if VCFs need to be bgzipped, defaults to False (preferred for small VCFs)\nFalse\n\n\nsample_list_uri\nOptional[str]\nURI with a list of VCF URIs, defaults to None\nNone\n\n\nconsolidate_resources\nOptional[Mapping[str, str]]\nmanual override for consolidate UDF resources, defaults to None\nCONSOLIDATE_RESOURCES\n\n\n\n\n\n\n\ncloud.vcf.ingestion.ingest_samples_udf(\n    dataset_uri,\n    sample_uris,\n    *,\n    config=None,\n    threads,\n    memory_mb,\n    sample_batch_size,\n    contig_mode='all',\n    contigs_to_keep_separate=None,\n    contig_fragment_merging=True,\n    resume=True,\n    create_index=True,\n    id='samples',\n    verbose=False,\n    trace_id=None,\n    use_remote_tmp=False,\n)\nIngest samples into the dataset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nsample_uris\nSequence[str]\nsample URIs\nrequired\n\n\nthreads\nint\nnumber of threads to use for ingestion\nrequired\n\n\nmemory_mb\nint\nmemory to use for ingestion in MiB\nrequired\n\n\nsample_batch_size\nint\nsample batch size to use for ingestion\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\ncontig_mode\nstr\ningestion mode, defaults to “all”\n'all'\n\n\ncontigs_to_keep_separate\nOptional[Sequence[str]]\nlist of contigs to keep separate, defaults to None\nNone\n\n\ncontig_fragment_merging\nbool\nenable contig fragment merging, defaults to True\nTrue\n\n\nresume\nbool\nenable resume ingestion mode, defaults to True\nTrue\n\n\ncreate_index\nbool\nforce creation of a local index file, defaults to True\nTrue\n\n\nid\nstr\nprofiler event id, defaults to “samples”\n'samples'\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\ntrace_id\nOptional[str]\ntrace ID for logging, defaults to None\nNone\n\n\nuse_remote_tmp\nbool\nuse remote tmp space if VCFs need to be bgzipped, defaults to False (preferred for small VCFs)\nFalse\n\n\n\n\n\n\n\ncloud.vcf.ingestion.ingest_vcf(\n    dataset_uri,\n    *,\n    acn=None,\n    config=None,\n    namespace=None,\n    register_name=None,\n    search_uri=None,\n    pattern=None,\n    ignore=None,\n    sample_list_uri=None,\n    metadata_uri=None,\n    metadata_attr='uri',\n    max_files=None,\n    max_samples=None,\n    contigs=Contigs.ALL,\n    resume=True,\n    extra_attrs=DEFAULT_ATTRIBUTES,\n    vcf_attrs=None,\n    anchor_gap=None,\n    compression_level=None,\n    manifest_batch_size=MANIFEST_BATCH_SIZE,\n    manifest_workers=MANIFEST_WORKERS,\n    vcf_batch_size=VCF_BATCH_SIZE,\n    vcf_workers=VCF_WORKERS,\n    vcf_threads=VCF_THREADS,\n    ingest_resources=None,\n    verbose=False,\n    create_index=True,\n    trace_id=None,\n    consolidate_stats=True,\n    aws_find_mode=False,\n    use_remote_tmp=False,\n    disable_manifest=False,\n    consolidate_resources=CONSOLIDATE_RESOURCES,\n    manifest_resources=MANIFEST_RESOURCES,\n)\nIngest samples into a dataset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nacn\nOptional[str]\nAccess Credentials Name (ACN) registered in TileDB Cloud (ARN type), defaults to None\nNone\n\n\nconfig\n\nconfig dictionary, defaults to None\nNone\n\n\nnamespace\nOptional[str]\nTileDB-Cloud namespace, defaults to None\nNone\n\n\nregister_name\nOptional[str]\nname to register the dataset with on TileDB Cloud, defaults to None\nNone\n\n\nsearch_uri\nOptional[str]\nURI to search for VCF files, defaults to None\nNone\n\n\npattern\nOptional[str]\nUnix shell style pattern to match when searching for VCF files, defaults to None\nNone\n\n\nignore\nOptional[str]\nUnix shell style pattern to ignore when searching for VCF files, defaults to None\nNone\n\n\nsample_list_uri\nOptional[str]\nURI with a list of VCF URIs, defaults to None\nNone\n\n\nmetadata_uri\nOptional[str]\nURI of metadata array holding VCF URIs, defaults to None\nNone\n\n\nmetadata_attr\nstr\nname of metadata attribute containing URIs, defaults to “uri”\n'uri'\n\n\nmax_files\nOptional[int]\nmaximum number of VCF URIs to read/find, defaults to None (no limit)\nNone\n\n\nmax_samples\nOptional[int]\nmaximum number of samples to ingest, defaults to None (no limit)\nNone\n\n\ncontigs\nOptional[Union[Sequence[str], Contigs]]\ncontig mode (Contigs.ALL | Contigs.CHROMOSOMES | Contigs.OTHER | Contigs.ALL_DISABLE_MERGE) or list of contigs to ingest, defaults to Contigs.ALL\nContigs.ALL\n\n\nresume\nbool\nenable resume ingestion mode, defaults to True\nTrue\n\n\nextra_attrs\nOptional[Union[Sequence[str], str]]\nINFO/FORMAT fields to materialize, defaults to repr(DEFAULT_ATTRIBUTES)\nDEFAULT_ATTRIBUTES\n\n\nvcf_attrs\nOptional[str]\nVCF with all INFO/FORMAT fields to materialize, defaults to None\nNone\n\n\nanchor_gap\nOptional[int]\nanchor gap for VCF dataset, defaults to None\nNone\n\n\ncompression_level\nOptional[int]\nzstd compression level for the VCF dataset, defaults to None (uses the default level in TileDB-VCF)\nNone\n\n\nmanifest_batch_size\nint\nbatch size for manifest ingestion, defaults to MANIFEST_BATCH_SIZE\nMANIFEST_BATCH_SIZE\n\n\nmanifest_workers\nint\nnumber of workers for manifest ingestion, defaults to MANIFEST_WORKERS\nMANIFEST_WORKERS\n\n\nvcf_batch_size\nint\nbatch size for VCF ingestion, defaults to VCF_BATCH_SIZE\nVCF_BATCH_SIZE\n\n\nvcf_workers\nint\nnumber of workers for VCF ingestion, defaults to VCF_WORKERS\nVCF_WORKERS\n\n\nvcf_threads\nint\nnumber of threads for VCF ingestion, defaults to VCF_THREADS\nVCF_THREADS\n\n\ningest_resources\nOptional[Mapping[str, str]]\nmanual override for ingest UDF resources, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\ncreate_index\nbool\nforce creation of a local index file, defaults to True\nTrue\n\n\ntrace_id\nOptional[str]\ntrace ID for logging, defaults to None\nNone\n\n\nconsolidate_stats\nbool\nconsolidate the stats arrays, defaults to True\nTrue\n\n\naws_find_mode\nbool\nuse AWS CLI to find VCFs, defaults to False\nFalse\n\n\nuse_remote_tmp\nbool\nuse remote tmp space if VCFs need to be sorted and bgzipped, defaults to False (preferred for small VCFs)\nFalse\n\n\ndisable_manifest\nbool\ndisable manifest creation, defaults to False\nFalse\n\n\nconsolidate_resources\nOptional[Mapping[str, str]]\nmanual override for consolidate UDF resources, defaults to CONSOLIDATE_RESOURCES\nCONSOLIDATE_RESOURCES\n\n\nmanifest_resources\nOptional[Mapping[str, str]]\nmanual override for manifest UDF resources, defaults to MANIFEST_RESOURCES\nMANIFEST_RESOURCES\n\n\n\n\n\n\n\ncloud.vcf.ingestion.ingest_vcf_annotations(\n    dataset_uri,\n    *,\n    vcf_uri=None,\n    search_uri=None,\n    pattern=None,\n    ignore=None,\n    create_index=True,\n    config=None,\n    acn=None,\n    namespace=None,\n    register_name=None,\n    ingest_resources=None,\n    verbose=False,\n    consolidate_resources=CONSOLIDATE_RESOURCES,\n)\nIngest annotation VCF into a dataset. For example, a ClinVar or gnomAD VCF.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nvcf_uri\nOptional[str]\nVCF URI, defaults to None\nNone\n\n\nsearch_uri\nOptional[str]\nURI to search for VCF files, defaults to None\nNone\n\n\npattern\nOptional[str]\nUnix shell style pattern to match when searching for VCF files, defaults to None\nNone\n\n\nignore\nOptional[str]\nUnix shell style pattern to ignore when searching for VCF files, defaults to None\nNone\n\n\ncreate_index\nbool\nforce creation of a local index file, defaults to True\nTrue\n\n\nconfig\n\nconfig dictionary, defaults to None\nNone\n\n\nacn\nOptional[str]\nAccess Credentials Name (ACN) registered in TileDB Cloud (ARN type), defaults to None\nNone\n\n\nnamespace\nOptional[str]\nTileDB-Cloud namespace, defaults to None\nNone\n\n\nregister_name\nOptional[str]\nname to register the dataset with on TileDB Cloud, defaults to None\nNone\n\n\ningest_resources\nOptional[Mapping[str, str]]\nmanual override for ingest UDF resources, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\nconsolidate_resources\nOptional[Mapping[str, str]]\nmanual override for consolidate UDF resources, defaults to None\nCONSOLIDATE_RESOURCES\n\n\n\n\n\n\n\ncloud.vcf.ingestion.read_metadata_uris_udf(\n    dataset_uri,\n    *,\n    config=None,\n    metadata_uri,\n    metadata_attr='uri',\n    max_files=None,\n    verbose=False,\n)\nRead a list of URIs from a TileDB array. The URIs will be read from the attribute specified in the metadata_attr argument.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nTileDB config, defaults to None\nNone\n\n\nmetadata_uri\nstr\nmetadata array URI\nrequired\n\n\nmetadata_attr\nstr\nname of metadata attribute containing URIs, defaults to “uri”\n'uri'\n\n\nmax_files\nOptional[int]\nmaximum number of URIs returned, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nSequence[str]\nlist of URIs\n\n\n\n\n\n\n\ncloud.vcf.ingestion.read_uris_udf(\n    dataset_uri,\n    list_uri,\n    *,\n    config=None,\n    max_files=None,\n    verbose=False,\n)\nRead a list of URIs from a URI.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nlist_uri\nstr\nURI of the list of URIs\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\nmax_files\nOptional[int]\nmaximum number of URIs returned, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nSequence[str]\nlist of URIs\n\n\n\n\n\n\n\ncloud.vcf.ingestion.register_dataset_udf(\n    dataset_uri,\n    *,\n    register_name,\n    acn,\n    namespace=None,\n    config=None,\n    verbose=False,\n)\nRegister the dataset on TileDB Cloud.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nregister_name\nstr\nname to register the dataset with on TileDB Cloud\nrequired\n\n\nnamespace\nOptional[str]\nTileDB Cloud namespace, defaults to the user’s default namespace\nNone\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse",
    "crumbs": [
      "Get Started",
      "Analyze",
      "vcf.ingestion"
    ]
  },
  {
    "objectID": "reference/taskgraphs.types.html",
    "href": "reference/taskgraphs.types.html",
    "title": "taskgraphs.types",
    "section": "",
    "text": "cloud.taskgraphs.types\nUser-facing types used in task graphs.\n\n\n\n\n\nName\nDescription\n\n\n\n\nArrayMultiIndex\nType returned from an array query.\n\n\nCallArg\nJSON-encodable params ready for calling the server (with values).\n\n\nCallArgStoredParams\nJSON-encodable params ready for calling the server (with stored params).\n\n\nRegisteredArg\nJSON-encodable values ready for writing into a registered task graph.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nLayout\nThe layout of a TileDB query.\n\n\n\n\n\ncloud.taskgraphs.types.Layout()\nThe layout of a TileDB query.",
    "crumbs": [
      "Get Started",
      "Scale",
      "taskgraphs.types"
    ]
  },
  {
    "objectID": "reference/taskgraphs.types.html#attributes",
    "href": "reference/taskgraphs.types.html#attributes",
    "title": "taskgraphs.types",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nArrayMultiIndex\nType returned from an array query.\n\n\nCallArg\nJSON-encodable params ready for calling the server (with values).\n\n\nCallArgStoredParams\nJSON-encodable params ready for calling the server (with stored params).\n\n\nRegisteredArg\nJSON-encodable values ready for writing into a registered task graph.",
    "crumbs": [
      "Get Started",
      "Scale",
      "taskgraphs.types"
    ]
  },
  {
    "objectID": "reference/taskgraphs.types.html#classes",
    "href": "reference/taskgraphs.types.html#classes",
    "title": "taskgraphs.types",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nLayout\nThe layout of a TileDB query.\n\n\n\n\n\ncloud.taskgraphs.types.Layout()\nThe layout of a TileDB query.",
    "crumbs": [
      "Get Started",
      "Scale",
      "taskgraphs.types"
    ]
  },
  {
    "objectID": "reference/dag.dag.html",
    "href": "reference/dag.dag.html",
    "title": "dag.dag",
    "section": "",
    "text": "cloud.dag.dag\nDirected acyclic graphs as TileDB task graphs.\n\n\n\n\n\nName\nDescription\n\n\n\n\nDAG\nLow-level API for creating and managing direct acyclic graphs\n\n\nNode\nRepresentation of a function to run in a DAG.\n\n\n\n\n\ncloud.dag.dag.DAG(\n    max_workers=None,\n    use_processes=False,\n    done_callback=None,\n    update_callback=None,\n    namespace=None,\n    name=None,\n    mode=Mode.REALTIME,\n    retry_strategy=None,\n    workflow_retry_strategy=None,\n    deadline=None,\n)\nLow-level API for creating and managing direct acyclic graphs as TileDB Cloud Task Graphs.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmax_workers\nOptional[int]\nNumber of workers to allocate to execute DAG.\nNone\n\n\nuse_processes\nbool\nIf true will use processes instead of threads.\nFalse\n\n\ndone_callback\nOptional[Callable[[DAG], None]]\nOptional call back function to register for when dag is completed. Function will be passed reference to this DAG.\nNone\n\n\nupdate_callback\nOptional[Callable[[DAG], None]]\nOptional call back function to register for when dag status is updated. Function will be passed reference to this DAG.\nNone\n\n\nnamespace\nOptional[str]\nNamespace to execute DAG in.\nNone\n\n\nname\nOptional[str]\nHuman-readable name for DAG to be showin in Task Graph logs.\nNone\n\n\nmode\nMode\nMode the DAG is to run in, valid options are: Mode.REALTIME, Mode.BATCH.\nMode.REALTIME\n\n\nretry_strategy\nOptional[models.RetryStrategy]\nK8S retry policy to be applied to each Node.\nNone\n\n\nworkflow_retry_strategy\nOptional[models.RetryStrategy]\nK8S retry policy to be applied to DAG.\nNone\n\n\ndeadline\nOptional[int]\nDuration (sec) DAG allowed to execute before timeout.\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncancelled_nodes\nCancelled Nodes.\n\n\ncompleted_nodes\nCompleted Nodes.\n\n\ndeadline\nDuration (sec) DAG allowed to execute before timeout.\n\n\nfailed_nodes\nFailed Nodes.\n\n\nid\nUUID for DAG instance.\n\n\nmax_workers\nFlag. If true will use processes instead of threads.\n\n\nmode\nMode the DAG is to run in.\n\n\nname\nHuman-readable name for DAG to be showin in Task Graph logs.\n\n\nnamespace\nNamespace to execute DAG in.\n\n\nnodes\nMapping of Node UUIDs to Node instances.\n\n\nnodes_by_name\nMapping of Node names to Node instances.\n\n\nnot_started_nodes\nQueued Nodes.\n\n\nretry_strategy\nK8S retry policy to be applied to each Node.\n\n\nrunning_nodes\nRunning Nodes.\n\n\nserver_graph_uuid\nThe server-generated UUID of this graph, used for logging.\n\n\nstatus\nGet DAG status.\n\n\nuse_processes\nNumber of works to allocate to execute DAG.\n\n\nvisualization\nThe following executors are initialized by calling\n\n\nworkflow_retry_strategy\nK8S retry policy to be applied to DAG.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nadd_done_callback\nAdd a callback for when DAG is completed\n\n\nadd_node\nCreate and add a node.\n\n\nadd_node_obj\nAdd node to DAG.\n\n\nadd_update_callback\nAdd a callback for when DAG status is updated\n\n\ncancel\nCancel DAG.\n\n\ncompute\nStart the DAG by executing root nodes.\n\n\nend_nodes\nFind all end nodes\n\n\nend_results\nGet all end results, will block if all results are not ready\n\n\nend_results_by_name\nGet all end results, will block if all results are not ready\n\n\nfind_end_nodes\nFind all end nodes.\n\n\nget_tiledb_plot_node_details\nBuild list of details needed for tiledb node graph\n\n\ninitial_setup\nPerforms one-time server-side setup tasks.\n\n\nregister\nRegister DAG to TileDB.\n\n\nreport_node_complete\nReport a node as complete.\n\n\nretry_all\nRetries all failed and cancelled nodes.\n\n\nstats\nGet DAG node statistics.\n\n\nsubmit_array_udf\nSubmit a function that will be executed in the cloud serverlessly.\n\n\nsubmit_local\nSubmit a function that will run locally.\n\n\nsubmit_sql\nSubmit a sql query to run serverlessly in the cloud.\n\n\nsubmit_udf\nSubmit a function that will be executed in the cloud serverlessly.\n\n\nsubmit_udf_stage\nSubmit a function that will be executed in the cloud serverlessly.\n\n\nvisualize\nBuild and render a tree diagram of the DAG.\n\n\nwait\nWait for DAG to be completed.\n\n\n\n\n\ncloud.dag.dag.DAG.add_done_callback(func)\nAdd a callback for when DAG is completed\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\n\nFunction to call when DAG status is updated. The function will be passed reference to this dag\nrequired\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.add_node(\n    func_exec,\n    *args,\n    name=None,\n    local_mode=True,\n    **kwargs,\n)\nCreate and add a node.\nDEPRECATED. Use submit_local instead.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc_exec\n\nfunction to execute\nrequired\n\n\nargs\n\narguments for function execution\n()\n\n\nname\n\nname\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nNode that is created\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.add_node_obj(node)\nAdd node to DAG.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnode\n\nto add to dag\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNode\nNode instance.\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.add_update_callback(func)\nAdd a callback for when DAG status is updated\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\n\nFunction to call when DAG status is updated. The function will be passed reference to this dag\nrequired\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.cancel()\nCancel DAG.\n\n\n\ncloud.dag.dag.DAG.compute()\nStart the DAG by executing root nodes.\n\n\n\ncloud.dag.dag.DAG.end_nodes()\nFind all end nodes\ndag = DAG() dag.add_node(Node())\nend_nodes = dag.end_nodes()\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nlist of root nodes\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.end_results()\nGet all end results, will block if all results are not ready\ndag = DAG() dag.add_node(Node())\nend_results = dag.end_results()\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nmap of results by node ID\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.end_results_by_name()\nGet all end results, will block if all results are not ready\ndag = DAG() dag.add_node(Node())\nend_results = dag.end_results_by_name()\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nmap of results by node name\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.find_end_nodes()\nFind all end nodes.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nList[Node]\nlist of end nodes\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.get_tiledb_plot_node_details()\nBuild list of details needed for tiledb node graph\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDict[str, Dict[str, str]]\nNode summary\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.initial_setup()\nPerforms one-time server-side setup tasks.\nCan safely be called multiple times.\n\n\n\ncloud.dag.dag.DAG.register(name=None)\nRegister DAG to TileDB.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nName to register DAG as. Uses self.name as default.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nRegistered name of task graph.\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.report_node_complete(node)\nReport a node as complete.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnode\nNode\nNode to mark as complete.\nrequired\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.retry_all()\nRetries all failed and cancelled nodes.\n\n\n\ncloud.dag.dag.DAG.stats()\nGet DAG node statistics.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDict[str, Union[int, float]]\nAll node stats.\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.submit_array_udf(func, *args, **kwargs)\nSubmit a function that will be executed in the cloud serverlessly.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\nCallable\nFunction to execute in UDF task.\nrequired\n\n\nargs\nAny\nPositional arguments to pass into Node instantiation.\n()\n\n\nkwargs\nAny\nKeyword args to pass into Node instantiation.\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nNode that is created.\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.submit_local(func, *args, **kwargs)\nSubmit a function that will run locally.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\nCallable\nFunction to execute in UDF task.\nrequired\n\n\nargs\nAny\nPositional arguments to pass into Node instantiation.\n()\n\n\nkwargs\n\nKeyword args to pass into Node instantiation.\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nNode that is created\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.submit_sql(sql, *args, **kwargs)\nSubmit a sql query to run serverlessly in the cloud.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsql\nstr\nQuery to execute.\nrequired\n\n\nargs\nAny\nPositional arguments to pass into Node instantiation.\n()\n\n\nkwargs\nAny\nKeyword args to pass into Node instantiation.\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNode\nNode that is created\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.submit_udf(func, *args, **kwargs)\nSubmit a function that will be executed in the cloud serverlessly.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\nCallable\nFunction to execute in UDF task.\nrequired\n\n\nargs\n\nPositional arguments to pass into Node instantiation.\n()\n\n\nkwargs\n\nKeyword args to pass into Node instantiation.\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nNode that is created.\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.submit_udf_stage(\n    func,\n    *args,\n    expand_node_output=None,\n    **kwargs,\n)\nSubmit a function that will be executed in the cloud serverlessly.\nExpand on node output simply means to dynamically allocate works to this UDF stage based on the output of the node indicated via the expand_node_output arg.\nFor example, if a node, NodeA (NodeA = DAG.submit(...)), returns a list of str values and NodeA is passed to expand_node_output, along with an arg in the func passed to submit_udf_stage that accepts a str is also passed NodeA, a node will spawn in parallel for each str value in the result of NodeA.\ngraph = DAG(...)\n\nNodeA = graph.submit()\n\nNodeB = graph.submit_udf_stage(..., expand_node_output=NodeA, str_arg=NodeA)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\nCallable\nFunction to execute in UDF task.\nrequired\n\n\nargs\nAny\nPositional arguments to pass into Node instantiation.\n()\n\n\nexpand_node_output\nOptional[Node]\nNode that we want to expand the output of. The output of the node should be a JSON encoded list.\nNone\n\n\nkwargs\nAny\nKeyword args to pass into Node instantiation.\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNode\nNode that is created.\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.visualize(notebook=True, auto_update=True, force_plotly=False)\nBuild and render a tree diagram of the DAG.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnotebook\n\nIs the visualization inside a jupyter notebook? If so we’ll use a widget.\nTrue\n\n\nauto_update\n\nShould the diagram be auto updated with each status change.\nTrue\n\n\nforce_plotly\n\nForce the use of plotly graphs instead of TileDB Plot Widget.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nreturns figure.\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.wait(timeout=None)\nWait for DAG to be completed.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntimeout\nOptional[float]\noptional timeout in seconds to wait for DAG to be completed\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone or raises TimeoutError if timeout occurs\n\n\n\n\n\n\n\n\n\ncloud.dag.dag.Node(\n    func,\n    *args,\n    name=None,\n    dag=None,\n    mode=Mode.REALTIME,\n    expand_node_output=None,\n    _download_results=None,\n    _internal_prewrapped_func=None,\n    _internal_accepts_stored_params=True,\n    **kwargs,\n)\nRepresentation of a function to run in a DAG.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\nCallable[…, _T]\nFunction to run as UDF task.\nrequired\n\n\n*args\nAny\nPositional arguments to pass to UDF.\n()\n\n\nname\nOptional[str]\nHuman-readable name of Node task.\nNone\n\n\ndag\nOptional[DAG]\nDAG this node is associated with.\nNone\n\n\nmode\nMode\nMode the Node is to run in.\nMode.REALTIME\n\n\nexpand_node_output\nOptional[Node]\nNode to expand processes upon.\nNone\n\n\n_download_results\nOptional[bool]\nAn optional boolean to override default result-downloading behavior. If True, will always download the results of the function immediately upon completion. If False, will not download the results of the function immediately, but will be downloaded when .result() is called.\nNone\n\n\n_internal_prewrapped_func\nCallable[…, results.Result[_T]]\nFor internal use only. A function that returns. something that is already a Result, which does not require wrapping. We assume that all prewrapped functions make server calls.\nNone\n\n\n_internal_accepts_stored_params\nbool\nFor internal use only. Applies only when _prewrapped_func is used. True if _prewrapped_func can accept stored parameters. False if it cannot, and all parameters must be serialized.\nTrue\n\n\n**kwargs\nAny\nKeyword arguments to pass to UDF.\n{}\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nargs\nPositional args to pass into UDF.\n\n\nchildren\nChild Nodes (Nodes dependent on this Node).\n\n\ndag\nDAG this Node is pinned to.\n\n\nerror\nReturn Node error if encountered.\n\n\nfuture\nReturns something that pretends to be a Future.\n\n\nid\nUUID for Node instance.\n\n\nkwargs\nKeyword args to pass into UDF.\n\n\nmode\nProcessing mode of Node.\n\n\nname\nThe human-readable name of Node.\n\n\nparents\nParent Nodes (Nodes this Node is dependent on).\n\n\nstatus\nNode status.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nadd_done_callback\nAdd callback function to execute at Node completion.\n\n\ncancel\nCancel Node.\n\n\ncancelled\nWhether Node is cancelled.\n\n\ndepends_on\nCreate dependency chain for node, useful when there is a dependency\n\n\nexception\nReturn execption if one was raised.\n\n\nresult\nFetch Node return.\n\n\nretry\nRetry Node.\n\n\nrunning\nWhether Node is actively running.\n\n\ntask_id\nGets the server-side Task ID of this node.\n\n\nto_registration_json\nConverts this node to the form used when registering the graph.\n\n\nwait\nWait for node to be completed.\n\n\n\n\n\ncloud.dag.dag.Node.add_done_callback(fn)\nAdd callback function to execute at Node completion.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfn\nCallable[[Node[_T]], None]\nCallaback to execute.\nrequired\n\n\n\n\n\n\n\ncloud.dag.dag.Node.cancel()\nCancel Node.\n\n\n\ncloud.dag.dag.Node.cancelled()\nWhether Node is cancelled.\n\n\n\ncloud.dag.dag.Node.depends_on(node)\nCreate dependency chain for node, useful when there is a dependency that does not rely directly on passing results from one to another.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnode\nNode\nnode to mark as a dependency of this node\nrequired\n\n\n\n\n\n\n\ncloud.dag.dag.Node.exception(timeout=None)\nReturn execption if one was raised.\n\n\n\ncloud.dag.dag.Node.result(timeout=None)\nFetch Node return.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntimeout\nOptional[float]\nTime to wait to fetch result.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n_T\nResults of Node processing.\n\n\n\n\n\n\n\ncloud.dag.dag.Node.retry()\nRetry Node.\n\n\n\ncloud.dag.dag.Node.running()\nWhether Node is actively running.\n\n\n\ncloud.dag.dag.Node.task_id()\nGets the server-side Task ID of this node.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nOptional[uuid.UUID]\nNone if this has no task ID (as it was run on the client side).\n\n\n\n\n\n\n\ncloud.dag.dag.Node.to_registration_json(existing_names)\nConverts this node to the form used when registering the graph.\nThis is the form of the Node that will be used to represent it in the RegisteredTaskGraph object, i.e. a RegisteredTaskGraphNode.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexisting_names\nSet[str]\nThe set of names that have already been used, so that we don’t generate a duplicate node name.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDict[str, Any]\nMapping of Node for registration.\n\n\n\n\n\n\n\ncloud.dag.dag.Node.wait(timeout=None)\nWait for node to be completed.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntimeout\nOptional[float]\noptional timeout in seconds to wait for DAG to be completed.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone or raises TimeoutError if timeout occurs.\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nlist_logs\nRetrieves the list of task graph logs you can view.\n\n\nreplace_stored_params\nDescends into data structures and replaces Stored Params with results.\n\n\nserver_logs\nRetrieves the full server-side logs for the given DAG.\n\n\n\n\n\ncloud.dag.dag.list_logs(\n    namespace=None,\n    created_by=None,\n    search=None,\n    start_time=None,\n    end_time=None,\n    page=1,\n    per_page=10,\n)\nRetrieves the list of task graph logs you can view.\nThe returned graph logs will be “light” versions, meaning they will not include any details about the execution state of an individual DAG. To retrieve those, pass the ID to :func:server_logs.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnamespace\nOptional[str]\nIf present, include logs for only this namespace. If absent, include logs for all namespaces you have access to.\nNone\n\n\ncreated_by\nOptional[str]\nInclude only logs from this user (if present).\nNone\n\n\nsearch\nOptional[str]\nA search string for the name of the task graph.\nNone\n\n\nstart_time\nOptional[datetime.datetime]\nInclude logs created after this time.\nNone\n\n\nend_time\nOptional[datetime.datetime]\nInclude logs created before this time.\nNone\n\n\npage\nint\nThe page number to use, starting from 1.\n1\n\n\nper_page\nint\nThe number of items per page.\n10\n\n\n\n\n\n\n\ncloud.dag.dag.replace_stored_params(tree, loader)\nDescends into data structures and replaces Stored Params with results.\n\n\n\ncloud.dag.dag.server_logs(dag_or_id, namespace=None)\nRetrieves the full server-side logs for the given DAG.\nThe DAG can be provided as a DAG object, or the server-provided UUID of a DAG’s execution log in either :class:uuid.UUID or string form. This can be used to access both completed DAGs and in-progress DAGs. The returned DAGs will include full data\nWill return None if called with a DAG object that has no server-side nodes.",
    "crumbs": [
      "Get Started",
      "Scale",
      "dag.dag"
    ]
  },
  {
    "objectID": "reference/dag.dag.html#classes",
    "href": "reference/dag.dag.html#classes",
    "title": "dag.dag",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nDAG\nLow-level API for creating and managing direct acyclic graphs\n\n\nNode\nRepresentation of a function to run in a DAG.\n\n\n\n\n\ncloud.dag.dag.DAG(\n    max_workers=None,\n    use_processes=False,\n    done_callback=None,\n    update_callback=None,\n    namespace=None,\n    name=None,\n    mode=Mode.REALTIME,\n    retry_strategy=None,\n    workflow_retry_strategy=None,\n    deadline=None,\n)\nLow-level API for creating and managing direct acyclic graphs as TileDB Cloud Task Graphs.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmax_workers\nOptional[int]\nNumber of workers to allocate to execute DAG.\nNone\n\n\nuse_processes\nbool\nIf true will use processes instead of threads.\nFalse\n\n\ndone_callback\nOptional[Callable[[DAG], None]]\nOptional call back function to register for when dag is completed. Function will be passed reference to this DAG.\nNone\n\n\nupdate_callback\nOptional[Callable[[DAG], None]]\nOptional call back function to register for when dag status is updated. Function will be passed reference to this DAG.\nNone\n\n\nnamespace\nOptional[str]\nNamespace to execute DAG in.\nNone\n\n\nname\nOptional[str]\nHuman-readable name for DAG to be showin in Task Graph logs.\nNone\n\n\nmode\nMode\nMode the DAG is to run in, valid options are: Mode.REALTIME, Mode.BATCH.\nMode.REALTIME\n\n\nretry_strategy\nOptional[models.RetryStrategy]\nK8S retry policy to be applied to each Node.\nNone\n\n\nworkflow_retry_strategy\nOptional[models.RetryStrategy]\nK8S retry policy to be applied to DAG.\nNone\n\n\ndeadline\nOptional[int]\nDuration (sec) DAG allowed to execute before timeout.\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncancelled_nodes\nCancelled Nodes.\n\n\ncompleted_nodes\nCompleted Nodes.\n\n\ndeadline\nDuration (sec) DAG allowed to execute before timeout.\n\n\nfailed_nodes\nFailed Nodes.\n\n\nid\nUUID for DAG instance.\n\n\nmax_workers\nFlag. If true will use processes instead of threads.\n\n\nmode\nMode the DAG is to run in.\n\n\nname\nHuman-readable name for DAG to be showin in Task Graph logs.\n\n\nnamespace\nNamespace to execute DAG in.\n\n\nnodes\nMapping of Node UUIDs to Node instances.\n\n\nnodes_by_name\nMapping of Node names to Node instances.\n\n\nnot_started_nodes\nQueued Nodes.\n\n\nretry_strategy\nK8S retry policy to be applied to each Node.\n\n\nrunning_nodes\nRunning Nodes.\n\n\nserver_graph_uuid\nThe server-generated UUID of this graph, used for logging.\n\n\nstatus\nGet DAG status.\n\n\nuse_processes\nNumber of works to allocate to execute DAG.\n\n\nvisualization\nThe following executors are initialized by calling\n\n\nworkflow_retry_strategy\nK8S retry policy to be applied to DAG.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nadd_done_callback\nAdd a callback for when DAG is completed\n\n\nadd_node\nCreate and add a node.\n\n\nadd_node_obj\nAdd node to DAG.\n\n\nadd_update_callback\nAdd a callback for when DAG status is updated\n\n\ncancel\nCancel DAG.\n\n\ncompute\nStart the DAG by executing root nodes.\n\n\nend_nodes\nFind all end nodes\n\n\nend_results\nGet all end results, will block if all results are not ready\n\n\nend_results_by_name\nGet all end results, will block if all results are not ready\n\n\nfind_end_nodes\nFind all end nodes.\n\n\nget_tiledb_plot_node_details\nBuild list of details needed for tiledb node graph\n\n\ninitial_setup\nPerforms one-time server-side setup tasks.\n\n\nregister\nRegister DAG to TileDB.\n\n\nreport_node_complete\nReport a node as complete.\n\n\nretry_all\nRetries all failed and cancelled nodes.\n\n\nstats\nGet DAG node statistics.\n\n\nsubmit_array_udf\nSubmit a function that will be executed in the cloud serverlessly.\n\n\nsubmit_local\nSubmit a function that will run locally.\n\n\nsubmit_sql\nSubmit a sql query to run serverlessly in the cloud.\n\n\nsubmit_udf\nSubmit a function that will be executed in the cloud serverlessly.\n\n\nsubmit_udf_stage\nSubmit a function that will be executed in the cloud serverlessly.\n\n\nvisualize\nBuild and render a tree diagram of the DAG.\n\n\nwait\nWait for DAG to be completed.\n\n\n\n\n\ncloud.dag.dag.DAG.add_done_callback(func)\nAdd a callback for when DAG is completed\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\n\nFunction to call when DAG status is updated. The function will be passed reference to this dag\nrequired\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.add_node(\n    func_exec,\n    *args,\n    name=None,\n    local_mode=True,\n    **kwargs,\n)\nCreate and add a node.\nDEPRECATED. Use submit_local instead.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc_exec\n\nfunction to execute\nrequired\n\n\nargs\n\narguments for function execution\n()\n\n\nname\n\nname\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nNode that is created\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.add_node_obj(node)\nAdd node to DAG.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnode\n\nto add to dag\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNode\nNode instance.\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.add_update_callback(func)\nAdd a callback for when DAG status is updated\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\n\nFunction to call when DAG status is updated. The function will be passed reference to this dag\nrequired\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.cancel()\nCancel DAG.\n\n\n\ncloud.dag.dag.DAG.compute()\nStart the DAG by executing root nodes.\n\n\n\ncloud.dag.dag.DAG.end_nodes()\nFind all end nodes\ndag = DAG() dag.add_node(Node())\nend_nodes = dag.end_nodes()\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nlist of root nodes\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.end_results()\nGet all end results, will block if all results are not ready\ndag = DAG() dag.add_node(Node())\nend_results = dag.end_results()\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nmap of results by node ID\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.end_results_by_name()\nGet all end results, will block if all results are not ready\ndag = DAG() dag.add_node(Node())\nend_results = dag.end_results_by_name()\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nmap of results by node name\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.find_end_nodes()\nFind all end nodes.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nList[Node]\nlist of end nodes\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.get_tiledb_plot_node_details()\nBuild list of details needed for tiledb node graph\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDict[str, Dict[str, str]]\nNode summary\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.initial_setup()\nPerforms one-time server-side setup tasks.\nCan safely be called multiple times.\n\n\n\ncloud.dag.dag.DAG.register(name=None)\nRegister DAG to TileDB.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nName to register DAG as. Uses self.name as default.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nRegistered name of task graph.\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.report_node_complete(node)\nReport a node as complete.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnode\nNode\nNode to mark as complete.\nrequired\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.retry_all()\nRetries all failed and cancelled nodes.\n\n\n\ncloud.dag.dag.DAG.stats()\nGet DAG node statistics.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDict[str, Union[int, float]]\nAll node stats.\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.submit_array_udf(func, *args, **kwargs)\nSubmit a function that will be executed in the cloud serverlessly.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\nCallable\nFunction to execute in UDF task.\nrequired\n\n\nargs\nAny\nPositional arguments to pass into Node instantiation.\n()\n\n\nkwargs\nAny\nKeyword args to pass into Node instantiation.\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nNode that is created.\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.submit_local(func, *args, **kwargs)\nSubmit a function that will run locally.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\nCallable\nFunction to execute in UDF task.\nrequired\n\n\nargs\nAny\nPositional arguments to pass into Node instantiation.\n()\n\n\nkwargs\n\nKeyword args to pass into Node instantiation.\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nNode that is created\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.submit_sql(sql, *args, **kwargs)\nSubmit a sql query to run serverlessly in the cloud.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsql\nstr\nQuery to execute.\nrequired\n\n\nargs\nAny\nPositional arguments to pass into Node instantiation.\n()\n\n\nkwargs\nAny\nKeyword args to pass into Node instantiation.\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNode\nNode that is created\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.submit_udf(func, *args, **kwargs)\nSubmit a function that will be executed in the cloud serverlessly.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\nCallable\nFunction to execute in UDF task.\nrequired\n\n\nargs\n\nPositional arguments to pass into Node instantiation.\n()\n\n\nkwargs\n\nKeyword args to pass into Node instantiation.\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nNode that is created.\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.submit_udf_stage(\n    func,\n    *args,\n    expand_node_output=None,\n    **kwargs,\n)\nSubmit a function that will be executed in the cloud serverlessly.\nExpand on node output simply means to dynamically allocate works to this UDF stage based on the output of the node indicated via the expand_node_output arg.\nFor example, if a node, NodeA (NodeA = DAG.submit(...)), returns a list of str values and NodeA is passed to expand_node_output, along with an arg in the func passed to submit_udf_stage that accepts a str is also passed NodeA, a node will spawn in parallel for each str value in the result of NodeA.\ngraph = DAG(...)\n\nNodeA = graph.submit()\n\nNodeB = graph.submit_udf_stage(..., expand_node_output=NodeA, str_arg=NodeA)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\nCallable\nFunction to execute in UDF task.\nrequired\n\n\nargs\nAny\nPositional arguments to pass into Node instantiation.\n()\n\n\nexpand_node_output\nOptional[Node]\nNode that we want to expand the output of. The output of the node should be a JSON encoded list.\nNone\n\n\nkwargs\nAny\nKeyword args to pass into Node instantiation.\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNode\nNode that is created.\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.visualize(notebook=True, auto_update=True, force_plotly=False)\nBuild and render a tree diagram of the DAG.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnotebook\n\nIs the visualization inside a jupyter notebook? If so we’ll use a widget.\nTrue\n\n\nauto_update\n\nShould the diagram be auto updated with each status change.\nTrue\n\n\nforce_plotly\n\nForce the use of plotly graphs instead of TileDB Plot Widget.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nreturns figure.\n\n\n\n\n\n\n\ncloud.dag.dag.DAG.wait(timeout=None)\nWait for DAG to be completed.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntimeout\nOptional[float]\noptional timeout in seconds to wait for DAG to be completed\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone or raises TimeoutError if timeout occurs\n\n\n\n\n\n\n\n\n\ncloud.dag.dag.Node(\n    func,\n    *args,\n    name=None,\n    dag=None,\n    mode=Mode.REALTIME,\n    expand_node_output=None,\n    _download_results=None,\n    _internal_prewrapped_func=None,\n    _internal_accepts_stored_params=True,\n    **kwargs,\n)\nRepresentation of a function to run in a DAG.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\nCallable[…, _T]\nFunction to run as UDF task.\nrequired\n\n\n*args\nAny\nPositional arguments to pass to UDF.\n()\n\n\nname\nOptional[str]\nHuman-readable name of Node task.\nNone\n\n\ndag\nOptional[DAG]\nDAG this node is associated with.\nNone\n\n\nmode\nMode\nMode the Node is to run in.\nMode.REALTIME\n\n\nexpand_node_output\nOptional[Node]\nNode to expand processes upon.\nNone\n\n\n_download_results\nOptional[bool]\nAn optional boolean to override default result-downloading behavior. If True, will always download the results of the function immediately upon completion. If False, will not download the results of the function immediately, but will be downloaded when .result() is called.\nNone\n\n\n_internal_prewrapped_func\nCallable[…, results.Result[_T]]\nFor internal use only. A function that returns. something that is already a Result, which does not require wrapping. We assume that all prewrapped functions make server calls.\nNone\n\n\n_internal_accepts_stored_params\nbool\nFor internal use only. Applies only when _prewrapped_func is used. True if _prewrapped_func can accept stored parameters. False if it cannot, and all parameters must be serialized.\nTrue\n\n\n**kwargs\nAny\nKeyword arguments to pass to UDF.\n{}\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nargs\nPositional args to pass into UDF.\n\n\nchildren\nChild Nodes (Nodes dependent on this Node).\n\n\ndag\nDAG this Node is pinned to.\n\n\nerror\nReturn Node error if encountered.\n\n\nfuture\nReturns something that pretends to be a Future.\n\n\nid\nUUID for Node instance.\n\n\nkwargs\nKeyword args to pass into UDF.\n\n\nmode\nProcessing mode of Node.\n\n\nname\nThe human-readable name of Node.\n\n\nparents\nParent Nodes (Nodes this Node is dependent on).\n\n\nstatus\nNode status.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nadd_done_callback\nAdd callback function to execute at Node completion.\n\n\ncancel\nCancel Node.\n\n\ncancelled\nWhether Node is cancelled.\n\n\ndepends_on\nCreate dependency chain for node, useful when there is a dependency\n\n\nexception\nReturn execption if one was raised.\n\n\nresult\nFetch Node return.\n\n\nretry\nRetry Node.\n\n\nrunning\nWhether Node is actively running.\n\n\ntask_id\nGets the server-side Task ID of this node.\n\n\nto_registration_json\nConverts this node to the form used when registering the graph.\n\n\nwait\nWait for node to be completed.\n\n\n\n\n\ncloud.dag.dag.Node.add_done_callback(fn)\nAdd callback function to execute at Node completion.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfn\nCallable[[Node[_T]], None]\nCallaback to execute.\nrequired\n\n\n\n\n\n\n\ncloud.dag.dag.Node.cancel()\nCancel Node.\n\n\n\ncloud.dag.dag.Node.cancelled()\nWhether Node is cancelled.\n\n\n\ncloud.dag.dag.Node.depends_on(node)\nCreate dependency chain for node, useful when there is a dependency that does not rely directly on passing results from one to another.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnode\nNode\nnode to mark as a dependency of this node\nrequired\n\n\n\n\n\n\n\ncloud.dag.dag.Node.exception(timeout=None)\nReturn execption if one was raised.\n\n\n\ncloud.dag.dag.Node.result(timeout=None)\nFetch Node return.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntimeout\nOptional[float]\nTime to wait to fetch result.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n_T\nResults of Node processing.\n\n\n\n\n\n\n\ncloud.dag.dag.Node.retry()\nRetry Node.\n\n\n\ncloud.dag.dag.Node.running()\nWhether Node is actively running.\n\n\n\ncloud.dag.dag.Node.task_id()\nGets the server-side Task ID of this node.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nOptional[uuid.UUID]\nNone if this has no task ID (as it was run on the client side).\n\n\n\n\n\n\n\ncloud.dag.dag.Node.to_registration_json(existing_names)\nConverts this node to the form used when registering the graph.\nThis is the form of the Node that will be used to represent it in the RegisteredTaskGraph object, i.e. a RegisteredTaskGraphNode.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexisting_names\nSet[str]\nThe set of names that have already been used, so that we don’t generate a duplicate node name.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDict[str, Any]\nMapping of Node for registration.\n\n\n\n\n\n\n\ncloud.dag.dag.Node.wait(timeout=None)\nWait for node to be completed.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntimeout\nOptional[float]\noptional timeout in seconds to wait for DAG to be completed.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone or raises TimeoutError if timeout occurs.",
    "crumbs": [
      "Get Started",
      "Scale",
      "dag.dag"
    ]
  },
  {
    "objectID": "reference/dag.dag.html#functions",
    "href": "reference/dag.dag.html#functions",
    "title": "dag.dag",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nlist_logs\nRetrieves the list of task graph logs you can view.\n\n\nreplace_stored_params\nDescends into data structures and replaces Stored Params with results.\n\n\nserver_logs\nRetrieves the full server-side logs for the given DAG.\n\n\n\n\n\ncloud.dag.dag.list_logs(\n    namespace=None,\n    created_by=None,\n    search=None,\n    start_time=None,\n    end_time=None,\n    page=1,\n    per_page=10,\n)\nRetrieves the list of task graph logs you can view.\nThe returned graph logs will be “light” versions, meaning they will not include any details about the execution state of an individual DAG. To retrieve those, pass the ID to :func:server_logs.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnamespace\nOptional[str]\nIf present, include logs for only this namespace. If absent, include logs for all namespaces you have access to.\nNone\n\n\ncreated_by\nOptional[str]\nInclude only logs from this user (if present).\nNone\n\n\nsearch\nOptional[str]\nA search string for the name of the task graph.\nNone\n\n\nstart_time\nOptional[datetime.datetime]\nInclude logs created after this time.\nNone\n\n\nend_time\nOptional[datetime.datetime]\nInclude logs created before this time.\nNone\n\n\npage\nint\nThe page number to use, starting from 1.\n1\n\n\nper_page\nint\nThe number of items per page.\n10\n\n\n\n\n\n\n\ncloud.dag.dag.replace_stored_params(tree, loader)\nDescends into data structures and replaces Stored Params with results.\n\n\n\ncloud.dag.dag.server_logs(dag_or_id, namespace=None)\nRetrieves the full server-side logs for the given DAG.\nThe DAG can be provided as a DAG object, or the server-provided UUID of a DAG’s execution log in either :class:uuid.UUID or string form. This can be used to access both completed DAGs and in-progress DAGs. The returned DAGs will include full data\nWill return None if called with a DAG object that has no server-side nodes.",
    "crumbs": [
      "Get Started",
      "Scale",
      "dag.dag"
    ]
  },
  {
    "objectID": "reference/groups.html",
    "href": "reference/groups.html",
    "title": "groups",
    "section": "",
    "text": "cloud.groups\nFunctions for managing TileDB Cloud groups.\n\n\n\n\n\nName\nDescription\n\n\n\n\ncontents\nGet a group’s contents.\n\n\ncreate\nCreates a new TileDB Cloud group.\n\n\ndelete\nDeletes a group.\n\n\nderegister\nDeregisters the given group from TileDB Cloud.\n\n\ninfo\nGets metadata about the named TileDB Cloud group.\n\n\nlist_shared_with\nList a group’s sharing policies.\n\n\nregister\nRegisters a pre-existing group.\n\n\nshare_group\nShares group with given namespace and permissions.\n\n\nunshare_group\nRemoves sharing of a group from given namespace\n\n\nupdate_info\nUpdate Group Attributes\n\n\n\n\n\ncloud.groups.contents(\n    uri,\n    async_req=None,\n    page=None,\n    per_page=None,\n    namespace=None,\n    search=None,\n    orderby=None,\n    tag=None,\n    exclude_tag=None,\n    member_type=None,\n    exclude_member_type=None,\n)\nGet a group’s contents.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbool\nasync_req\nexecute request asynchronously\nrequired\n\n\npage\nint\npagination offset for assets\nNone\n\n\nper_page\nint\npagination limit for assets\nNone\n\n\nnamespace\nstr\nnamespace to search for\nNone\n\n\nsearch\nstr\nsearch string that will look at name, namespace or description fields\nNone\n\n\norderby\nstr\nsort by which field valid values include last_accessed, size, name\nNone\n\n\ntag\nlist[str]\ntag to search for, more than one can be included\nNone\n\n\nexclude_tag\nlist[str]\ntags to exclude matching array in results, more than one can be included\nNone\n\n\nmember_type\nlist[str]\nmember type to search for, more than one can be included\nNone\n\n\nexclude_member_type\nlist[str]\nmember type to exclude matching groups in results, more than one can be included\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nrest_api.GroupContents\nGroupContents If the method is called asynchronously, returns the request thread.\n\n\n\n\n\n\n\ncloud.groups.create(\n    name,\n    *,\n    namespace=None,\n    parent_uri=None,\n    storage_uri=None,\n    credentials_name=None,\n)\nCreates a new TileDB Cloud group.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nThe name of the group to create, or its URI.\nrequired\n\n\nnamespace\nOptional[str]\nThe namespace to create the group in. If name is a URI, this must not be provided. If not provided, the current logged-in user will be used.\nNone\n\n\nparent_uri\nOptional[str]\nThe parent URI to add the group to, if desired.\nNone\n\n\nstorage_uri\nOptional[str]\nThe backend URI where the group will be stored. If not provided, uses the namespace’s default storage path for groups.\nNone\n\n\ncredentials_name\nOptional[str]\nThe name of the storage credential to use for creating the group. If not provided, uses the namespace’s default credential for groups.\nNone\n\n\n\n\n\n\n\ncloud.groups.delete(uri, recursive=False)\nDeletes a group.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nTileDB Group URI.\nrequired\n\n\nrecursive\nbool\nDelete all off the group’s contents, defaults to False\nFalse\n\n\n\n\n\n\n\ncloud.groups.deregister(uri, *, recursive=False)\nDeregisters the given group from TileDB Cloud.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nThe URI of the group to deregister.\nrequired\n\n\nrecursive\nbool\nIf true, deregister the group recursively by deregistering all of the elements of the group (and all elements of those groups, recursively) before deregistering the group itself.\nFalse\n\n\n\n\n\n\n\ncloud.groups.info(uri)\nGets metadata about the named TileDB Cloud group.\n\n\n\ncloud.groups.list_shared_with(uri, async_req=False)\nList a group’s sharing policies.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\ntiledb URI of the asset.\nrequired\n\n\nasync_req\n\nreturn future instead of results for async support.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\na list of GroupSharing objects.\n\n\n\n\n\n\n\ncloud.groups.register(\n    storage_uri,\n    *,\n    dest_uri=None,\n    name=None,\n    namespace=None,\n    credentials_name=None,\n    parent_uri=None,\n)\nRegisters a pre-existing group.\n\n\n\ncloud.groups.share_group(uri, namespace, permissions, async_req=False)\nShares group with given namespace and permissions.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\ntiledb URI of the asset.\nrequired\n\n\nnamespace\nstr\n\nrequired\n\n\npermissions\nlist(str)\n\nrequired\n\n\nasync_req\n\nreturn future instead of results for async support.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nNone.\n\n\n\n\n\n\n\ncloud.groups.unshare_group(uri, namespace, async_req=False)\nRemoves sharing of a group from given namespace\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnamespace\nstr\nnamespace to remove shared access to the group\nrequired\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncloud.groups.update_info(\n    uri,\n    *,\n    description=None,\n    name=None,\n    logo=None,\n    tags=None,\n)\nUpdate Group Attributes\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nURI of the group in the form ‘tiledb:///’\nrequired\n\n\ndescription\nOptional[str]\nGroup description, defaults to None\nNone\n\n\nname\nOptional[str]\nGroup’s name, defaults to None\nNone\n\n\nlogo\nOptional[str]\nGroup’s logo, defaults to None\nNone\n\n\ntags\nOptional[List[str]]\nGroup tags, defaults to None\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone",
    "crumbs": [
      "Get Started",
      "Catalog",
      "groups"
    ]
  },
  {
    "objectID": "reference/groups.html#functions",
    "href": "reference/groups.html#functions",
    "title": "groups",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncontents\nGet a group’s contents.\n\n\ncreate\nCreates a new TileDB Cloud group.\n\n\ndelete\nDeletes a group.\n\n\nderegister\nDeregisters the given group from TileDB Cloud.\n\n\ninfo\nGets metadata about the named TileDB Cloud group.\n\n\nlist_shared_with\nList a group’s sharing policies.\n\n\nregister\nRegisters a pre-existing group.\n\n\nshare_group\nShares group with given namespace and permissions.\n\n\nunshare_group\nRemoves sharing of a group from given namespace\n\n\nupdate_info\nUpdate Group Attributes\n\n\n\n\n\ncloud.groups.contents(\n    uri,\n    async_req=None,\n    page=None,\n    per_page=None,\n    namespace=None,\n    search=None,\n    orderby=None,\n    tag=None,\n    exclude_tag=None,\n    member_type=None,\n    exclude_member_type=None,\n)\nGet a group’s contents.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbool\nasync_req\nexecute request asynchronously\nrequired\n\n\npage\nint\npagination offset for assets\nNone\n\n\nper_page\nint\npagination limit for assets\nNone\n\n\nnamespace\nstr\nnamespace to search for\nNone\n\n\nsearch\nstr\nsearch string that will look at name, namespace or description fields\nNone\n\n\norderby\nstr\nsort by which field valid values include last_accessed, size, name\nNone\n\n\ntag\nlist[str]\ntag to search for, more than one can be included\nNone\n\n\nexclude_tag\nlist[str]\ntags to exclude matching array in results, more than one can be included\nNone\n\n\nmember_type\nlist[str]\nmember type to search for, more than one can be included\nNone\n\n\nexclude_member_type\nlist[str]\nmember type to exclude matching groups in results, more than one can be included\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nrest_api.GroupContents\nGroupContents If the method is called asynchronously, returns the request thread.\n\n\n\n\n\n\n\ncloud.groups.create(\n    name,\n    *,\n    namespace=None,\n    parent_uri=None,\n    storage_uri=None,\n    credentials_name=None,\n)\nCreates a new TileDB Cloud group.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nThe name of the group to create, or its URI.\nrequired\n\n\nnamespace\nOptional[str]\nThe namespace to create the group in. If name is a URI, this must not be provided. If not provided, the current logged-in user will be used.\nNone\n\n\nparent_uri\nOptional[str]\nThe parent URI to add the group to, if desired.\nNone\n\n\nstorage_uri\nOptional[str]\nThe backend URI where the group will be stored. If not provided, uses the namespace’s default storage path for groups.\nNone\n\n\ncredentials_name\nOptional[str]\nThe name of the storage credential to use for creating the group. If not provided, uses the namespace’s default credential for groups.\nNone\n\n\n\n\n\n\n\ncloud.groups.delete(uri, recursive=False)\nDeletes a group.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nTileDB Group URI.\nrequired\n\n\nrecursive\nbool\nDelete all off the group’s contents, defaults to False\nFalse\n\n\n\n\n\n\n\ncloud.groups.deregister(uri, *, recursive=False)\nDeregisters the given group from TileDB Cloud.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nThe URI of the group to deregister.\nrequired\n\n\nrecursive\nbool\nIf true, deregister the group recursively by deregistering all of the elements of the group (and all elements of those groups, recursively) before deregistering the group itself.\nFalse\n\n\n\n\n\n\n\ncloud.groups.info(uri)\nGets metadata about the named TileDB Cloud group.\n\n\n\ncloud.groups.list_shared_with(uri, async_req=False)\nList a group’s sharing policies.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\ntiledb URI of the asset.\nrequired\n\n\nasync_req\n\nreturn future instead of results for async support.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\na list of GroupSharing objects.\n\n\n\n\n\n\n\ncloud.groups.register(\n    storage_uri,\n    *,\n    dest_uri=None,\n    name=None,\n    namespace=None,\n    credentials_name=None,\n    parent_uri=None,\n)\nRegisters a pre-existing group.\n\n\n\ncloud.groups.share_group(uri, namespace, permissions, async_req=False)\nShares group with given namespace and permissions.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\ntiledb URI of the asset.\nrequired\n\n\nnamespace\nstr\n\nrequired\n\n\npermissions\nlist(str)\n\nrequired\n\n\nasync_req\n\nreturn future instead of results for async support.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nNone.\n\n\n\n\n\n\n\ncloud.groups.unshare_group(uri, namespace, async_req=False)\nRemoves sharing of a group from given namespace\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnamespace\nstr\nnamespace to remove shared access to the group\nrequired\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncloud.groups.update_info(\n    uri,\n    *,\n    description=None,\n    name=None,\n    logo=None,\n    tags=None,\n)\nUpdate Group Attributes\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nURI of the group in the form ‘tiledb:///’\nrequired\n\n\ndescription\nOptional[str]\nGroup description, defaults to None\nNone\n\n\nname\nOptional[str]\nGroup’s name, defaults to None\nNone\n\n\nlogo\nOptional[str]\nGroup’s logo, defaults to None\nNone\n\n\ntags\nOptional[List[str]]\nGroup tags, defaults to None\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone",
    "crumbs": [
      "Get Started",
      "Catalog",
      "groups"
    ]
  },
  {
    "objectID": "reference/files.ingestion.html",
    "href": "reference/files.ingestion.html",
    "title": "files.ingestion",
    "section": "",
    "text": "cloud.files.ingestion\n\n\n\n\n\nName\nDescription\n\n\n\n\ningest_and_index\nIngest files into a dataset and index them afterwards.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nadd_arrays_to_group_udf\nAdd a list of TileDB array uris in a TileDB group.\n\n\ningest_files\nIngest files into a dataset.\n\n\ningest_files_udf\nIngest files.\n\n\n\n\n\ncloud.files.ingestion.add_arrays_to_group_udf(\n    array_uris,\n    group_uri,\n    *,\n    config=None,\n    verbose=False,\n)\nAdd a list of TileDB array uris in a TileDB group.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\narray_uris\nSequence[str]\nAn iterable of TileDB URIs.\nrequired\n\n\ngroup_uri\nstr\nA TileDB Group URI.\nrequired\n\n\nconfig\nOptional[dict]\nConfig dictionary, defaults to None\nNone\n\n\nverbose\nbool\nVerbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\ncloud.files.ingestion.ingest_files(\n    dataset_uri,\n    *,\n    search_uri=None,\n    pattern=None,\n    ignore=None,\n    max_files=None,\n    batch_size=file_udfs.DEFAULT_BATCH_SIZE,\n    acn=None,\n    config=None,\n    namespace=None,\n    group_uri=None,\n    taskgraph_name=DEFAULT_FILE_INGESTION_NAME,\n    ingest_resources=dag.MIN_BATCH_RESOURCES,\n    verbose=False,\n)\nIngest files into a dataset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\nThe dataset URI\nrequired\n\n\nsearch_uri\nOptional[Union[Sequence[str], str]]\nURI or an iterable of URIs of input files. Defaults to None.\nNone\n\n\npattern\nOptional[str]\nUNIX shell style pattern to filter files in the search, defaults to None\nNone\n\n\nignore\nOptional[str]\nUNIX shell style pattern to filter files out of the search, defaults to None\nNone\n\n\nmax_files\nOptional[int]\nmaximum number of File URIs to read/find, defaults to None (no limit)\nNone\n\n\nbatch_size\nOptional[int]\nBatch size for file ingestion, defaults to 100.\nfile_udfs.DEFAULT_BATCH_SIZE\n\n\nacn\nOptional[str]\nAccess Credentials Name (ACN) registered in TileDB Cloud (ARN type), defaults to None\nNone\n\n\nconfig\nOptional[dict]\nConfig dictionary, defaults to None\nNone\n\n\nnamespace\nOptional[str]\nTileDB-Cloud namespace, defaults to None\nNone\n\n\ngroup_uri\nOptional[str]\nA TileDB Group URI, defaults to None.\nNone\n\n\ntaskgraph_name\nOptional[str]\nOptional name for taskgraph, defaults to “file-ingestion”.\nDEFAULT_FILE_INGESTION_NAME\n\n\ningest_resources\nOptional[Mapping[str, Any]]\nConfiguration for node specs, defaults to {“cpu”: “1”, “memory”: “2Gi”}\ndag.MIN_BATCH_RESOURCES\n\n\nverbose\nbool\nVerbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nThe resulting TaskGraph’s server UUID.\n\n\n\n\n\n\n\ncloud.files.ingestion.ingest_files_udf(\n    dataset_uri,\n    file_uris,\n    *,\n    acn=None,\n    namespace=None,\n    verbose=False,\n)\nIngest files.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\nThe dataset URI.\nrequired\n\n\nfile_uris\nSequence[str]\nAn iterable of file URIs.\nrequired\n\n\nacn\nOptional[str]\nAccess Credentials Name (ACN) registered in TileDB Cloud (ARN type), defaults to None\nNone\n\n\nnamespace\nOptional[str]\nTileDB-Cloud namespace, defaults to None.\nNone\n\n\nverbose\nbool\nVerbose logging, defaults to False.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nList[str]\nA list of the ingested files’ resulting URIs.",
    "crumbs": [
      "Get Started",
      "Analyze",
      "files.ingestion"
    ]
  },
  {
    "objectID": "reference/files.ingestion.html#attributes",
    "href": "reference/files.ingestion.html#attributes",
    "title": "files.ingestion",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ningest_and_index\nIngest files into a dataset and index them afterwards.",
    "crumbs": [
      "Get Started",
      "Analyze",
      "files.ingestion"
    ]
  },
  {
    "objectID": "reference/files.ingestion.html#functions",
    "href": "reference/files.ingestion.html#functions",
    "title": "files.ingestion",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nadd_arrays_to_group_udf\nAdd a list of TileDB array uris in a TileDB group.\n\n\ningest_files\nIngest files into a dataset.\n\n\ningest_files_udf\nIngest files.\n\n\n\n\n\ncloud.files.ingestion.add_arrays_to_group_udf(\n    array_uris,\n    group_uri,\n    *,\n    config=None,\n    verbose=False,\n)\nAdd a list of TileDB array uris in a TileDB group.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\narray_uris\nSequence[str]\nAn iterable of TileDB URIs.\nrequired\n\n\ngroup_uri\nstr\nA TileDB Group URI.\nrequired\n\n\nconfig\nOptional[dict]\nConfig dictionary, defaults to None\nNone\n\n\nverbose\nbool\nVerbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\ncloud.files.ingestion.ingest_files(\n    dataset_uri,\n    *,\n    search_uri=None,\n    pattern=None,\n    ignore=None,\n    max_files=None,\n    batch_size=file_udfs.DEFAULT_BATCH_SIZE,\n    acn=None,\n    config=None,\n    namespace=None,\n    group_uri=None,\n    taskgraph_name=DEFAULT_FILE_INGESTION_NAME,\n    ingest_resources=dag.MIN_BATCH_RESOURCES,\n    verbose=False,\n)\nIngest files into a dataset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\nThe dataset URI\nrequired\n\n\nsearch_uri\nOptional[Union[Sequence[str], str]]\nURI or an iterable of URIs of input files. Defaults to None.\nNone\n\n\npattern\nOptional[str]\nUNIX shell style pattern to filter files in the search, defaults to None\nNone\n\n\nignore\nOptional[str]\nUNIX shell style pattern to filter files out of the search, defaults to None\nNone\n\n\nmax_files\nOptional[int]\nmaximum number of File URIs to read/find, defaults to None (no limit)\nNone\n\n\nbatch_size\nOptional[int]\nBatch size for file ingestion, defaults to 100.\nfile_udfs.DEFAULT_BATCH_SIZE\n\n\nacn\nOptional[str]\nAccess Credentials Name (ACN) registered in TileDB Cloud (ARN type), defaults to None\nNone\n\n\nconfig\nOptional[dict]\nConfig dictionary, defaults to None\nNone\n\n\nnamespace\nOptional[str]\nTileDB-Cloud namespace, defaults to None\nNone\n\n\ngroup_uri\nOptional[str]\nA TileDB Group URI, defaults to None.\nNone\n\n\ntaskgraph_name\nOptional[str]\nOptional name for taskgraph, defaults to “file-ingestion”.\nDEFAULT_FILE_INGESTION_NAME\n\n\ningest_resources\nOptional[Mapping[str, Any]]\nConfiguration for node specs, defaults to {“cpu”: “1”, “memory”: “2Gi”}\ndag.MIN_BATCH_RESOURCES\n\n\nverbose\nbool\nVerbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nThe resulting TaskGraph’s server UUID.\n\n\n\n\n\n\n\ncloud.files.ingestion.ingest_files_udf(\n    dataset_uri,\n    file_uris,\n    *,\n    acn=None,\n    namespace=None,\n    verbose=False,\n)\nIngest files.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\nThe dataset URI.\nrequired\n\n\nfile_uris\nSequence[str]\nAn iterable of file URIs.\nrequired\n\n\nacn\nOptional[str]\nAccess Credentials Name (ACN) registered in TileDB Cloud (ARN type), defaults to None\nNone\n\n\nnamespace\nOptional[str]\nTileDB-Cloud namespace, defaults to None.\nNone\n\n\nverbose\nbool\nVerbose logging, defaults to False.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nList[str]\nA list of the ingested files’ resulting URIs.",
    "crumbs": [
      "Get Started",
      "Analyze",
      "files.ingestion"
    ]
  },
  {
    "objectID": "reference/utilities.consolidate.html",
    "href": "reference/utilities.consolidate.html",
    "title": "utilities.consolidate",
    "section": "",
    "text": "cloud.utilities.consolidate\n\n\n\n\n\nName\nDescription\n\n\n\n\nconsolidate\nConsolidate fragments\n\n\nconsolidate_and_vacuum\nConsolidate and vacuum commits and fragment metadata, with an option to\n\n\nconsolidate_fragments\nConsolidate fragments in an array.\n\n\ngroup_fragments\nGet a list of fragment info objects, optionally grouping fragments that have the\n\n\n\n\n\ncloud.utilities.consolidate.consolidate(\n    array_uri,\n    fragments,\n    *,\n    config=None,\n    max_fragment_size=MAX_FRAGMENT_SIZE_BYTES,\n)\nConsolidate fragments\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\narray_uri\nstr\narray URI\nrequired\n\n\nfragments\nSequence[tiledb.FragmentInfo]\nlist of fragments\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\nmax_fragment_size\nint\nmax size of consolidated fragments, defaults to MAX_FRAGMENT_SIZE_BYTES\nMAX_FRAGMENT_SIZE_BYTES\n\n\n\n\n\n\n\ncloud.utilities.consolidate.consolidate_and_vacuum(\n    array_uri,\n    *,\n    config=None,\n    vacuum_fragments=False,\n)\nConsolidate and vacuum commits and fragment metadata, with an option to vacuum fragments as the first step.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\narray_uri\nstr\narray URI\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\nvacuum_fragments\nbool\nvacuum fragments first, defaults to False\nFalse\n\n\n\n\n\n\n\ncloud.utilities.consolidate.consolidate_fragments(\n    array_uri,\n    *,\n    acn=None,\n    config=None,\n    group_by_first_dim=False,\n    graph=None,\n    dependencies=None,\n    consolidate_resources=None,\n    namespace=None,\n    max_fragment_size=MAX_FRAGMENT_SIZE_BYTES,\n)\nConsolidate fragments in an array.\nIf group_by_first_dim is True, fragments with the same value for the first dimension will be consolidated together. Otherwise, all fragments will be consolidated together.\nIf graph is provided, the consolidation task nodes will be submitted to the graph. If dependencies is provided, the consolidation nodes will depend on the nodes in the list.\nIf graph is not provided, a new graph will be created and submitted to TileDB Cloud.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\narray_uri\nstr\narray URI\nrequired\n\n\nacn\nOptional[str]\nAccess Credentials Name (ACN) registered in TileDB Cloud (ARN type), defaults to None\nNone\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\ngroup_by_first_dim\nbool\ngroup fragment by first dimension, defaults to True\nFalse\n\n\ngraph\nOptional[dag.DAG]\ngraph to submit nodes to, defaults to None\nNone\n\n\ndependencies\nOptional[Sequence[dag.Node]]\nlist of nodes in the graph to depend on, defaults to None\nNone\n\n\nconsolidate_resources\nOptional[Mapping[str, str]]\nresources for the consolidate node, defaults to None\nNone\n\n\nnamespace\nOptional[str]\nTileDB Cloud namespace, defaults to the user’s default namespace\nNone\n\n\nmax_fragment_size\nint\nmax size of consolidated fragments, defaults to MAX_FRAGMENT_SIZE_BYTES\nMAX_FRAGMENT_SIZE_BYTES\n\n\n\n\n\n\n\ncloud.utilities.consolidate.group_fragments(\n    array_uri,\n    *,\n    config=None,\n    group_by_first_dim=True,\n)\nGet a list of fragment info objects, optionally grouping fragments that have the same value for the first dimension.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\narray_uri\nstr\narray URI\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\ngroup_by_first_dim\nbool\ngroup by first dimension, defaults to True\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nSequence[Sequence[tiledb.FragmentInfo]]\nlist of lists of fragment info objects",
    "crumbs": [
      "Get Started",
      "Scale",
      "utilities.consolidate"
    ]
  },
  {
    "objectID": "reference/utilities.consolidate.html#functions",
    "href": "reference/utilities.consolidate.html#functions",
    "title": "utilities.consolidate",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nconsolidate\nConsolidate fragments\n\n\nconsolidate_and_vacuum\nConsolidate and vacuum commits and fragment metadata, with an option to\n\n\nconsolidate_fragments\nConsolidate fragments in an array.\n\n\ngroup_fragments\nGet a list of fragment info objects, optionally grouping fragments that have the\n\n\n\n\n\ncloud.utilities.consolidate.consolidate(\n    array_uri,\n    fragments,\n    *,\n    config=None,\n    max_fragment_size=MAX_FRAGMENT_SIZE_BYTES,\n)\nConsolidate fragments\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\narray_uri\nstr\narray URI\nrequired\n\n\nfragments\nSequence[tiledb.FragmentInfo]\nlist of fragments\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\nmax_fragment_size\nint\nmax size of consolidated fragments, defaults to MAX_FRAGMENT_SIZE_BYTES\nMAX_FRAGMENT_SIZE_BYTES\n\n\n\n\n\n\n\ncloud.utilities.consolidate.consolidate_and_vacuum(\n    array_uri,\n    *,\n    config=None,\n    vacuum_fragments=False,\n)\nConsolidate and vacuum commits and fragment metadata, with an option to vacuum fragments as the first step.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\narray_uri\nstr\narray URI\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\nvacuum_fragments\nbool\nvacuum fragments first, defaults to False\nFalse\n\n\n\n\n\n\n\ncloud.utilities.consolidate.consolidate_fragments(\n    array_uri,\n    *,\n    acn=None,\n    config=None,\n    group_by_first_dim=False,\n    graph=None,\n    dependencies=None,\n    consolidate_resources=None,\n    namespace=None,\n    max_fragment_size=MAX_FRAGMENT_SIZE_BYTES,\n)\nConsolidate fragments in an array.\nIf group_by_first_dim is True, fragments with the same value for the first dimension will be consolidated together. Otherwise, all fragments will be consolidated together.\nIf graph is provided, the consolidation task nodes will be submitted to the graph. If dependencies is provided, the consolidation nodes will depend on the nodes in the list.\nIf graph is not provided, a new graph will be created and submitted to TileDB Cloud.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\narray_uri\nstr\narray URI\nrequired\n\n\nacn\nOptional[str]\nAccess Credentials Name (ACN) registered in TileDB Cloud (ARN type), defaults to None\nNone\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\ngroup_by_first_dim\nbool\ngroup fragment by first dimension, defaults to True\nFalse\n\n\ngraph\nOptional[dag.DAG]\ngraph to submit nodes to, defaults to None\nNone\n\n\ndependencies\nOptional[Sequence[dag.Node]]\nlist of nodes in the graph to depend on, defaults to None\nNone\n\n\nconsolidate_resources\nOptional[Mapping[str, str]]\nresources for the consolidate node, defaults to None\nNone\n\n\nnamespace\nOptional[str]\nTileDB Cloud namespace, defaults to the user’s default namespace\nNone\n\n\nmax_fragment_size\nint\nmax size of consolidated fragments, defaults to MAX_FRAGMENT_SIZE_BYTES\nMAX_FRAGMENT_SIZE_BYTES\n\n\n\n\n\n\n\ncloud.utilities.consolidate.group_fragments(\n    array_uri,\n    *,\n    config=None,\n    group_by_first_dim=True,\n)\nGet a list of fragment info objects, optionally grouping fragments that have the same value for the first dimension.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\narray_uri\nstr\narray URI\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\ngroup_by_first_dim\nbool\ngroup by first dimension, defaults to True\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nSequence[Sequence[tiledb.FragmentInfo]]\nlist of lists of fragment info objects",
    "crumbs": [
      "Get Started",
      "Scale",
      "utilities.consolidate"
    ]
  },
  {
    "objectID": "reference/bioimg.exportation.html",
    "href": "reference/bioimg.exportation.html",
    "title": "bioimg.exportation",
    "section": "",
    "text": "cloud.bioimg.exportation\n\n\n\n\n\nName\nDescription\n\n\n\n\nexport\nThe function exports microscopy images from TileDB arrays\n\n\n\n\n\ncloud.bioimg.exportation.export(\n    source,\n    output,\n    *args,\n    access_credentials_name,\n    config=None,\n    taskgraph_name=None,\n    num_batches=None,\n    resources=None,\n    compute=True,\n    mode=Mode.BATCH,\n    namespace=None,\n    verbose=False,\n    output_ext='tiff',\n    **kwargs,\n)\nThe function exports microscopy images from TileDB arrays\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource\nUnion[Sequence[str], str]\nuri / iterable of uris of input files If the uri points to a directory of files make sure it ends with a trailing ‘/’\nrequired\n\n\noutput\nUnion[Sequence[str], str]\nuri / iterable of uris of input files. If the uri points to a directory of files make sure it ends with a trailing ‘/’\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\ndict configuration to pass credentials of the destination\nNone\n\n\ntaskgraph_name\nOptional[str]\nOptional name for taskgraph, defaults to None\nNone\n\n\nnum_batches\nOptional[int]\nNumber of graph nodes to spawn. Performs it sequentially if default, defaults to 1\nNone\n\n\nthreads\n\nNumber of threads for node side multiprocessing, defaults to 8\nrequired\n\n\nresources\nOptional[Mapping[str, Any]]\nconfiguration for node specs e.g. {“cpu”: “8”, “memory”: “4Gi”}, defaults to None\nNone\n\n\ncompute\nbool\nWhen True the DAG returned will be computed inside the function otherwise DAG will only be returned.\nTrue\n\n\nmode\nOptional[Mode]\nBy default runs Mode.Batch\nMode.BATCH\n\n\nnamespace\nOptional[str]\nThe namespace where the DAG will run\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\naccess_credentials_name\nstr\nAccess Credentials Name (ACN) registered in TileDB Cloud (ARN type)\nrequired\n\n\noutput_ext\nstr\nextension for the output images in tiledb\n'tiff'",
    "crumbs": [
      "Get Started",
      "Analyze",
      "bioimg.exportation"
    ]
  },
  {
    "objectID": "reference/bioimg.exportation.html#functions",
    "href": "reference/bioimg.exportation.html#functions",
    "title": "bioimg.exportation",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nexport\nThe function exports microscopy images from TileDB arrays\n\n\n\n\n\ncloud.bioimg.exportation.export(\n    source,\n    output,\n    *args,\n    access_credentials_name,\n    config=None,\n    taskgraph_name=None,\n    num_batches=None,\n    resources=None,\n    compute=True,\n    mode=Mode.BATCH,\n    namespace=None,\n    verbose=False,\n    output_ext='tiff',\n    **kwargs,\n)\nThe function exports microscopy images from TileDB arrays\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource\nUnion[Sequence[str], str]\nuri / iterable of uris of input files If the uri points to a directory of files make sure it ends with a trailing ‘/’\nrequired\n\n\noutput\nUnion[Sequence[str], str]\nuri / iterable of uris of input files. If the uri points to a directory of files make sure it ends with a trailing ‘/’\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\ndict configuration to pass credentials of the destination\nNone\n\n\ntaskgraph_name\nOptional[str]\nOptional name for taskgraph, defaults to None\nNone\n\n\nnum_batches\nOptional[int]\nNumber of graph nodes to spawn. Performs it sequentially if default, defaults to 1\nNone\n\n\nthreads\n\nNumber of threads for node side multiprocessing, defaults to 8\nrequired\n\n\nresources\nOptional[Mapping[str, Any]]\nconfiguration for node specs e.g. {“cpu”: “8”, “memory”: “4Gi”}, defaults to None\nNone\n\n\ncompute\nbool\nWhen True the DAG returned will be computed inside the function otherwise DAG will only be returned.\nTrue\n\n\nmode\nOptional[Mode]\nBy default runs Mode.Batch\nMode.BATCH\n\n\nnamespace\nOptional[str]\nThe namespace where the DAG will run\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\naccess_credentials_name\nstr\nAccess Credentials Name (ACN) registered in TileDB Cloud (ARN type)\nrequired\n\n\noutput_ext\nstr\nextension for the output images in tiledb\n'tiff'",
    "crumbs": [
      "Get Started",
      "Analyze",
      "bioimg.exportation"
    ]
  },
  {
    "objectID": "reference/taskgraphs.registration.html",
    "href": "reference/taskgraphs.registration.html",
    "title": "taskgraphs.registration",
    "section": "",
    "text": "cloud.taskgraphs.registration\n\n\n\n\n\nName\nDescription\n\n\n\n\ndelete\nDeletes the given task graph.\n\n\nload\nRetrieves the given task graph from the server.\n\n\nregister\nRegisters the graph constructed by the TaskGraphBuilder.\n\n\nupdate\nUpdates the registered task graph at the given location.\n\n\n\n\n\ncloud.taskgraphs.registration.delete(name_or_nsname, *, namespace=None)\nDeletes the given task graph.\nThis deregisters the graph and also removes the graph array from storage.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname_or_nsname\nstr\nThe graph’s identifier, either in the form namespace/name, or just name to use the namespace param.\nrequired\n\n\nnamespace\nOptional[str]\nIf set, the namespace of the graph. If name_or_nsname is of the form namespace/name, must be None. If name_or_nsname is just a name and this is None, will use the current user’s namespace.\nNone\n\n\n\n\n\n\n\ncloud.taskgraphs.registration.load(name_or_nsname, *, namespace=None)\nRetrieves the given task graph from the server.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname_or_nsname\nstr\nThe graph’s identifier, either in the form namespace/name, or just name to use the namespace param.\nrequired\n\n\nnamespace\nOptional[str]\nIf set, the namespace of the graph. If name_or_nsname is of the form namespace/name, must be None. If name_or_nsname is just a name and this is None, will use the current user’s namespace.\nNone\n\n\n\n\n\n\n\ncloud.taskgraphs.registration.register(graph, name=None, *, namespace=None)\nRegisters the graph constructed by the TaskGraphBuilder.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ngraph\nbuilder.TaskGraphBuilder\nThe graph to register.\nrequired\n\n\nname\nOptional[str]\nThe name to register the graph with. By default, will use the name specified in graph. This must be a bare name, with no namespace (i.e. my-graph, not me/my-graph).\nNone\n\n\nnamespace\nOptional[str]\nThe namespace, if not your own, to register the graph in.\nNone\n\n\n\n\n\n\n\ncloud.taskgraphs.registration.update(graph, old_name=None, *, namespace=None)\nUpdates the registered task graph at the given location.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ngraph\nbuilder.TaskGraphBuilder\nThe new graph to replace the old value.\nrequired\n\n\nold_name\nOptional[str]\nThe name of the graph to rename from, if present. If graph is to be renamed, the new name of the graph must appear in graph.name.\nNone\n\n\nnamespace\nOptional[str]\nThe namespace, if not your own, where the graph will be updated.\nNone",
    "crumbs": [
      "Get Started",
      "Scale",
      "taskgraphs.registration"
    ]
  },
  {
    "objectID": "reference/taskgraphs.registration.html#functions",
    "href": "reference/taskgraphs.registration.html#functions",
    "title": "taskgraphs.registration",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndelete\nDeletes the given task graph.\n\n\nload\nRetrieves the given task graph from the server.\n\n\nregister\nRegisters the graph constructed by the TaskGraphBuilder.\n\n\nupdate\nUpdates the registered task graph at the given location.\n\n\n\n\n\ncloud.taskgraphs.registration.delete(name_or_nsname, *, namespace=None)\nDeletes the given task graph.\nThis deregisters the graph and also removes the graph array from storage.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname_or_nsname\nstr\nThe graph’s identifier, either in the form namespace/name, or just name to use the namespace param.\nrequired\n\n\nnamespace\nOptional[str]\nIf set, the namespace of the graph. If name_or_nsname is of the form namespace/name, must be None. If name_or_nsname is just a name and this is None, will use the current user’s namespace.\nNone\n\n\n\n\n\n\n\ncloud.taskgraphs.registration.load(name_or_nsname, *, namespace=None)\nRetrieves the given task graph from the server.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname_or_nsname\nstr\nThe graph’s identifier, either in the form namespace/name, or just name to use the namespace param.\nrequired\n\n\nnamespace\nOptional[str]\nIf set, the namespace of the graph. If name_or_nsname is of the form namespace/name, must be None. If name_or_nsname is just a name and this is None, will use the current user’s namespace.\nNone\n\n\n\n\n\n\n\ncloud.taskgraphs.registration.register(graph, name=None, *, namespace=None)\nRegisters the graph constructed by the TaskGraphBuilder.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ngraph\nbuilder.TaskGraphBuilder\nThe graph to register.\nrequired\n\n\nname\nOptional[str]\nThe name to register the graph with. By default, will use the name specified in graph. This must be a bare name, with no namespace (i.e. my-graph, not me/my-graph).\nNone\n\n\nnamespace\nOptional[str]\nThe namespace, if not your own, to register the graph in.\nNone\n\n\n\n\n\n\n\ncloud.taskgraphs.registration.update(graph, old_name=None, *, namespace=None)\nUpdates the registered task graph at the given location.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ngraph\nbuilder.TaskGraphBuilder\nThe new graph to replace the old value.\nrequired\n\n\nold_name\nOptional[str]\nThe name of the graph to rename from, if present. If graph is to be renamed, the new name of the graph must appear in graph.name.\nNone\n\n\nnamespace\nOptional[str]\nThe namespace, if not your own, where the graph will be updated.\nNone",
    "crumbs": [
      "Get Started",
      "Scale",
      "taskgraphs.registration"
    ]
  },
  {
    "objectID": "reference/cloudarray.html",
    "href": "reference/cloudarray.html",
    "title": "cloudarray",
    "section": "",
    "text": "cloud.cloudarray\n\n\n\n\n\nName\nDescription\n\n\n\n\nCloudArray\nArray cloud interface.\n\n\n\n\n\ncloud.cloudarray.CloudArray()\nArray cloud interface.\n\n\n\n\n\nName\nDescription\n\n\n\n\napply\nApply a user-defined function to this array, synchronously.\n\n\napply_async\nApply a user-defined function to this array, asynchronously.\n\n\n\n\n\ncloud.cloudarray.CloudArray.apply(*args, **kwargs)\nApply a user-defined function to this array, synchronously.\nParams are the same as array.apply, but this instance provides the URI.\nExample\nimport tiledb, tiledb.cloud, numpy\n\ndef median(df):\n    return numpy.median(df[\"a\"])\n\n# Open the array then run the UDF\nwith tiledb.SparseArray(\"tiledb://TileDB-Inc/quickstart_dense\", ctx=tiledb.cloud.ctx()) as A:\n    A.apply(median, [(0,5), (0,5)], attrs=[\"a\", \"b\", \"c\"])\n\n\n\ncloud.cloudarray.CloudArray.apply_async(*args, **kwargs)\nApply a user-defined function to this array, asynchronously.\nParams are the same as array.apply_async, but this instance provides the URI.",
    "crumbs": [
      "Get Started",
      "Scale",
      "cloudarray"
    ]
  },
  {
    "objectID": "reference/cloudarray.html#classes",
    "href": "reference/cloudarray.html#classes",
    "title": "cloudarray",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nCloudArray\nArray cloud interface.\n\n\n\n\n\ncloud.cloudarray.CloudArray()\nArray cloud interface.\n\n\n\n\n\nName\nDescription\n\n\n\n\napply\nApply a user-defined function to this array, synchronously.\n\n\napply_async\nApply a user-defined function to this array, asynchronously.\n\n\n\n\n\ncloud.cloudarray.CloudArray.apply(*args, **kwargs)\nApply a user-defined function to this array, synchronously.\nParams are the same as array.apply, but this instance provides the URI.\nExample\nimport tiledb, tiledb.cloud, numpy\n\ndef median(df):\n    return numpy.median(df[\"a\"])\n\n# Open the array then run the UDF\nwith tiledb.SparseArray(\"tiledb://TileDB-Inc/quickstart_dense\", ctx=tiledb.cloud.ctx()) as A:\n    A.apply(median, [(0,5), (0,5)], attrs=[\"a\", \"b\", \"c\"])\n\n\n\ncloud.cloudarray.CloudArray.apply_async(*args, **kwargs)\nApply a user-defined function to this array, asynchronously.\nParams are the same as array.apply_async, but this instance provides the URI.",
    "crumbs": [
      "Get Started",
      "Scale",
      "cloudarray"
    ]
  },
  {
    "objectID": "reference/notebook.html",
    "href": "reference/notebook.html",
    "title": "notebook",
    "section": "",
    "text": "cloud.notebook\nPython support for notebook I/O on Tiledb Cloud. All notebook JSON content is assumed to be encoded as UTF-8.\n\n\n\n\n\nName\nDescription\n\n\n\n\nOnExists\nAction to take if the array already exists.\n\n\n\n\n\ncloud.notebook.OnExists()\nAction to take if the array already exists.\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ndownload_notebook_contents\nDownloads a notebook file from TileDB Cloud to contents as a string,\n\n\ndownload_notebook_to_file\nDownloads a notebook file from TileDB Cloud to local disk.\n\n\nrename_notebook\nUpdate an array’s info\n\n\nupload_notebook_contents\nUploads a notebook file to TileDB Cloud.\n\n\nupload_notebook_from_file\nUploads a local-disk notebook file to TileDB Cloud.\n\n\n\n\n\ncloud.notebook.download_notebook_contents(tiledb_uri, *, timestamp=None)\nDownloads a notebook file from TileDB Cloud to contents as a string, nominally in JSON format.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntiledb_uri\nstr\nsuch as “tiledb://TileDB-Inc/quickstart_dense”.\nrequired\n\n\ntimestamp\nUnion[int, datetime.datetime, None]\nIf set, the timestamp to download the notebook as of.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\ncontents of the notebook file as a string, nominally in JSON format.\n\n\n\n\n\n\n\ncloud.notebook.download_notebook_to_file(\n    tiledb_uri,\n    ipynb_file_name,\n    *,\n    timestamp=None,\n)\nDownloads a notebook file from TileDB Cloud to local disk.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntiledb_uri\nstr\nsuch as “tiledb://TileDB-Inc/quickstart_dense”.\nrequired\n\n\nipynb_file_name\nstr\npath to save to, such as “./mycopy.ipynb”. Must be local; no other VFS backends are currently supported.\nrequired\n\n\ntimestamp\nUnion[int, datetime.datetime, None]\nIf set, the timestamp to download the notebook as of.\nNone\n\n\n\n\n\n\n\ncloud.notebook.rename_notebook(\n    tiledb_uri,\n    notebook_name=None,\n    access_credentials_name=None,\n    async_req=False,\n)\nUpdate an array’s info\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntiledb_uri\nstr\nsuch as “tiledb://TileDB-Inc/quickstart_dense”.\nrequired\n\n\nnotebook_name\nstr\nsuch as “quickstart_dense_new_name”.\nNone\n\n\naccess_credentials_name\nstr\noptional name of access credentials to use. If left blank. default for namespace will be used.\nNone\n\n\nasync_req\nbool\nreturn future instead of results for async support.\nFalse\n\n\n\n\n\n\n\ncloud.notebook.upload_notebook_contents(\n    ipynb_file_contents,\n    *,\n    dest_uri=None,\n    namespace=None,\n    array_name=None,\n    storage_path=None,\n    storage_credential_name=None,\n    on_exists,\n)\nUploads a notebook file to TileDB Cloud.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nipynb_file_contents\nstr\nThe contents of the notebook file as a string, nominally in JSON format.\nrequired\n\n\ndest_uri\nOptional[str]\nThe destination URI to upload the notebook to, such as tiledb://janedoe/testing-upload. If this is set, namespace and array_name may not be set.\nNone\n\n\nstorage_path\nOptional[str]\nsuch as “s3://acmecorp-janedoe”, typically from the user’s account settings.\nNone\n\n\n\narray_name\nname to be seen in the UI, such as “testing-upload”\nrequired\n\n\nnamespace\nOptional[str]\nsuch as “janedoe”.\nNone\n\n\nstorage_credential_name\nOptional[str]\nsuch as “janedoe-creds”, typically from the user’s account settings.\nNone\n\n\non_exists\nOnExists\nsuch as OnExists.FAIL (default), OVERWRITE or AUTO-INCREMENT\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nTileDB array name, such as “tiledb://janedoe/testing-upload”.\n\n\n\n\n\n\n\ncloud.notebook.upload_notebook_from_file(\n    ipynb_file_name,\n    *,\n    dest_uri=None,\n    namespace=None,\n    array_name=None,\n    storage_path=None,\n    storage_credential_name=None,\n    on_exists=OnExists.FAIL,\n)\nUploads a local-disk notebook file to TileDB Cloud.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nipnyb_file_name\n\nsuch as “./mycopy.ipynb”. Must be local; no S3 URI support at present.\nrequired\n\n\ndest_uri\nOptional[str]\nThe destination URI to upload the notebook to, such as tiledb://janedoe/testing-upload. If this is set, namespace and array_name may not be set.\nNone\n\n\nnamespace\nOptional[str]\nsuch as “janedoe”.\nNone\n\n\narray_name\nOptional[str]\nname to be seen in the UI, such as “testing-upload”.\nNone\n\n\nstorage_path\nOptional[str]\nsuch as “s3://acmecorp-janedoe”, typically from the user’s account settings.\nNone\n\n\nstorage_credential_name\nOptional[str]\nsuch as “janedoe-creds”, typically from the user’s account settings.\nNone\n\n\non_exists\nOnExists\nsuch as OnExists.FAIL (default), OVERWRITE or AUTO-INCREMENT\nOnExists.FAIL\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nTileDB array name, such as “tiledb://janedoe/testing-upload”.",
    "crumbs": [
      "Get Started",
      "Catalog",
      "notebook"
    ]
  },
  {
    "objectID": "reference/notebook.html#classes",
    "href": "reference/notebook.html#classes",
    "title": "notebook",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nOnExists\nAction to take if the array already exists.\n\n\n\n\n\ncloud.notebook.OnExists()\nAction to take if the array already exists.",
    "crumbs": [
      "Get Started",
      "Catalog",
      "notebook"
    ]
  },
  {
    "objectID": "reference/notebook.html#functions",
    "href": "reference/notebook.html#functions",
    "title": "notebook",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndownload_notebook_contents\nDownloads a notebook file from TileDB Cloud to contents as a string,\n\n\ndownload_notebook_to_file\nDownloads a notebook file from TileDB Cloud to local disk.\n\n\nrename_notebook\nUpdate an array’s info\n\n\nupload_notebook_contents\nUploads a notebook file to TileDB Cloud.\n\n\nupload_notebook_from_file\nUploads a local-disk notebook file to TileDB Cloud.\n\n\n\n\n\ncloud.notebook.download_notebook_contents(tiledb_uri, *, timestamp=None)\nDownloads a notebook file from TileDB Cloud to contents as a string, nominally in JSON format.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntiledb_uri\nstr\nsuch as “tiledb://TileDB-Inc/quickstart_dense”.\nrequired\n\n\ntimestamp\nUnion[int, datetime.datetime, None]\nIf set, the timestamp to download the notebook as of.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\ncontents of the notebook file as a string, nominally in JSON format.\n\n\n\n\n\n\n\ncloud.notebook.download_notebook_to_file(\n    tiledb_uri,\n    ipynb_file_name,\n    *,\n    timestamp=None,\n)\nDownloads a notebook file from TileDB Cloud to local disk.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntiledb_uri\nstr\nsuch as “tiledb://TileDB-Inc/quickstart_dense”.\nrequired\n\n\nipynb_file_name\nstr\npath to save to, such as “./mycopy.ipynb”. Must be local; no other VFS backends are currently supported.\nrequired\n\n\ntimestamp\nUnion[int, datetime.datetime, None]\nIf set, the timestamp to download the notebook as of.\nNone\n\n\n\n\n\n\n\ncloud.notebook.rename_notebook(\n    tiledb_uri,\n    notebook_name=None,\n    access_credentials_name=None,\n    async_req=False,\n)\nUpdate an array’s info\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntiledb_uri\nstr\nsuch as “tiledb://TileDB-Inc/quickstart_dense”.\nrequired\n\n\nnotebook_name\nstr\nsuch as “quickstart_dense_new_name”.\nNone\n\n\naccess_credentials_name\nstr\noptional name of access credentials to use. If left blank. default for namespace will be used.\nNone\n\n\nasync_req\nbool\nreturn future instead of results for async support.\nFalse\n\n\n\n\n\n\n\ncloud.notebook.upload_notebook_contents(\n    ipynb_file_contents,\n    *,\n    dest_uri=None,\n    namespace=None,\n    array_name=None,\n    storage_path=None,\n    storage_credential_name=None,\n    on_exists,\n)\nUploads a notebook file to TileDB Cloud.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nipynb_file_contents\nstr\nThe contents of the notebook file as a string, nominally in JSON format.\nrequired\n\n\ndest_uri\nOptional[str]\nThe destination URI to upload the notebook to, such as tiledb://janedoe/testing-upload. If this is set, namespace and array_name may not be set.\nNone\n\n\nstorage_path\nOptional[str]\nsuch as “s3://acmecorp-janedoe”, typically from the user’s account settings.\nNone\n\n\n\narray_name\nname to be seen in the UI, such as “testing-upload”\nrequired\n\n\nnamespace\nOptional[str]\nsuch as “janedoe”.\nNone\n\n\nstorage_credential_name\nOptional[str]\nsuch as “janedoe-creds”, typically from the user’s account settings.\nNone\n\n\non_exists\nOnExists\nsuch as OnExists.FAIL (default), OVERWRITE or AUTO-INCREMENT\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nTileDB array name, such as “tiledb://janedoe/testing-upload”.\n\n\n\n\n\n\n\ncloud.notebook.upload_notebook_from_file(\n    ipynb_file_name,\n    *,\n    dest_uri=None,\n    namespace=None,\n    array_name=None,\n    storage_path=None,\n    storage_credential_name=None,\n    on_exists=OnExists.FAIL,\n)\nUploads a local-disk notebook file to TileDB Cloud.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nipnyb_file_name\n\nsuch as “./mycopy.ipynb”. Must be local; no S3 URI support at present.\nrequired\n\n\ndest_uri\nOptional[str]\nThe destination URI to upload the notebook to, such as tiledb://janedoe/testing-upload. If this is set, namespace and array_name may not be set.\nNone\n\n\nnamespace\nOptional[str]\nsuch as “janedoe”.\nNone\n\n\narray_name\nOptional[str]\nname to be seen in the UI, such as “testing-upload”.\nNone\n\n\nstorage_path\nOptional[str]\nsuch as “s3://acmecorp-janedoe”, typically from the user’s account settings.\nNone\n\n\nstorage_credential_name\nOptional[str]\nsuch as “janedoe-creds”, typically from the user’s account settings.\nNone\n\n\non_exists\nOnExists\nsuch as OnExists.FAIL (default), OVERWRITE or AUTO-INCREMENT\nOnExists.FAIL\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nTileDB array name, such as “tiledb://janedoe/testing-upload”.",
    "crumbs": [
      "Get Started",
      "Catalog",
      "notebook"
    ]
  },
  {
    "objectID": "reference/asset.html",
    "href": "reference/asset.html",
    "title": "asset",
    "section": "",
    "text": "cloud.asset\nAn asset may be an array or a group.\n\n\n\n\n\nName\nDescription\n\n\n\n\ndelete\nDeregister an asset and remove its objects from storage.\n\n\nderegister\nDeregister an asset.\n\n\ninfo\nRetrieve information about an asset.\n\n\nlist\nList/search for stored assets.\n\n\nlist_public\nList/search for publicly-shared assets.\n\n\nlist_shared_with\nList an asset’s sharing policies.\n\n\nregister\nRegister stored objects as an asset.\n\n\nshare\nGive another namespace permission to access an asset.\n\n\nunshare\nRemove access permissions for another namespace.\n\n\nupdate_info\nUpdate asset info settings.\n\n\n\n\n\ncloud.asset.delete(uri, *, recursive=False)\nDeregister an asset and remove its objects from storage.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\ntiledb URI of the asset.\nrequired\n\n\nrecursive\nOptional[bool]\nif True, contents of an asset will be recursively deleted. Default: False.\nFalse\n\n\n\n\n\n\n\ncloud.asset.deregister(uri, *, recursive=False)\nDeregister an asset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\ntiledb URI of the asset.\nrequired\n\n\nrecursive\nOptional[bool]\nif True, contents of an asset will be recursively deregistered Default: False.\nFalse\n\n\n\n\n\n\n\ncloud.asset.info(uri)\nRetrieve information about an asset.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\ntiledb URI of the asset.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nUnion[models.ArrayInfo, models.GroupInfo]\nArrayInfo or GroupInfo.\n\n\n\n\n\n\n\ncloud.asset.list(\n    namespace=None,\n    search=None,\n    type=None,\n    ownership_level=None,\n    depth=None,\n    expand=None,\n    page=None,\n    per_page=None,\n    order_by=None,\n)\nList/search for stored assets.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnamespace\nOptional[str]\nThe namespace to use, or the current user if absent.\nNone\n\n\nsearch\nOptional[str]\nA search string to use.\nNone\n\n\ntype\nOptional[_AssetType]\nIf provided, include only assets of the specified type (“array” or “group”).\nNone\n\n\nownership_level\nOptional[_OwnershipLevel]\nIf provided, include only assets you own (“owned”), or only assets that are shared with you (“shared”).\nNone\n\n\ndepth\nOptional[_Depth]\nThe depth to provide return information. If “root”, only root assets (i.e., arrays and groups that are not contained within another group) will be returned. If “all”, all assets that match (including those which are contained in another group) will be included.\nNone\n\n\nexpand\nOptional[_CSVString]\nComma-separated string specifying additional information to include in the response. As of this writing, “metadata” is supported.\nNone\n\n\npage\nOptional[int]\nWhich page of results to retrieve. 1-based.\nNone\n\n\nper_page\nOptional[int]\nHow many results to include on each page.\nNone\n\n\norder_by\nOptional[str]\nThe order to return assets, by default “created_at desc”. Supported keys are “created_at”, “name”, and “asset_type”. They can be used alone or with “asc” or “desc” separated by a space (e.g. “created_at”, “asset_type asc”).\nNone\n\n\n\n\n\n\n\ncloud.asset.list_public(\n    search=None,\n    type=None,\n    depth=None,\n    page=None,\n    per_page=None,\n    order_by=None,\n)\nList/search for publicly-shared assets.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsearch\nOptional[str]\nA search string to use.\nNone\n\n\ntype\nOptional[_AssetType]\nIf provided, include only assets of the specified type (“array” or “group”).\nNone\n\n\ndepth\nOptional[_Depth]\nThe depth to provide return information. If “root”, only root assets (i.e., arrays and groups that are not contained within another group) will be returned. If “all”, all assets that match (including those which are contained in another group) will be included.\nNone\n\n\nexpand\n\nComma-separated string specifying additional information to include in the response. As of this writing, “metadata” is supported.\nrequired\n\n\npage\nOptional[int]\nWhich page of results to retrieve. 1-based.\nNone\n\n\nper_page\nOptional[int]\nHow many results to include on each page.\nNone\n\n\norder_by\nOptional[str]\nThe order to return assets, by default “created_at desc”. Supported keys are “created_at”, “name”, and “asset_type”. They can be used alone or with “asc” or “desc” separated by a space (e.g. “created_at”, “asset_type asc”).\nNone\n\n\n\n\n\n\n\ncloud.asset.list_shared_with(uri)\nList an asset’s sharing policies.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\ntiledb URI of the asset.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\na list of ArraySharing or GroupSharing objects.\n\n\n\n\n\n\n\ncloud.asset.register(\n    storage_uri,\n    type,\n    *,\n    dest_uri=None,\n    name=None,\n    namespace=None,\n    credentials_name=None,\n    parent_uri=None,\n)\nRegister stored objects as an asset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstorage_uri\nstr\nS3, for example, URI of the data to be registered.\nrequired\n\n\ntype\n_AssetType\nThe type of asset, “array” or “group”.\nrequired\n\n\nnamespace\nOptional[str]\nThe user or organization to register the asset under. If unset will default to the logged-in user’s namespace.\nNone\n\n\nname\nOptional[str]\nName of asset.\nNone\n\n\ndescription\nstr\nOptional description.\nrequired\n\n\ncredentials_name\nOptional[str]\nOptional name of access credentials to use. If omitted, the default for namespace will be used.\nNone\n\n\nparent_uri\nOptional[str]\nOptional parent URI for group type assets.\nNone\n\n\n\n\n\n\n\ncloud.asset.share(uri, namespace, permissions='read')\nGive another namespace permission to access an asset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\ntiledb URI of the asset.\nrequired\n\n\nnamespace\nstr\nthe target namespace.\nrequired\n\n\npermissions\nOptional[Union[str, List[str]]]\n‘read’, ‘write’, or [‘read’, ‘write’].\n'read'\n\n\n\n\n\n\n\ncloud.asset.unshare(uri, namespace)\nRemove access permissions for another namespace.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\ntiledb URI of the asset.\nrequired\n\n\nnamespace\nstr\nthe target namespace.\nrequired\n\n\n\n\n\n\n\ncloud.asset.update_info(\n    uri,\n    *,\n    description=None,\n    name=None,\n    tags=None,\n    access_credentials_name=None,\n)\nUpdate asset info settings.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\ntiledb URI of the asset.\nrequired\n\n\ndescription\nOptional[str]\nAsset description, defaults to None\nNone\n\n\nname\nOptional[str]\nAsset name, defaults to None\nNone\n\n\ntags\nOptional[List[str]]\nAsset tags, defaults to None\nNone",
    "crumbs": [
      "Get Started",
      "Catalog",
      "asset"
    ]
  },
  {
    "objectID": "reference/asset.html#functions",
    "href": "reference/asset.html#functions",
    "title": "asset",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndelete\nDeregister an asset and remove its objects from storage.\n\n\nderegister\nDeregister an asset.\n\n\ninfo\nRetrieve information about an asset.\n\n\nlist\nList/search for stored assets.\n\n\nlist_public\nList/search for publicly-shared assets.\n\n\nlist_shared_with\nList an asset’s sharing policies.\n\n\nregister\nRegister stored objects as an asset.\n\n\nshare\nGive another namespace permission to access an asset.\n\n\nunshare\nRemove access permissions for another namespace.\n\n\nupdate_info\nUpdate asset info settings.\n\n\n\n\n\ncloud.asset.delete(uri, *, recursive=False)\nDeregister an asset and remove its objects from storage.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\ntiledb URI of the asset.\nrequired\n\n\nrecursive\nOptional[bool]\nif True, contents of an asset will be recursively deleted. Default: False.\nFalse\n\n\n\n\n\n\n\ncloud.asset.deregister(uri, *, recursive=False)\nDeregister an asset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\ntiledb URI of the asset.\nrequired\n\n\nrecursive\nOptional[bool]\nif True, contents of an asset will be recursively deregistered Default: False.\nFalse\n\n\n\n\n\n\n\ncloud.asset.info(uri)\nRetrieve information about an asset.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\ntiledb URI of the asset.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nUnion[models.ArrayInfo, models.GroupInfo]\nArrayInfo or GroupInfo.\n\n\n\n\n\n\n\ncloud.asset.list(\n    namespace=None,\n    search=None,\n    type=None,\n    ownership_level=None,\n    depth=None,\n    expand=None,\n    page=None,\n    per_page=None,\n    order_by=None,\n)\nList/search for stored assets.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnamespace\nOptional[str]\nThe namespace to use, or the current user if absent.\nNone\n\n\nsearch\nOptional[str]\nA search string to use.\nNone\n\n\ntype\nOptional[_AssetType]\nIf provided, include only assets of the specified type (“array” or “group”).\nNone\n\n\nownership_level\nOptional[_OwnershipLevel]\nIf provided, include only assets you own (“owned”), or only assets that are shared with you (“shared”).\nNone\n\n\ndepth\nOptional[_Depth]\nThe depth to provide return information. If “root”, only root assets (i.e., arrays and groups that are not contained within another group) will be returned. If “all”, all assets that match (including those which are contained in another group) will be included.\nNone\n\n\nexpand\nOptional[_CSVString]\nComma-separated string specifying additional information to include in the response. As of this writing, “metadata” is supported.\nNone\n\n\npage\nOptional[int]\nWhich page of results to retrieve. 1-based.\nNone\n\n\nper_page\nOptional[int]\nHow many results to include on each page.\nNone\n\n\norder_by\nOptional[str]\nThe order to return assets, by default “created_at desc”. Supported keys are “created_at”, “name”, and “asset_type”. They can be used alone or with “asc” or “desc” separated by a space (e.g. “created_at”, “asset_type asc”).\nNone\n\n\n\n\n\n\n\ncloud.asset.list_public(\n    search=None,\n    type=None,\n    depth=None,\n    page=None,\n    per_page=None,\n    order_by=None,\n)\nList/search for publicly-shared assets.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsearch\nOptional[str]\nA search string to use.\nNone\n\n\ntype\nOptional[_AssetType]\nIf provided, include only assets of the specified type (“array” or “group”).\nNone\n\n\ndepth\nOptional[_Depth]\nThe depth to provide return information. If “root”, only root assets (i.e., arrays and groups that are not contained within another group) will be returned. If “all”, all assets that match (including those which are contained in another group) will be included.\nNone\n\n\nexpand\n\nComma-separated string specifying additional information to include in the response. As of this writing, “metadata” is supported.\nrequired\n\n\npage\nOptional[int]\nWhich page of results to retrieve. 1-based.\nNone\n\n\nper_page\nOptional[int]\nHow many results to include on each page.\nNone\n\n\norder_by\nOptional[str]\nThe order to return assets, by default “created_at desc”. Supported keys are “created_at”, “name”, and “asset_type”. They can be used alone or with “asc” or “desc” separated by a space (e.g. “created_at”, “asset_type asc”).\nNone\n\n\n\n\n\n\n\ncloud.asset.list_shared_with(uri)\nList an asset’s sharing policies.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\ntiledb URI of the asset.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\na list of ArraySharing or GroupSharing objects.\n\n\n\n\n\n\n\ncloud.asset.register(\n    storage_uri,\n    type,\n    *,\n    dest_uri=None,\n    name=None,\n    namespace=None,\n    credentials_name=None,\n    parent_uri=None,\n)\nRegister stored objects as an asset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstorage_uri\nstr\nS3, for example, URI of the data to be registered.\nrequired\n\n\ntype\n_AssetType\nThe type of asset, “array” or “group”.\nrequired\n\n\nnamespace\nOptional[str]\nThe user or organization to register the asset under. If unset will default to the logged-in user’s namespace.\nNone\n\n\nname\nOptional[str]\nName of asset.\nNone\n\n\ndescription\nstr\nOptional description.\nrequired\n\n\ncredentials_name\nOptional[str]\nOptional name of access credentials to use. If omitted, the default for namespace will be used.\nNone\n\n\nparent_uri\nOptional[str]\nOptional parent URI for group type assets.\nNone\n\n\n\n\n\n\n\ncloud.asset.share(uri, namespace, permissions='read')\nGive another namespace permission to access an asset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\ntiledb URI of the asset.\nrequired\n\n\nnamespace\nstr\nthe target namespace.\nrequired\n\n\npermissions\nOptional[Union[str, List[str]]]\n‘read’, ‘write’, or [‘read’, ‘write’].\n'read'\n\n\n\n\n\n\n\ncloud.asset.unshare(uri, namespace)\nRemove access permissions for another namespace.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\ntiledb URI of the asset.\nrequired\n\n\nnamespace\nstr\nthe target namespace.\nrequired\n\n\n\n\n\n\n\ncloud.asset.update_info(\n    uri,\n    *,\n    description=None,\n    name=None,\n    tags=None,\n    access_credentials_name=None,\n)\nUpdate asset info settings.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\ntiledb URI of the asset.\nrequired\n\n\ndescription\nOptional[str]\nAsset description, defaults to None\nNone\n\n\nname\nOptional[str]\nAsset name, defaults to None\nNone\n\n\ntags\nOptional[List[str]]\nAsset tags, defaults to None\nNone",
    "crumbs": [
      "Get Started",
      "Catalog",
      "asset"
    ]
  },
  {
    "objectID": "reference/config.html",
    "href": "reference/config.html",
    "title": "config",
    "section": "",
    "text": "cloud.config\n\n\n\n\n\nName\nDescription\n\n\n\n\nuser\nThe default user to use.",
    "crumbs": [
      "Get Started",
      "Account",
      "config"
    ]
  },
  {
    "objectID": "reference/config.html#attributes",
    "href": "reference/config.html#attributes",
    "title": "config",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nuser\nThe default user to use.",
    "crumbs": [
      "Get Started",
      "Account",
      "config"
    ]
  },
  {
    "objectID": "get_started.html",
    "href": "get_started.html",
    "title": "Get started with TileDB Cloud for Python",
    "section": "",
    "text": "This is a starting guide for the TileDB-Cloud-Py client."
  },
  {
    "objectID": "get_started.html#username-and-password-method",
    "href": "get_started.html#username-and-password-method",
    "title": "Get started with TileDB Cloud for Python",
    "section": "Username and Password method",
    "text": "Username and Password method\nimport tiledb.cloud\n\ntiledb.cloud.login(\n    host=&lt;tiledb.host&gt;,\n    username=&lt;username&gt;,\n    password=&lt;password&gt;\n)"
  },
  {
    "objectID": "get_started.html#token-method",
    "href": "get_started.html#token-method",
    "title": "Get started with TileDB Cloud for Python",
    "section": "Token method",
    "text": "Token method\nimport tiledb.cloud\n\ntiledb.cloud.login(\n    host=&lt;tiledb.host&gt;,\n    token=&lt;token&gt;\n)"
  },
  {
    "objectID": "reference/dag.visualization.html",
    "href": "reference/dag.visualization.html",
    "title": "dag.visualization",
    "section": "",
    "text": "cloud.dag.visualization\n\n\n\n\n\nName\nDescription\n\n\n\n\nbuild_graph_node_details\n\n\n\nbuild_visualization_positions\nBuilds the positional spacing of all nodes(markers) based on either pydot\n\n\nhierarchy_pos\nTaken from https://epidemicsonnetworks.readthedocs.io/en/latest/_modules/EoN/auxiliary.html#hierarchy_pos\n\n\nupdate_plotly_graph\nUpdate a graph based on based node status and figure\n\n\nupdate_tiledb_graph\nUpdate a tiledb plot widge graph\n\n\n\n\n\ncloud.dag.visualization.build_graph_node_details(nodes)\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnodes\n\nList of nodes to get status of\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\ntuple of node_colors and node_text\n\n\n\n\n\n\n\ncloud.dag.visualization.build_visualization_positions(network)\nBuilds the positional spacing of all nodes(markers) based on either pydot if available or falling back to a python computation\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnetwork\n\n\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nposition array\n\n\n\n\n\n\n\ncloud.dag.visualization.hierarchy_pos(\n    G,\n    root=None,\n    width=1.0,\n    vert_gap=0.2,\n    vert_loc=0,\n    leaf_vs_root_factor=0.5,\n)\nTaken from https://epidemicsonnetworks.readthedocs.io/en/latest/_modules/EoN/auxiliary.html#hierarchy_pos Licensed under MIT: https://epidemicsonnetworks.readthedocs.io/en/latest/_downloads/8e9c8138fef49ddba8102fa7799c29d7/license.txt\nIf the graph is a tree this will return the positions to plot this in a hierarchical layout.\nBased on Joel’s answer at https://stackoverflow.com/a/29597209/2966723, but with some modifications.\nWe include this because it may be useful for plotting transmission trees, and there is currently no networkx equivalent (though it may be coming soon).\nThere are two basic approaches we think of to allocate the horizontal location of a node.\n\nTop down: we allocate horizontal space to a node. Then its k descendants split up that horizontal space equally. This tends to result in overlapping nodes when some have many descendants.\nBottom up: we allocate horizontal space to each leaf node. A node at a higher level gets the entire space allocated to its descendant leaves. Based on this, leaf nodes at higher levels get the same space as leaf nodes very deep in the tree.\n\nWe use use both of these approaches simultaneously with leaf_vs_root_factor determining how much of the horizontal space is based on the bottom up or top down approaches. 0 gives pure bottom up, while 1 gives pure top down.\n:Arguments:\nG the graph (must be a tree)\nroot the root node of the tree - if the tree is directed and this is not given, the root will be found and used - if the tree is directed and this is given, then the positions will be just for the descendants of this node. - if the tree is undirected and not given, then a random choice will be used.\nwidth horizontal space allocated for this branch - avoids overlap with other branches\nvert_gap gap between levels of hierarchy\nvert_loc vertical location of root\nleaf_vs_root_factor\nxcenter: horizontal location of root\n\n\n\ncloud.dag.visualization.update_plotly_graph(nodes, fig=None)\nUpdate a graph based on based node status and figure\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnodes\n\nlist of notes to update\nrequired\n\n\nfig\n\n\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncloud.dag.visualization.update_tiledb_graph(\n    nodes,\n    edges,\n    node_details,\n    positions,\n    fig,\n)\nUpdate a tiledb plot widge graph\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnodes\n\nnodes of graph\nrequired\n\n\nedges\n\nedges for graph\nrequired\n\n\nnode_details\n\nNode details\nrequired\n\n\npositions\n\npositions for graph\nrequired\n\n\nfig\n\nfigure\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription",
    "crumbs": [
      "Get Started",
      "Scale",
      "dag.visualization"
    ]
  },
  {
    "objectID": "reference/dag.visualization.html#functions",
    "href": "reference/dag.visualization.html#functions",
    "title": "dag.visualization",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nbuild_graph_node_details\n\n\n\nbuild_visualization_positions\nBuilds the positional spacing of all nodes(markers) based on either pydot\n\n\nhierarchy_pos\nTaken from https://epidemicsonnetworks.readthedocs.io/en/latest/_modules/EoN/auxiliary.html#hierarchy_pos\n\n\nupdate_plotly_graph\nUpdate a graph based on based node status and figure\n\n\nupdate_tiledb_graph\nUpdate a tiledb plot widge graph\n\n\n\n\n\ncloud.dag.visualization.build_graph_node_details(nodes)\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnodes\n\nList of nodes to get status of\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\ntuple of node_colors and node_text\n\n\n\n\n\n\n\ncloud.dag.visualization.build_visualization_positions(network)\nBuilds the positional spacing of all nodes(markers) based on either pydot if available or falling back to a python computation\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnetwork\n\n\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nposition array\n\n\n\n\n\n\n\ncloud.dag.visualization.hierarchy_pos(\n    G,\n    root=None,\n    width=1.0,\n    vert_gap=0.2,\n    vert_loc=0,\n    leaf_vs_root_factor=0.5,\n)\nTaken from https://epidemicsonnetworks.readthedocs.io/en/latest/_modules/EoN/auxiliary.html#hierarchy_pos Licensed under MIT: https://epidemicsonnetworks.readthedocs.io/en/latest/_downloads/8e9c8138fef49ddba8102fa7799c29d7/license.txt\nIf the graph is a tree this will return the positions to plot this in a hierarchical layout.\nBased on Joel’s answer at https://stackoverflow.com/a/29597209/2966723, but with some modifications.\nWe include this because it may be useful for plotting transmission trees, and there is currently no networkx equivalent (though it may be coming soon).\nThere are two basic approaches we think of to allocate the horizontal location of a node.\n\nTop down: we allocate horizontal space to a node. Then its k descendants split up that horizontal space equally. This tends to result in overlapping nodes when some have many descendants.\nBottom up: we allocate horizontal space to each leaf node. A node at a higher level gets the entire space allocated to its descendant leaves. Based on this, leaf nodes at higher levels get the same space as leaf nodes very deep in the tree.\n\nWe use use both of these approaches simultaneously with leaf_vs_root_factor determining how much of the horizontal space is based on the bottom up or top down approaches. 0 gives pure bottom up, while 1 gives pure top down.\n:Arguments:\nG the graph (must be a tree)\nroot the root node of the tree - if the tree is directed and this is not given, the root will be found and used - if the tree is directed and this is given, then the positions will be just for the descendants of this node. - if the tree is undirected and not given, then a random choice will be used.\nwidth horizontal space allocated for this branch - avoids overlap with other branches\nvert_gap gap between levels of hierarchy\nvert_loc vertical location of root\nleaf_vs_root_factor\nxcenter: horizontal location of root\n\n\n\ncloud.dag.visualization.update_plotly_graph(nodes, fig=None)\nUpdate a graph based on based node status and figure\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnodes\n\nlist of notes to update\nrequired\n\n\nfig\n\n\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncloud.dag.visualization.update_tiledb_graph(\n    nodes,\n    edges,\n    node_details,\n    positions,\n    fig,\n)\nUpdate a tiledb plot widge graph\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnodes\n\nnodes of graph\nrequired\n\n\nedges\n\nedges for graph\nrequired\n\n\nnode_details\n\nNode details\nrequired\n\n\npositions\n\npositions for graph\nrequired\n\n\nfig\n\nfigure\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription",
    "crumbs": [
      "Get Started",
      "Scale",
      "dag.visualization"
    ]
  },
  {
    "objectID": "reference/taskgraphs.executor.html",
    "href": "reference/taskgraphs.executor.html",
    "title": "taskgraphs.executor",
    "section": "",
    "text": "cloud.taskgraphs.executor\nGeneric interfaces for task graph executors.\n\n\n\n\n\nName\nDescription\n\n\n\n\nGraphStructure\nThe structure of a task graph, as the JSON serialization or a Builder.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nExecutor\nAn interface allowing for execution and management of a task graph.\n\n\nNode\nAn abstract type specifying the operations on a Node of a task graph.\n\n\nParentFailedError\nRaised when the parent of a Node fails.\n\n\nStatus\nThe current status of a Node (or a graph).\n\n\n\n\n\ncloud.taskgraphs.executor.Executor(graph)\nAn interface allowing for execution and management of a task graph.\nThis is the basic interface fulfilled by any task graph executor. While some implementations may provide more control, these operations, to the extent that they are supported, are universal across task graph implementations.\n\n\n\n\n\nName\nDescription\n\n\n\n\nserver_graph_uuid\nThe UUID of this execution’s log as returned by the server.\n\n\nstatus\nThe status of the entire graph.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nadd_update_callback\nAdds a callback that will be called when a node’s state changes.\n\n\ncancel\nIf possible, cancels further execution of this graph.\n\n\ncancelled_nodes\nA snapshot of all nodes that have been cancelled.\n\n\nexecute\nStarts execution of this graph with the given input values.\n\n\nfailed_nodes\nA snapshot of all nodes that have failed.\n\n\nnode\nGets the node identified either by name, ID, or builder node.\n\n\nnodes_by_name\nA dictionary of all nodes, keyed by node name.\n\n\nretry\nAttempts to retry a node.\n\n\nretry_all\nRetries all retry-able nodes.\n\n\nrunning_nodes\nA snapshot of all nodes currently running.\n\n\nsuccessful_nodes\nA snapshot of all nodes that have completed successfully.\n\n\nunstarted_nodes\nA snapshot of all nodes that have not yet been started.\n\n\nvisualize\nReturns a visualization of this graph for a Jupyter notebook.\n\n\nwait\nWaits for the execution of this task graph to complete.\n\n\n\n\n\ncloud.taskgraphs.executor.Executor.add_update_callback(callback)\nAdds a callback that will be called when a node’s state changes.\nCallbacks are delivered on a best-effort basis and may be asynchronous or batched depending upon the implementation. There is no guarantee as to what thread the callback may come in on.\n\n\n\ncloud.taskgraphs.executor.Executor.cancel()\nIf possible, cancels further execution of this graph.\nLike futures.Future.cancel, this returns True if the graph could be cancelled, and False if not.\n\n\n\ncloud.taskgraphs.executor.Executor.cancelled_nodes()\nA snapshot of all nodes that have been cancelled.\nThis includes both nodes that were manually cancelled, and nodes that are not executed because their parents failed.\n\n\n\ncloud.taskgraphs.executor.Executor.execute(**inputs)\nStarts execution of this graph with the given input values.\n\n\n\ncloud.taskgraphs.executor.Executor.failed_nodes()\nA snapshot of all nodes that have failed.\nThe returned collection of nodes is not guaranteed to be fully consistent, since in the process of iterating over nodes, the status of some nodes may have changed.\n\n\n\ncloud.taskgraphs.executor.Executor.node(nid)\nGets the node identified either by name, ID, or builder node.\nWhen passed: - a str: The node with the given name. - a :class:uuid.UUID: The node with the given ID. - a :class:builder.Node: The execution node corresponding to the given node from the :class:builder.Builder.\n\n\n\ncloud.taskgraphs.executor.Executor.nodes_by_name()\nA dictionary of all nodes, keyed by node name.\n\n\n\ncloud.taskgraphs.executor.Executor.retry(node)\nAttempts to retry a node.\nReturns True if the node was retried, false if not.\n\n\n\ncloud.taskgraphs.executor.Executor.retry_all()\nRetries all retry-able nodes.\n\n\n\ncloud.taskgraphs.executor.Executor.running_nodes()\nA snapshot of all nodes currently running.\nThe returned collection of nodes is not guaranteed to be fully consistent, since in the process of iterating over nodes, the status of some nodes may have changed.\n\n\n\ncloud.taskgraphs.executor.Executor.successful_nodes()\nA snapshot of all nodes that have completed successfully.\nThe returned collection of nodes is not guaranteed to be fully consistent, since in the process of iterating over nodes, the status of some nodes may have changed.\n\n\n\ncloud.taskgraphs.executor.Executor.unstarted_nodes()\nA snapshot of all nodes that have not yet been started.\nThe returned collection of nodes is not guaranteed to be fully consistent, since in the process of iterating over nodes, the status of some nodes may have changed.\n\n\n\ncloud.taskgraphs.executor.Executor.visualize()\nReturns a visualization of this graph for a Jupyter notebook.\n\n\n\ncloud.taskgraphs.executor.Executor.wait(timeout=None)\nWaits for the execution of this task graph to complete.\n\n\n\n\n\ncloud.taskgraphs.executor.Node(uid, owner, name)\nAn abstract type specifying the operations on a Node of a task graph.\nExecutor implementations will return instances of implementations of these Nodes when executing a task graph. If a caller uses only the methods here when manipulating task graph nodes, the actions they take will work (to the extent that they are supported) no matter the specifics of the executor itself (client-side, server-side, etc.).\nThe external-facing API matches that of futures.Future, with some added niceties (like status), and without the internal methods that are only “meant for use in unit tests and Executor implementations”: set_running_or_notify_cancel, set_result, and set_exception.\nThe generic types on this (which are really only of concern to Executor implementors) represent the Executor type and the type the Node yields::\nNode[MyExecutor, int]\n# A Node subtype that is executed by a MyExecutor\n# and whose .result() is an int.\n\n\n\n\n\nName\nDescription\n\n\n\n\ndisplay_name\nThe name for this node that should show up in UIs.\n\n\nfallback_name\nA fallback name for this node if unnamed.\n\n\nid\nThe client-generated UUID of this node.\n\n\nname\nThe name of the node, if present.\n\n\nowner\nThe executor which this node belongs to.\n\n\nstatus\nThe current lifecycle state of this Node.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nadd_done_callback\nAdds a callback that will be called when this Node completes.\n\n\ncancel\nIf possible, cancels execution of this node.\n\n\ncancelled\nReturns True if the Node was cancelled.\n\n\ndone\nReturns True if this Node has completed or been cancelled.\n\n\nexception\nIf this node failed, the exception that was raised.\n\n\nresult\nThe value resulting from executing this node.\n\n\nretry\nAttempts to submit this node for retry, if possible.\n\n\nrunning\nReturns True if the Node is currently executing.\n\n\ntask_id\nThe task ID that was returned from the server, if applicable.\n\n\nwait\nWaits for the given amount of time for this Node to complete.\n\n\n\n\n\ncloud.taskgraphs.executor.Node.add_done_callback(fn)\nAdds a callback that will be called when this Node completes.\nWhile the current behavior is similar to the way add_done_callback on a regular Future works, we don’t guarantee that it will remain the same (e.g. will it be called immediately, on what thread). A callback may be called back multiple times if the Node completes multiple times.\n\n\n\ncloud.taskgraphs.executor.Node.cancel()\nIf possible, cancels execution of this node.\nReturns True if cancellation succeeded, False if we could not cancel.\n\n\n\ncloud.taskgraphs.executor.Node.cancelled()\nReturns True if the Node was cancelled.\n\n\n\ncloud.taskgraphs.executor.Node.done()\nReturns True if this Node has completed or been cancelled.\n\n\n\ncloud.taskgraphs.executor.Node.exception(timeout=None)\nIf this node failed, the exception that was raised.\nIf the Node succeeded, returns None. If the Node was cancelled, a futures.CancelledError will be raised rather than returned.\n\n\n\ncloud.taskgraphs.executor.Node.result(timeout=None)\nThe value resulting from executing this node.\nReturns the result if present, or raises an exception if execution raised an exception.\n\n\n\ncloud.taskgraphs.executor.Node.retry()\nAttempts to submit this node for retry, if possible.\n\n\n\ncloud.taskgraphs.executor.Node.running()\nReturns True if the Node is currently executing.\n\n\n\ncloud.taskgraphs.executor.Node.task_id(timeout=None)\nThe task ID that was returned from the server, if applicable.\nIf this was executed on the server side, this should return the UUID of the actual execution of this task. If it was purely client-side, or the server did not return a UUID, this should return None.\n\n\n\ncloud.taskgraphs.executor.Node.wait(timeout=None)\nWaits for the given amount of time for this Node to complete.\n\n\n\n\n\ncloud.taskgraphs.executor.ParentFailedError(cause, node)\nRaised when the parent of a Node fails.\n\n\n\ncloud.taskgraphs.executor.Status()\nThe current status of a Node (or a graph).\n\n\n\n\n\nName\nDescription\n\n\n\n\nCANCELLED\nThe Node was cancelled before it could complete.\n\n\nFAILED\nThe Node failed to complete.\n\n\nPARENT_FAILED\nOne of the Node’s parents failed, so the Node could not run.\n\n\nREADY\nAll the inputs of a Node have resolved and it can run.\n\n\nRUNNING\nThe Node is currently running.\n\n\nSUCCEEDED\nThe Node completed successfully.\n\n\nWAITING\nA Node is waiting for input values.",
    "crumbs": [
      "Get Started",
      "Scale",
      "taskgraphs.executor"
    ]
  },
  {
    "objectID": "reference/taskgraphs.executor.html#attributes",
    "href": "reference/taskgraphs.executor.html#attributes",
    "title": "taskgraphs.executor",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nGraphStructure\nThe structure of a task graph, as the JSON serialization or a Builder.",
    "crumbs": [
      "Get Started",
      "Scale",
      "taskgraphs.executor"
    ]
  },
  {
    "objectID": "reference/taskgraphs.executor.html#classes",
    "href": "reference/taskgraphs.executor.html#classes",
    "title": "taskgraphs.executor",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nExecutor\nAn interface allowing for execution and management of a task graph.\n\n\nNode\nAn abstract type specifying the operations on a Node of a task graph.\n\n\nParentFailedError\nRaised when the parent of a Node fails.\n\n\nStatus\nThe current status of a Node (or a graph).\n\n\n\n\n\ncloud.taskgraphs.executor.Executor(graph)\nAn interface allowing for execution and management of a task graph.\nThis is the basic interface fulfilled by any task graph executor. While some implementations may provide more control, these operations, to the extent that they are supported, are universal across task graph implementations.\n\n\n\n\n\nName\nDescription\n\n\n\n\nserver_graph_uuid\nThe UUID of this execution’s log as returned by the server.\n\n\nstatus\nThe status of the entire graph.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nadd_update_callback\nAdds a callback that will be called when a node’s state changes.\n\n\ncancel\nIf possible, cancels further execution of this graph.\n\n\ncancelled_nodes\nA snapshot of all nodes that have been cancelled.\n\n\nexecute\nStarts execution of this graph with the given input values.\n\n\nfailed_nodes\nA snapshot of all nodes that have failed.\n\n\nnode\nGets the node identified either by name, ID, or builder node.\n\n\nnodes_by_name\nA dictionary of all nodes, keyed by node name.\n\n\nretry\nAttempts to retry a node.\n\n\nretry_all\nRetries all retry-able nodes.\n\n\nrunning_nodes\nA snapshot of all nodes currently running.\n\n\nsuccessful_nodes\nA snapshot of all nodes that have completed successfully.\n\n\nunstarted_nodes\nA snapshot of all nodes that have not yet been started.\n\n\nvisualize\nReturns a visualization of this graph for a Jupyter notebook.\n\n\nwait\nWaits for the execution of this task graph to complete.\n\n\n\n\n\ncloud.taskgraphs.executor.Executor.add_update_callback(callback)\nAdds a callback that will be called when a node’s state changes.\nCallbacks are delivered on a best-effort basis and may be asynchronous or batched depending upon the implementation. There is no guarantee as to what thread the callback may come in on.\n\n\n\ncloud.taskgraphs.executor.Executor.cancel()\nIf possible, cancels further execution of this graph.\nLike futures.Future.cancel, this returns True if the graph could be cancelled, and False if not.\n\n\n\ncloud.taskgraphs.executor.Executor.cancelled_nodes()\nA snapshot of all nodes that have been cancelled.\nThis includes both nodes that were manually cancelled, and nodes that are not executed because their parents failed.\n\n\n\ncloud.taskgraphs.executor.Executor.execute(**inputs)\nStarts execution of this graph with the given input values.\n\n\n\ncloud.taskgraphs.executor.Executor.failed_nodes()\nA snapshot of all nodes that have failed.\nThe returned collection of nodes is not guaranteed to be fully consistent, since in the process of iterating over nodes, the status of some nodes may have changed.\n\n\n\ncloud.taskgraphs.executor.Executor.node(nid)\nGets the node identified either by name, ID, or builder node.\nWhen passed: - a str: The node with the given name. - a :class:uuid.UUID: The node with the given ID. - a :class:builder.Node: The execution node corresponding to the given node from the :class:builder.Builder.\n\n\n\ncloud.taskgraphs.executor.Executor.nodes_by_name()\nA dictionary of all nodes, keyed by node name.\n\n\n\ncloud.taskgraphs.executor.Executor.retry(node)\nAttempts to retry a node.\nReturns True if the node was retried, false if not.\n\n\n\ncloud.taskgraphs.executor.Executor.retry_all()\nRetries all retry-able nodes.\n\n\n\ncloud.taskgraphs.executor.Executor.running_nodes()\nA snapshot of all nodes currently running.\nThe returned collection of nodes is not guaranteed to be fully consistent, since in the process of iterating over nodes, the status of some nodes may have changed.\n\n\n\ncloud.taskgraphs.executor.Executor.successful_nodes()\nA snapshot of all nodes that have completed successfully.\nThe returned collection of nodes is not guaranteed to be fully consistent, since in the process of iterating over nodes, the status of some nodes may have changed.\n\n\n\ncloud.taskgraphs.executor.Executor.unstarted_nodes()\nA snapshot of all nodes that have not yet been started.\nThe returned collection of nodes is not guaranteed to be fully consistent, since in the process of iterating over nodes, the status of some nodes may have changed.\n\n\n\ncloud.taskgraphs.executor.Executor.visualize()\nReturns a visualization of this graph for a Jupyter notebook.\n\n\n\ncloud.taskgraphs.executor.Executor.wait(timeout=None)\nWaits for the execution of this task graph to complete.\n\n\n\n\n\ncloud.taskgraphs.executor.Node(uid, owner, name)\nAn abstract type specifying the operations on a Node of a task graph.\nExecutor implementations will return instances of implementations of these Nodes when executing a task graph. If a caller uses only the methods here when manipulating task graph nodes, the actions they take will work (to the extent that they are supported) no matter the specifics of the executor itself (client-side, server-side, etc.).\nThe external-facing API matches that of futures.Future, with some added niceties (like status), and without the internal methods that are only “meant for use in unit tests and Executor implementations”: set_running_or_notify_cancel, set_result, and set_exception.\nThe generic types on this (which are really only of concern to Executor implementors) represent the Executor type and the type the Node yields::\nNode[MyExecutor, int]\n# A Node subtype that is executed by a MyExecutor\n# and whose .result() is an int.\n\n\n\n\n\nName\nDescription\n\n\n\n\ndisplay_name\nThe name for this node that should show up in UIs.\n\n\nfallback_name\nA fallback name for this node if unnamed.\n\n\nid\nThe client-generated UUID of this node.\n\n\nname\nThe name of the node, if present.\n\n\nowner\nThe executor which this node belongs to.\n\n\nstatus\nThe current lifecycle state of this Node.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nadd_done_callback\nAdds a callback that will be called when this Node completes.\n\n\ncancel\nIf possible, cancels execution of this node.\n\n\ncancelled\nReturns True if the Node was cancelled.\n\n\ndone\nReturns True if this Node has completed or been cancelled.\n\n\nexception\nIf this node failed, the exception that was raised.\n\n\nresult\nThe value resulting from executing this node.\n\n\nretry\nAttempts to submit this node for retry, if possible.\n\n\nrunning\nReturns True if the Node is currently executing.\n\n\ntask_id\nThe task ID that was returned from the server, if applicable.\n\n\nwait\nWaits for the given amount of time for this Node to complete.\n\n\n\n\n\ncloud.taskgraphs.executor.Node.add_done_callback(fn)\nAdds a callback that will be called when this Node completes.\nWhile the current behavior is similar to the way add_done_callback on a regular Future works, we don’t guarantee that it will remain the same (e.g. will it be called immediately, on what thread). A callback may be called back multiple times if the Node completes multiple times.\n\n\n\ncloud.taskgraphs.executor.Node.cancel()\nIf possible, cancels execution of this node.\nReturns True if cancellation succeeded, False if we could not cancel.\n\n\n\ncloud.taskgraphs.executor.Node.cancelled()\nReturns True if the Node was cancelled.\n\n\n\ncloud.taskgraphs.executor.Node.done()\nReturns True if this Node has completed or been cancelled.\n\n\n\ncloud.taskgraphs.executor.Node.exception(timeout=None)\nIf this node failed, the exception that was raised.\nIf the Node succeeded, returns None. If the Node was cancelled, a futures.CancelledError will be raised rather than returned.\n\n\n\ncloud.taskgraphs.executor.Node.result(timeout=None)\nThe value resulting from executing this node.\nReturns the result if present, or raises an exception if execution raised an exception.\n\n\n\ncloud.taskgraphs.executor.Node.retry()\nAttempts to submit this node for retry, if possible.\n\n\n\ncloud.taskgraphs.executor.Node.running()\nReturns True if the Node is currently executing.\n\n\n\ncloud.taskgraphs.executor.Node.task_id(timeout=None)\nThe task ID that was returned from the server, if applicable.\nIf this was executed on the server side, this should return the UUID of the actual execution of this task. If it was purely client-side, or the server did not return a UUID, this should return None.\n\n\n\ncloud.taskgraphs.executor.Node.wait(timeout=None)\nWaits for the given amount of time for this Node to complete.\n\n\n\n\n\ncloud.taskgraphs.executor.ParentFailedError(cause, node)\nRaised when the parent of a Node fails.\n\n\n\ncloud.taskgraphs.executor.Status()\nThe current status of a Node (or a graph).\n\n\n\n\n\nName\nDescription\n\n\n\n\nCANCELLED\nThe Node was cancelled before it could complete.\n\n\nFAILED\nThe Node failed to complete.\n\n\nPARENT_FAILED\nOne of the Node’s parents failed, so the Node could not run.\n\n\nREADY\nAll the inputs of a Node have resolved and it can run.\n\n\nRUNNING\nThe Node is currently running.\n\n\nSUCCEEDED\nThe Node completed successfully.\n\n\nWAITING\nA Node is waiting for input values.",
    "crumbs": [
      "Get Started",
      "Scale",
      "taskgraphs.executor"
    ]
  },
  {
    "objectID": "reference/geospatial.ingestion.html",
    "href": "reference/geospatial.ingestion.html",
    "title": "geospatial.ingestion",
    "section": "",
    "text": "cloud.geospatial.ingestion\n\n\n\n\n\nName\nDescription\n\n\n\n\nbuild_file_list_udf\nBuild a list of sources\n\n\nbuild_inputs_udf\nGroups input URIs into batches.\n\n\nconsolidate_meta\nConsolidate arrays in the dataset.\n\n\ningest_datasets\nIngest samples into a dataset.\n\n\ningest_datasets_dag\nIngests geospatial point clouds, geometries and images into TileDB arrays\n\n\ningest_geometry_udf\nInternal udf that ingests server side batch of geometry files\n\n\ningest_point_cloud_udf\nInternal udf that ingests server side batch of point cloud files\n\n\ningest_raster_udf\nInternal udf that ingests server side batch of raster files\n\n\nload_geometry_metadata\nReturn geospatial metadata for a sequence of input geometry data files\n\n\nload_pointcloud_metadata\nReturn geospatial metadata for a sequence of input point cloud data files\n\n\nload_raster_metadata\nReturn geospatial metadata for a sequence of input raster data files\n\n\nread_uris\nRead a list of URIs from a URI.\n\n\nregister_dataset_udf\nRegister the dataset on TileDB Cloud.\n\n\nremove_dataset_type_from_array_meta\nRemoves dataset_type meta if the ingested result is an array.\n\n\n\n\n\ncloud.geospatial.ingestion.build_file_list_udf(\n    dataset_type,\n    config=None,\n    search_uri=None,\n    pattern=None,\n    ignore=None,\n    dataset_list_uri=None,\n    max_files=None,\n    verbose=False,\n    trace=False,\n    log_uri=None,\n)\nBuild a list of sources\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_type\nDatasetType\ndataset type, one of pointcloud, raster or geometry\nrequired\n\n\nconfig\nOptional[Mapping[str, object]]\nconfig dictionary, defaults to None\nNone\n\n\nsearch_uri\nOptional[str]\nURI to search for geospatial dataset files, defaults to None\nNone\n\n\npattern\nOptional[str]\nUnix shell style pattern to match when searching for files, defaults to None\nNone\n\n\nignore\nOptional[str]\nUnix shell style pattern to ignore when searching for files, defaults to None\nNone\n\n\ndataset_list_uri\nOptional[str]\nURI with a list of dataset URIs, defaults to None\nNone\n\n\nmax_files\nOptional[int]\nmaximum number of URIs to read/find, defaults to None (no limit)\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\ntrace\nbool\nbool, enabling log tracing, defaults to False\nFalse\n\n\nlog_uri\nOptional[str]\nlog array URI\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nSequence[str]\nA sequence of source files grouped into batches\n\n\n\n\n\n\n\ncloud.geospatial.ingestion.build_inputs_udf(\n    dataset_type,\n    sources,\n    config=None,\n    compression_filter=None,\n    tile_size=RASTER_TILE_SIZE,\n    pixels_per_fragment=PIXELS_PER_FRAGMENT,\n    chunk_size=POINT_CLOUD_CHUNK_SIZE,\n    nodata=None,\n    resampling='bilinear',\n    res=None,\n    verbose=False,\n    trace=False,\n    log_uri=None,\n)\nGroups input URIs into batches.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_type\nDatasetType\ndataset type, one of pointcloud, raster or geometry\nrequired\n\n\nsources\nSequence[str]\nURIs to process\nrequired\n\n\nconfig\nOptional[Mapping[str, object]]\nconfig dictionary, defaults to None\nNone\n\n\ncompression_filter\nOptional[dict]\nserialized tiledb filter, defaults to None\nNone\n\n\ntile_size\nint\nfor rasters this is the tile (block) size for the merged destination array, defaults to 1024\nRASTER_TILE_SIZE\n\n\npixels_per_fragment\nint\nThis is the number of pixels that will be written per fragment. Ideally aim to align as a factor of tile_size\nPIXELS_PER_FRAGMENT\n\n\nchunk_size\nint\nfor point cloud this is the PDAL chunk size, defaults to 1000000\nPOINT_CLOUD_CHUNK_SIZE\n\n\nnodata\nOptional[float]\nNODATA value for raster merging\nNone\n\n\nresampling\nOptional[str]\nstring, resampling method, one of None, bilinear, cubic, nearest and average\n'bilinear'\n\n\nres\nTuple[float, float]\nTuple[float, float], output resolution in x/y\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\ntrace\nbool\nbool, enabling log tracing, to False\nFalse\n\n\nlog_uri\nOptional[str]\nlog array URI\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict[str, object]\nA dict containing the kwargs needed for the next function call\n\n\n\n\n\n\n\ncloud.geospatial.ingestion.consolidate_meta(\n    dataset_uri,\n    *,\n    config=None,\n    id='consolidate',\n    verbose=False,\n    trace=False,\n    log_uri=None,\n)\nConsolidate arrays in the dataset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nconfig\nOptional[Mapping[str, object]]\nconfig dictionary, defaults to None\nNone\n\n\nid\nstr\nprofiler event id, defaults to “consolidate”\n'consolidate'\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\ncloud.geospatial.ingestion.ingest_datasets(\n    dataset_uri,\n    *,\n    dataset_type,\n    acn=None,\n    config=None,\n    namespace=None,\n    register_name=None,\n    search_uri=None,\n    pattern=None,\n    ignore=None,\n    dataset_list_uri=None,\n    max_files=None,\n    compression_filter=None,\n    workers=MAX_WORKERS,\n    batch_size=BATCH_SIZE,\n    tile_size=RASTER_TILE_SIZE,\n    pixels_per_fragment=PIXELS_PER_FRAGMENT,\n    chunk_size=POINT_CLOUD_CHUNK_SIZE,\n    nodata=None,\n    res=None,\n    stats=False,\n    verbose=False,\n    trace=False,\n    log_uri=None,\n)\nIngest samples into a dataset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\ndataset_type\nDatasetType\ndataset type, one of pointcloud, raster or geometry\nrequired\n\n\nacn\nOptional[str]\nAccess Credentials Name (ACN) registered in TileDB Cloud (ARN type), defaults to None\nNone\n\n\nconfig\n\nconfig dictionary, defaults to None\nNone\n\n\nnamespace\nOptional[str]\nTileDB-Cloud namespace, defaults to None\nNone\n\n\nregister_name\nOptional[str]\nname to register the dataset with on TileDB Cloud, defaults to None\nNone\n\n\nsearch_uri\nOptional[str]\nURI to search for geospatial dataset files, defaults to None\nNone\n\n\npattern\nOptional[str]\nUnix shell style pattern to match when searching for files, defaults to None\nNone\n\n\nignore\nOptional[str]\nUnix shell style pattern to ignore when searching for files, defaults to None\nNone\n\n\ndataset_list_uri\nOptional[str]\nURI with a list of dataset URIs, defaults to None\nNone\n\n\nmax_files\nOptional[int]\nmaximum number of URIs to read/find, defaults to None (no limit)\nNone\n\n\ncompression_filter\nOptional[dict]\nserialized tiledb filter, defaults to None\nNone\n\n\nworkers\nint\nnumber of workers for dataset ingestion, defaults to MAX_WORKERS\nMAX_WORKERS\n\n\nbatch_size\nint\nbatch size for dataset ingestion, defaults to BATCH_SIZE\nBATCH_SIZE\n\n\ntile_size\nint\nfor rasters this is the tile (block) size for the merged destination array defaults to 1024\nRASTER_TILE_SIZE\n\n\npixels_per_fragment\nint\nThis is the number of pixels that will be written per fragment. Ideally aim to align as a factor of tile_size\nPIXELS_PER_FRAGMENT\n\n\nchunk_size\nint\nfor point cloud this is the PDAL chunk size, defaults to 1000000\nPOINT_CLOUD_CHUNK_SIZE\n\n\nnodata\nOptional[float]\nNODATA value for rasters\nNone\n\n\nres\nTuple[float, float]\nTuple[float, float], output resolution in x/y\nNone\n\n\nstats\nbool\nbool, print TileDB stats to stdout\nFalse\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\ntrace\nbool\nbool, enable trace for logging, defaults to False\nFalse\n\n\nlog_uri\nOptional[str]\nlog array URI\nNone\n\n\n\n\n\n\n\ncloud.geospatial.ingestion.ingest_datasets_dag(\n    dataset_uri,\n    *,\n    dataset_type,\n    acn=None,\n    config=None,\n    namespace=None,\n    register_name=None,\n    search_uri=None,\n    pattern=None,\n    ignore=None,\n    dataset_list_uri=None,\n    max_files=None,\n    compression_filter=None,\n    workers=MAX_WORKERS,\n    batch_size=BATCH_SIZE,\n    tile_size=RASTER_TILE_SIZE,\n    pixels_per_fragment=PIXELS_PER_FRAGMENT,\n    chunk_size=POINT_CLOUD_CHUNK_SIZE,\n    nodata=None,\n    resampling='bilinear',\n    res=None,\n    stats=False,\n    verbose=False,\n    trace=False,\n    log_uri=None,\n)\nIngests geospatial point clouds, geometries and images into TileDB arrays\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\ndataset_type\nDatasetType\ndataset type, one of pointcloud, raster or geometry\nrequired\n\n\nacn\nOptional[str]\nAccess Credentials Name (ACN) registered in TileDB Cloud (ARN type), defaults to None\nNone\n\n\nconfig\nOptional[Mapping[str, object]]\nconfig dictionary, defaults to None\nNone\n\n\nnamespace\nOptional[str]\nTileDB-Cloud namespace, defaults to None\nNone\n\n\nregister_name\nOptional[str]\nname to register the dataset with on TileDB Cloud, defaults to None and the destination array is not registered\nNone\n\n\nsearch_uri\nOptional[str]\nURI to search for geospatial dataset files, defaults to None\nNone\n\n\npattern\nOptional[str]\nUnix shell style pattern to match when searching for files, defaults to None\nNone\n\n\nignore\nOptional[str]\nUnix shell style pattern to ignore when searching for files, defaults to None\nNone\n\n\ndataset_list_uri\nOptional[str]\nURI with a list of dataset URIs, defaults to None\nNone\n\n\nmax_files\nOptional[int]\nmaximum number of URIs to read/find, defaults to None (no limit)\nNone\n\n\ncompression_filter\nOptional[dict]\nserialized tiledb filter, defaults to None\nNone\n\n\nworkers\nint\nnumber of workers for dataset ingestion, defaults to MAX_WORKERS\nMAX_WORKERS\n\n\nbatch_size\nint\nbatch size for dataset ingestion, defaults to BATCH_SIZE\nBATCH_SIZE\n\n\ntile_size\nint\nfor rasters this is the tile (block) size for the merged destination array, defaults to 1024\nRASTER_TILE_SIZE\n\n\npixels_per_fragment\nint\nThis is the number of pixels that will be written per fragment. Ideally aim to align as a factor of tile_size\nPIXELS_PER_FRAGMENT\n\n\nchunk_size\nint\nfor point cloud this is the PDAL chunk size, defaults to 1000000\nPOINT_CLOUD_CHUNK_SIZE\n\n\nnodata\nOptional[float]\nNODATA value for raster merging\nNone\n\n\nresampling\nOptional[str]\nstring, resampling method, one of None, bilinear, cubic, nearest and average\n'bilinear'\n\n\nres\nTuple[float, float]\nTuple[float, float], output resolution in x/y\nNone\n\n\nstats\nbool\nbool, print TileDB stats to stdout\nFalse\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\ntrace\nbool\nbool, enabling log tracing, defaults to False\nFalse\n\n\nlog_uri\nOptional[str]\nlog array URI\nNone\n\n\n\n\n\n\n\ncloud.geospatial.ingestion.ingest_geometry_udf(\n    dataset_uri,\n    args={},\n    sources=None,\n    schema=None,\n    extents=None,\n    crs=None,\n    chunk_size=GEOMETRY_CHUNK_SIZE,\n    batch_size=BATCH_SIZE,\n    compressor=None,\n    append=False,\n    verbose=False,\n    stats=False,\n    config=None,\n    id='geometry',\n    trace=False,\n    log_uri=None,\n)\nInternal udf that ingests server side batch of geometry files into tiledb arrays using Fiona API\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\nstr, output TileDB array name\nrequired\n\n\nargs\nUnion[Dict, List]\ndict, input key value arguments as a dictionary\n{}\n\n\nsources\nSequence[str]\nSequence of input geometry file names\nNone\n\n\nschema\ndict\ndict, dictionary of schema attributes and geometries\nNone\n\n\nextents\nOptional[XYBoundsTuple]\nExtents of the destination geometry array\nNone\n\n\ncrs\nOptional[str]\nstr, CRS for the destination dataset\nNone\n\n\nchunk_size\nOptional[int]\nint, sets tile capacity and the number of geometries written at once\nGEOMETRY_CHUNK_SIZE\n\n\nbatch_size\nOptional[int]\nbatch size for dataset ingestion, defaults to BATCH_SIZE\nBATCH_SIZE\n\n\ncompressor\nOptional[dict]\ndict, serialized compression filter\nNone\n\n\nappend\nbool\nbool, whether to append to the array\nFalse\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\nstats\nbool\nbool, print TileDB stats to stdout\nFalse\n\n\nconfig\nOptional[Mapping[str, object]]\ndict, configuration to pass on tiledb.VFS\nNone\n\n\nid\nstr\nstr, ID for logging\n'geometry'\n\n\nlog_uri\nOptional[str]\nlog array URI\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nUnion[Sequence[os.PathLike], None]\nif not appending then the function returns a tuple of file paths\n\n\n\n\n\n\n\ncloud.geospatial.ingestion.ingest_point_cloud_udf(\n    args={},\n    dataset_uri,\n    sources=None,\n    append=False,\n    chunk_size=POINT_CLOUD_CHUNK_SIZE,\n    batch_size=BATCH_SIZE,\n    verbose=False,\n    stats=False,\n    config=None,\n    id='pointcloud',\n    trace=False,\n    log_uri=None,\n)\nInternal udf that ingests server side batch of point cloud files into tiledb arrays using PDAL API. Compression uses the default profile built in to PDAL.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nUnion[Dict, List]\ndict or list, input key value arguments as a dictionary\n{}\n\n\ndataset_uri\nstr\nstr, output TileDB array name\nrequired\n\n\nsources\nSequence[GeoMetadata]\nSequence of GeoMetadata objects\nNone\n\n\nappend\nbool\nbool, whether to append to the array\nFalse\n\n\nchunk_size\nOptional[int]\nPDAL configuration for chunking fragments\nPOINT_CLOUD_CHUNK_SIZE\n\n\nbatch_size\nOptional[int]\nbatch size for dataset ingestion, defaults to BATCH_SIZE\nBATCH_SIZE\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\nstats\nbool\nbool, print TileDB stats to stdout\nFalse\n\n\nconfig\nOptional[Mapping[str, object]]\ndict, configuration to pass on tiledb.VFS\nNone\n\n\nid\nstr\nstr, ID for logging\n'pointcloud'\n\n\nlog_uri\nOptional[str]\nlog array URI\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nUnion[Sequence[os.PathLike], None]\nif not appending then a sequence of file paths\n\n\n\n\n\n\n\ncloud.geospatial.ingestion.ingest_raster_udf(\n    args={},\n    dataset_uri,\n    sources=None,\n    extents=None,\n    band_count=None,\n    dtype=None,\n    nodata=None,\n    pixels_per_fragment=PIXELS_PER_FRAGMENT,\n    tile_size=RASTER_TILE_SIZE,\n    resampling=DEFAULT_RASTER_SAMPLING,\n    append=False,\n    batch_size=BATCH_SIZE,\n    stats=False,\n    verbose=False,\n    config=None,\n    compressor=None,\n    id='raster',\n    trace=False,\n    log_uri=None,\n)\nInternal udf that ingests server side batch of raster files into tiledb arrays using Rasterio API\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nUnion[Dict, List]\ndict, input key value arguments as a dictionary\n{}\n\n\ndataset_uri\nstr\nstr, output TileDB array name\nrequired\n\n\nsources\nTuple[GeoBlockMetadata]\ntuple, sequence of GeoBlockMetadata objects containing the destination raster window and the input files that contribute to this window\nNone\n\n\nextents\nOptional[BoundingBox]\nExtents of the destination raster\nNone\n\n\nband_count\nOptional[int]\nint, number of bands in destination array\nNone\n\n\ndtype\nOptional[str]\nstr, dtype of destination array\nNone\n\n\nnodata\nOptional[float]\nfloat, NODATA value for destination raster\nNone\n\n\ntile_size\nint\nfor rasters this is the tile (block) size for the merged destination array, defaults to 1024\nRASTER_TILE_SIZE\n\n\npixels_per_fragment\nint\nThis is the number of pixels that will be written per fragment. Ideally aim to align as a factor of tile_size\nPIXELS_PER_FRAGMENT\n\n\nresampling\nstr\nstring, resampling method, one of None, bilinear, cubic, nearest and average\nDEFAULT_RASTER_SAMPLING\n\n\nappend\nbool\nbool, whether to append to the array\nFalse\n\n\nbatch_size\nint\nbatch size for dataset ingestion, defaults to BATCH_SIZE\nBATCH_SIZE\n\n\nstats\nbool\nbool, print TileDB stats to stdout\nFalse\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\nconfig\nOptional[Mapping[str, object]]\ndict, configuration to pass on tiledb.VFS\nNone\n\n\ncompressor\nOptional[dict]\ndict, serialized compression filter\nNone\n\n\nid\nstr\nstr, ID for logging\n'raster'\n\n\nlog_uri\nOptional[str]\nlog array URI\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nUnion[Sequence[GeoBlockMetadata], None]\nif not appending then a sequence of populated GeoBlockMetadata objects\n\n\n\n\n\n\n\ncloud.geospatial.ingestion.load_geometry_metadata(\n    sources,\n    *,\n    config=None,\n    verbose=False,\n    id='pointcloud_metadata',\n    trace=False,\n    log_uri=None,\n)\nReturn geospatial metadata for a sequence of input geometry data files\n:Return: list[GeoMetadata], a list of populated GeoMetadata objects\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsources\nIterable[os.PathLike]\nA sequence of paths or path to input\nrequired\n\n\nconfig\nOptional[Mapping[str, object]]\ndict configuration to pass on tiledb.VFS\nNone\n\n\nverbose\nbool\nbool, enable verbose logging, default is False\nFalse\n\n\ntrace\nbool\nbool, enable trace logging, default is False\nFalse\n\n\nlog_uri\nOptional[str]\nOptional[str] = None,\nNone\n\n\n\n\n\n\n\ncloud.geospatial.ingestion.load_pointcloud_metadata(\n    sources,\n    *,\n    config=None,\n    verbose=False,\n    id='pointcloud_metadata',\n    trace=False,\n    log_uri=None,\n)\nReturn geospatial metadata for a sequence of input point cloud data files\n:Return: list[GeoMetadata], a list of populated GeoMetadata objects\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsources\nIterable[os.PathLike]\niterator, paths or path to process\nrequired\n\n\nconfig\nOptional[Mapping[str, object]]\ndict, configuration to pass on tiledb.VFS\nNone\n\n\nverbose\nbool\nbool, enable verbose logging, default is False\nFalse\n\n\ntrace\nbool\nbool, enable trace logging, default is False\nFalse\n\n\nlog_uri\nOptional[str]\nOptional[str] = None,\nNone\n\n\n\n\n\n\n\ncloud.geospatial.ingestion.load_raster_metadata(\n    sources,\n    *,\n    config=None,\n    verbose=False,\n    id='raster_metadata',\n    trace=False,\n    log_uri=None,\n)\nReturn geospatial metadata for a sequence of input raster data files\n:Return: list[GeoMetadata]: list of populated GeoMetadata objects\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsources\nIterable[os.PathLike]\niterator, paths or path to process\nrequired\n\n\nconfig\nOptional[Mapping[str, object]]\ndict, configuration to pass on tiledb.VFS\nNone\n\n\nverbose\nbool\nbool, enable verbose logging, default is False\nFalse\n\n\ntrace\nbool\nbool, enable trace logging, default is False\nFalse\n\n\nid\nstr\nstr, ID for logging\n'raster_metadata'\n\n\nlog_uri\nOptional[str]\nOptional[str] = None,\nNone\n\n\n\n\n\n\n\ncloud.geospatial.ingestion.read_uris(\n    list_uri,\n    dataset_type,\n    *,\n    log_uri=None,\n    config=None,\n    max_files=None,\n    verbose=False,\n)\nRead a list of URIs from a URI.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlist_uri\nstr\nURI of the list of URIs\nrequired\n\n\ndataset_type\nDatasetType\ndataset type, one of pointcloud, raster or geometry\nrequired\n\n\nlog_uri\nOptional[str]\nlog array URI\nNone\n\n\nconfig\nOptional[Mapping[str, object]]\nconfig dictionary, defaults to None\nNone\n\n\nmax_files\nOptional[int]\nmaximum number of URIs returned, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nSequence[str]\nlist of URIs\n\n\n\n\n\n\n\ncloud.geospatial.ingestion.register_dataset_udf(\n    dataset_uri,\n    *,\n    register_name,\n    namespace=None,\n    acn=None,\n    config=None,\n    verbose=False,\n)\nRegister the dataset on TileDB Cloud.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nregister_name\nstr\nname to register the dataset with on TileDB Cloud\nrequired\n\n\nnamespace\nOptional[str]\nTileDB Cloud namespace, defaults to the user’s default namespace\nNone\n\n\nacn\nOptional[str]\nAccess Credentials Name (ACN) registered in TileDB Cloud (ARN type), defaults to None\nNone\n\n\nconfig\nOptional[Mapping[str, object]]\nconfig dictionary, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\ncloud.geospatial.ingestion.remove_dataset_type_from_array_meta(\n    dataset_uri,\n    *,\n    verbose=False,\n)\nRemoves dataset_type meta if the ingested result is an array. FIXME: This exists to fix an internal UI issue until formally fixed. FIXME: Related ticket -&gt; sc-48098\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse",
    "crumbs": [
      "Get Started",
      "Analyze",
      "geospatial.ingestion"
    ]
  },
  {
    "objectID": "reference/geospatial.ingestion.html#functions",
    "href": "reference/geospatial.ingestion.html#functions",
    "title": "geospatial.ingestion",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nbuild_file_list_udf\nBuild a list of sources\n\n\nbuild_inputs_udf\nGroups input URIs into batches.\n\n\nconsolidate_meta\nConsolidate arrays in the dataset.\n\n\ningest_datasets\nIngest samples into a dataset.\n\n\ningest_datasets_dag\nIngests geospatial point clouds, geometries and images into TileDB arrays\n\n\ningest_geometry_udf\nInternal udf that ingests server side batch of geometry files\n\n\ningest_point_cloud_udf\nInternal udf that ingests server side batch of point cloud files\n\n\ningest_raster_udf\nInternal udf that ingests server side batch of raster files\n\n\nload_geometry_metadata\nReturn geospatial metadata for a sequence of input geometry data files\n\n\nload_pointcloud_metadata\nReturn geospatial metadata for a sequence of input point cloud data files\n\n\nload_raster_metadata\nReturn geospatial metadata for a sequence of input raster data files\n\n\nread_uris\nRead a list of URIs from a URI.\n\n\nregister_dataset_udf\nRegister the dataset on TileDB Cloud.\n\n\nremove_dataset_type_from_array_meta\nRemoves dataset_type meta if the ingested result is an array.\n\n\n\n\n\ncloud.geospatial.ingestion.build_file_list_udf(\n    dataset_type,\n    config=None,\n    search_uri=None,\n    pattern=None,\n    ignore=None,\n    dataset_list_uri=None,\n    max_files=None,\n    verbose=False,\n    trace=False,\n    log_uri=None,\n)\nBuild a list of sources\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_type\nDatasetType\ndataset type, one of pointcloud, raster or geometry\nrequired\n\n\nconfig\nOptional[Mapping[str, object]]\nconfig dictionary, defaults to None\nNone\n\n\nsearch_uri\nOptional[str]\nURI to search for geospatial dataset files, defaults to None\nNone\n\n\npattern\nOptional[str]\nUnix shell style pattern to match when searching for files, defaults to None\nNone\n\n\nignore\nOptional[str]\nUnix shell style pattern to ignore when searching for files, defaults to None\nNone\n\n\ndataset_list_uri\nOptional[str]\nURI with a list of dataset URIs, defaults to None\nNone\n\n\nmax_files\nOptional[int]\nmaximum number of URIs to read/find, defaults to None (no limit)\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\ntrace\nbool\nbool, enabling log tracing, defaults to False\nFalse\n\n\nlog_uri\nOptional[str]\nlog array URI\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nSequence[str]\nA sequence of source files grouped into batches\n\n\n\n\n\n\n\ncloud.geospatial.ingestion.build_inputs_udf(\n    dataset_type,\n    sources,\n    config=None,\n    compression_filter=None,\n    tile_size=RASTER_TILE_SIZE,\n    pixels_per_fragment=PIXELS_PER_FRAGMENT,\n    chunk_size=POINT_CLOUD_CHUNK_SIZE,\n    nodata=None,\n    resampling='bilinear',\n    res=None,\n    verbose=False,\n    trace=False,\n    log_uri=None,\n)\nGroups input URIs into batches.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_type\nDatasetType\ndataset type, one of pointcloud, raster or geometry\nrequired\n\n\nsources\nSequence[str]\nURIs to process\nrequired\n\n\nconfig\nOptional[Mapping[str, object]]\nconfig dictionary, defaults to None\nNone\n\n\ncompression_filter\nOptional[dict]\nserialized tiledb filter, defaults to None\nNone\n\n\ntile_size\nint\nfor rasters this is the tile (block) size for the merged destination array, defaults to 1024\nRASTER_TILE_SIZE\n\n\npixels_per_fragment\nint\nThis is the number of pixels that will be written per fragment. Ideally aim to align as a factor of tile_size\nPIXELS_PER_FRAGMENT\n\n\nchunk_size\nint\nfor point cloud this is the PDAL chunk size, defaults to 1000000\nPOINT_CLOUD_CHUNK_SIZE\n\n\nnodata\nOptional[float]\nNODATA value for raster merging\nNone\n\n\nresampling\nOptional[str]\nstring, resampling method, one of None, bilinear, cubic, nearest and average\n'bilinear'\n\n\nres\nTuple[float, float]\nTuple[float, float], output resolution in x/y\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\ntrace\nbool\nbool, enabling log tracing, to False\nFalse\n\n\nlog_uri\nOptional[str]\nlog array URI\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict[str, object]\nA dict containing the kwargs needed for the next function call\n\n\n\n\n\n\n\ncloud.geospatial.ingestion.consolidate_meta(\n    dataset_uri,\n    *,\n    config=None,\n    id='consolidate',\n    verbose=False,\n    trace=False,\n    log_uri=None,\n)\nConsolidate arrays in the dataset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nconfig\nOptional[Mapping[str, object]]\nconfig dictionary, defaults to None\nNone\n\n\nid\nstr\nprofiler event id, defaults to “consolidate”\n'consolidate'\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\ncloud.geospatial.ingestion.ingest_datasets(\n    dataset_uri,\n    *,\n    dataset_type,\n    acn=None,\n    config=None,\n    namespace=None,\n    register_name=None,\n    search_uri=None,\n    pattern=None,\n    ignore=None,\n    dataset_list_uri=None,\n    max_files=None,\n    compression_filter=None,\n    workers=MAX_WORKERS,\n    batch_size=BATCH_SIZE,\n    tile_size=RASTER_TILE_SIZE,\n    pixels_per_fragment=PIXELS_PER_FRAGMENT,\n    chunk_size=POINT_CLOUD_CHUNK_SIZE,\n    nodata=None,\n    res=None,\n    stats=False,\n    verbose=False,\n    trace=False,\n    log_uri=None,\n)\nIngest samples into a dataset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\ndataset_type\nDatasetType\ndataset type, one of pointcloud, raster or geometry\nrequired\n\n\nacn\nOptional[str]\nAccess Credentials Name (ACN) registered in TileDB Cloud (ARN type), defaults to None\nNone\n\n\nconfig\n\nconfig dictionary, defaults to None\nNone\n\n\nnamespace\nOptional[str]\nTileDB-Cloud namespace, defaults to None\nNone\n\n\nregister_name\nOptional[str]\nname to register the dataset with on TileDB Cloud, defaults to None\nNone\n\n\nsearch_uri\nOptional[str]\nURI to search for geospatial dataset files, defaults to None\nNone\n\n\npattern\nOptional[str]\nUnix shell style pattern to match when searching for files, defaults to None\nNone\n\n\nignore\nOptional[str]\nUnix shell style pattern to ignore when searching for files, defaults to None\nNone\n\n\ndataset_list_uri\nOptional[str]\nURI with a list of dataset URIs, defaults to None\nNone\n\n\nmax_files\nOptional[int]\nmaximum number of URIs to read/find, defaults to None (no limit)\nNone\n\n\ncompression_filter\nOptional[dict]\nserialized tiledb filter, defaults to None\nNone\n\n\nworkers\nint\nnumber of workers for dataset ingestion, defaults to MAX_WORKERS\nMAX_WORKERS\n\n\nbatch_size\nint\nbatch size for dataset ingestion, defaults to BATCH_SIZE\nBATCH_SIZE\n\n\ntile_size\nint\nfor rasters this is the tile (block) size for the merged destination array defaults to 1024\nRASTER_TILE_SIZE\n\n\npixels_per_fragment\nint\nThis is the number of pixels that will be written per fragment. Ideally aim to align as a factor of tile_size\nPIXELS_PER_FRAGMENT\n\n\nchunk_size\nint\nfor point cloud this is the PDAL chunk size, defaults to 1000000\nPOINT_CLOUD_CHUNK_SIZE\n\n\nnodata\nOptional[float]\nNODATA value for rasters\nNone\n\n\nres\nTuple[float, float]\nTuple[float, float], output resolution in x/y\nNone\n\n\nstats\nbool\nbool, print TileDB stats to stdout\nFalse\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\ntrace\nbool\nbool, enable trace for logging, defaults to False\nFalse\n\n\nlog_uri\nOptional[str]\nlog array URI\nNone\n\n\n\n\n\n\n\ncloud.geospatial.ingestion.ingest_datasets_dag(\n    dataset_uri,\n    *,\n    dataset_type,\n    acn=None,\n    config=None,\n    namespace=None,\n    register_name=None,\n    search_uri=None,\n    pattern=None,\n    ignore=None,\n    dataset_list_uri=None,\n    max_files=None,\n    compression_filter=None,\n    workers=MAX_WORKERS,\n    batch_size=BATCH_SIZE,\n    tile_size=RASTER_TILE_SIZE,\n    pixels_per_fragment=PIXELS_PER_FRAGMENT,\n    chunk_size=POINT_CLOUD_CHUNK_SIZE,\n    nodata=None,\n    resampling='bilinear',\n    res=None,\n    stats=False,\n    verbose=False,\n    trace=False,\n    log_uri=None,\n)\nIngests geospatial point clouds, geometries and images into TileDB arrays\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\ndataset_type\nDatasetType\ndataset type, one of pointcloud, raster or geometry\nrequired\n\n\nacn\nOptional[str]\nAccess Credentials Name (ACN) registered in TileDB Cloud (ARN type), defaults to None\nNone\n\n\nconfig\nOptional[Mapping[str, object]]\nconfig dictionary, defaults to None\nNone\n\n\nnamespace\nOptional[str]\nTileDB-Cloud namespace, defaults to None\nNone\n\n\nregister_name\nOptional[str]\nname to register the dataset with on TileDB Cloud, defaults to None and the destination array is not registered\nNone\n\n\nsearch_uri\nOptional[str]\nURI to search for geospatial dataset files, defaults to None\nNone\n\n\npattern\nOptional[str]\nUnix shell style pattern to match when searching for files, defaults to None\nNone\n\n\nignore\nOptional[str]\nUnix shell style pattern to ignore when searching for files, defaults to None\nNone\n\n\ndataset_list_uri\nOptional[str]\nURI with a list of dataset URIs, defaults to None\nNone\n\n\nmax_files\nOptional[int]\nmaximum number of URIs to read/find, defaults to None (no limit)\nNone\n\n\ncompression_filter\nOptional[dict]\nserialized tiledb filter, defaults to None\nNone\n\n\nworkers\nint\nnumber of workers for dataset ingestion, defaults to MAX_WORKERS\nMAX_WORKERS\n\n\nbatch_size\nint\nbatch size for dataset ingestion, defaults to BATCH_SIZE\nBATCH_SIZE\n\n\ntile_size\nint\nfor rasters this is the tile (block) size for the merged destination array, defaults to 1024\nRASTER_TILE_SIZE\n\n\npixels_per_fragment\nint\nThis is the number of pixels that will be written per fragment. Ideally aim to align as a factor of tile_size\nPIXELS_PER_FRAGMENT\n\n\nchunk_size\nint\nfor point cloud this is the PDAL chunk size, defaults to 1000000\nPOINT_CLOUD_CHUNK_SIZE\n\n\nnodata\nOptional[float]\nNODATA value for raster merging\nNone\n\n\nresampling\nOptional[str]\nstring, resampling method, one of None, bilinear, cubic, nearest and average\n'bilinear'\n\n\nres\nTuple[float, float]\nTuple[float, float], output resolution in x/y\nNone\n\n\nstats\nbool\nbool, print TileDB stats to stdout\nFalse\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\ntrace\nbool\nbool, enabling log tracing, defaults to False\nFalse\n\n\nlog_uri\nOptional[str]\nlog array URI\nNone\n\n\n\n\n\n\n\ncloud.geospatial.ingestion.ingest_geometry_udf(\n    dataset_uri,\n    args={},\n    sources=None,\n    schema=None,\n    extents=None,\n    crs=None,\n    chunk_size=GEOMETRY_CHUNK_SIZE,\n    batch_size=BATCH_SIZE,\n    compressor=None,\n    append=False,\n    verbose=False,\n    stats=False,\n    config=None,\n    id='geometry',\n    trace=False,\n    log_uri=None,\n)\nInternal udf that ingests server side batch of geometry files into tiledb arrays using Fiona API\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\nstr, output TileDB array name\nrequired\n\n\nargs\nUnion[Dict, List]\ndict, input key value arguments as a dictionary\n{}\n\n\nsources\nSequence[str]\nSequence of input geometry file names\nNone\n\n\nschema\ndict\ndict, dictionary of schema attributes and geometries\nNone\n\n\nextents\nOptional[XYBoundsTuple]\nExtents of the destination geometry array\nNone\n\n\ncrs\nOptional[str]\nstr, CRS for the destination dataset\nNone\n\n\nchunk_size\nOptional[int]\nint, sets tile capacity and the number of geometries written at once\nGEOMETRY_CHUNK_SIZE\n\n\nbatch_size\nOptional[int]\nbatch size for dataset ingestion, defaults to BATCH_SIZE\nBATCH_SIZE\n\n\ncompressor\nOptional[dict]\ndict, serialized compression filter\nNone\n\n\nappend\nbool\nbool, whether to append to the array\nFalse\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\nstats\nbool\nbool, print TileDB stats to stdout\nFalse\n\n\nconfig\nOptional[Mapping[str, object]]\ndict, configuration to pass on tiledb.VFS\nNone\n\n\nid\nstr\nstr, ID for logging\n'geometry'\n\n\nlog_uri\nOptional[str]\nlog array URI\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nUnion[Sequence[os.PathLike], None]\nif not appending then the function returns a tuple of file paths\n\n\n\n\n\n\n\ncloud.geospatial.ingestion.ingest_point_cloud_udf(\n    args={},\n    dataset_uri,\n    sources=None,\n    append=False,\n    chunk_size=POINT_CLOUD_CHUNK_SIZE,\n    batch_size=BATCH_SIZE,\n    verbose=False,\n    stats=False,\n    config=None,\n    id='pointcloud',\n    trace=False,\n    log_uri=None,\n)\nInternal udf that ingests server side batch of point cloud files into tiledb arrays using PDAL API. Compression uses the default profile built in to PDAL.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nUnion[Dict, List]\ndict or list, input key value arguments as a dictionary\n{}\n\n\ndataset_uri\nstr\nstr, output TileDB array name\nrequired\n\n\nsources\nSequence[GeoMetadata]\nSequence of GeoMetadata objects\nNone\n\n\nappend\nbool\nbool, whether to append to the array\nFalse\n\n\nchunk_size\nOptional[int]\nPDAL configuration for chunking fragments\nPOINT_CLOUD_CHUNK_SIZE\n\n\nbatch_size\nOptional[int]\nbatch size for dataset ingestion, defaults to BATCH_SIZE\nBATCH_SIZE\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\nstats\nbool\nbool, print TileDB stats to stdout\nFalse\n\n\nconfig\nOptional[Mapping[str, object]]\ndict, configuration to pass on tiledb.VFS\nNone\n\n\nid\nstr\nstr, ID for logging\n'pointcloud'\n\n\nlog_uri\nOptional[str]\nlog array URI\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nUnion[Sequence[os.PathLike], None]\nif not appending then a sequence of file paths\n\n\n\n\n\n\n\ncloud.geospatial.ingestion.ingest_raster_udf(\n    args={},\n    dataset_uri,\n    sources=None,\n    extents=None,\n    band_count=None,\n    dtype=None,\n    nodata=None,\n    pixels_per_fragment=PIXELS_PER_FRAGMENT,\n    tile_size=RASTER_TILE_SIZE,\n    resampling=DEFAULT_RASTER_SAMPLING,\n    append=False,\n    batch_size=BATCH_SIZE,\n    stats=False,\n    verbose=False,\n    config=None,\n    compressor=None,\n    id='raster',\n    trace=False,\n    log_uri=None,\n)\nInternal udf that ingests server side batch of raster files into tiledb arrays using Rasterio API\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nUnion[Dict, List]\ndict, input key value arguments as a dictionary\n{}\n\n\ndataset_uri\nstr\nstr, output TileDB array name\nrequired\n\n\nsources\nTuple[GeoBlockMetadata]\ntuple, sequence of GeoBlockMetadata objects containing the destination raster window and the input files that contribute to this window\nNone\n\n\nextents\nOptional[BoundingBox]\nExtents of the destination raster\nNone\n\n\nband_count\nOptional[int]\nint, number of bands in destination array\nNone\n\n\ndtype\nOptional[str]\nstr, dtype of destination array\nNone\n\n\nnodata\nOptional[float]\nfloat, NODATA value for destination raster\nNone\n\n\ntile_size\nint\nfor rasters this is the tile (block) size for the merged destination array, defaults to 1024\nRASTER_TILE_SIZE\n\n\npixels_per_fragment\nint\nThis is the number of pixels that will be written per fragment. Ideally aim to align as a factor of tile_size\nPIXELS_PER_FRAGMENT\n\n\nresampling\nstr\nstring, resampling method, one of None, bilinear, cubic, nearest and average\nDEFAULT_RASTER_SAMPLING\n\n\nappend\nbool\nbool, whether to append to the array\nFalse\n\n\nbatch_size\nint\nbatch size for dataset ingestion, defaults to BATCH_SIZE\nBATCH_SIZE\n\n\nstats\nbool\nbool, print TileDB stats to stdout\nFalse\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\nconfig\nOptional[Mapping[str, object]]\ndict, configuration to pass on tiledb.VFS\nNone\n\n\ncompressor\nOptional[dict]\ndict, serialized compression filter\nNone\n\n\nid\nstr\nstr, ID for logging\n'raster'\n\n\nlog_uri\nOptional[str]\nlog array URI\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nUnion[Sequence[GeoBlockMetadata], None]\nif not appending then a sequence of populated GeoBlockMetadata objects\n\n\n\n\n\n\n\ncloud.geospatial.ingestion.load_geometry_metadata(\n    sources,\n    *,\n    config=None,\n    verbose=False,\n    id='pointcloud_metadata',\n    trace=False,\n    log_uri=None,\n)\nReturn geospatial metadata for a sequence of input geometry data files\n:Return: list[GeoMetadata], a list of populated GeoMetadata objects\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsources\nIterable[os.PathLike]\nA sequence of paths or path to input\nrequired\n\n\nconfig\nOptional[Mapping[str, object]]\ndict configuration to pass on tiledb.VFS\nNone\n\n\nverbose\nbool\nbool, enable verbose logging, default is False\nFalse\n\n\ntrace\nbool\nbool, enable trace logging, default is False\nFalse\n\n\nlog_uri\nOptional[str]\nOptional[str] = None,\nNone\n\n\n\n\n\n\n\ncloud.geospatial.ingestion.load_pointcloud_metadata(\n    sources,\n    *,\n    config=None,\n    verbose=False,\n    id='pointcloud_metadata',\n    trace=False,\n    log_uri=None,\n)\nReturn geospatial metadata for a sequence of input point cloud data files\n:Return: list[GeoMetadata], a list of populated GeoMetadata objects\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsources\nIterable[os.PathLike]\niterator, paths or path to process\nrequired\n\n\nconfig\nOptional[Mapping[str, object]]\ndict, configuration to pass on tiledb.VFS\nNone\n\n\nverbose\nbool\nbool, enable verbose logging, default is False\nFalse\n\n\ntrace\nbool\nbool, enable trace logging, default is False\nFalse\n\n\nlog_uri\nOptional[str]\nOptional[str] = None,\nNone\n\n\n\n\n\n\n\ncloud.geospatial.ingestion.load_raster_metadata(\n    sources,\n    *,\n    config=None,\n    verbose=False,\n    id='raster_metadata',\n    trace=False,\n    log_uri=None,\n)\nReturn geospatial metadata for a sequence of input raster data files\n:Return: list[GeoMetadata]: list of populated GeoMetadata objects\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsources\nIterable[os.PathLike]\niterator, paths or path to process\nrequired\n\n\nconfig\nOptional[Mapping[str, object]]\ndict, configuration to pass on tiledb.VFS\nNone\n\n\nverbose\nbool\nbool, enable verbose logging, default is False\nFalse\n\n\ntrace\nbool\nbool, enable trace logging, default is False\nFalse\n\n\nid\nstr\nstr, ID for logging\n'raster_metadata'\n\n\nlog_uri\nOptional[str]\nOptional[str] = None,\nNone\n\n\n\n\n\n\n\ncloud.geospatial.ingestion.read_uris(\n    list_uri,\n    dataset_type,\n    *,\n    log_uri=None,\n    config=None,\n    max_files=None,\n    verbose=False,\n)\nRead a list of URIs from a URI.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlist_uri\nstr\nURI of the list of URIs\nrequired\n\n\ndataset_type\nDatasetType\ndataset type, one of pointcloud, raster or geometry\nrequired\n\n\nlog_uri\nOptional[str]\nlog array URI\nNone\n\n\nconfig\nOptional[Mapping[str, object]]\nconfig dictionary, defaults to None\nNone\n\n\nmax_files\nOptional[int]\nmaximum number of URIs returned, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nSequence[str]\nlist of URIs\n\n\n\n\n\n\n\ncloud.geospatial.ingestion.register_dataset_udf(\n    dataset_uri,\n    *,\n    register_name,\n    namespace=None,\n    acn=None,\n    config=None,\n    verbose=False,\n)\nRegister the dataset on TileDB Cloud.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nregister_name\nstr\nname to register the dataset with on TileDB Cloud\nrequired\n\n\nnamespace\nOptional[str]\nTileDB Cloud namespace, defaults to the user’s default namespace\nNone\n\n\nacn\nOptional[str]\nAccess Credentials Name (ACN) registered in TileDB Cloud (ARN type), defaults to None\nNone\n\n\nconfig\nOptional[Mapping[str, object]]\nconfig dictionary, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\ncloud.geospatial.ingestion.remove_dataset_type_from_array_meta(\n    dataset_uri,\n    *,\n    verbose=False,\n)\nRemoves dataset_type meta if the ingested result is an array. FIXME: This exists to fix an internal UI issue until formally fixed. FIXME: Related ticket -&gt; sc-48098\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse",
    "crumbs": [
      "Get Started",
      "Analyze",
      "geospatial.ingestion"
    ]
  },
  {
    "objectID": "reference/taskgraphs.builder.html",
    "href": "reference/taskgraphs.builder.html",
    "title": "taskgraphs.builder",
    "section": "",
    "text": "cloud.taskgraphs.builder\nThe code to build task graphs for later registration and execution.\n\n\n\n\n\nName\nDescription\n\n\n\n\nValOrNode\nType indicating that you can pass either a direct value or an input node.\n\n\nValOrNodeSeq\nEither a Node that yields a sequence or a sequence that may contain nodes.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nNode\nThe root type of a Node when building a task graph.\n\n\nTaskGraphBuilder\nThe thing you use to build a task graph.\n\n\n\n\n\ncloud.taskgraphs.builder.Node(owner, name, deps, *, fallback_name=None)\nThe root type of a Node when building a task graph.\nThe basic building block of a task graph. Nodes represent the data and execution steps within a TileDB task graph.\nbuilder.Nodes themselves are inert; they only represent the steps that will be taken by an Executor implementation to run the task graph. They should be treated as opaque and immutable; the Executor’s node objects are the ones that can be interacted with to get status and results.\n\n\n\n\n\nName\nDescription\n\n\n\n\ndisplay_name\nA friendly name for the Node.\n\n\nid\nA unique ID for this node.\n\n\nname\nThe name of the node. If absent, the node is unnamed.\n\n\nowner\nThe Builder this node comes from.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nto_registration_json\nConverts this node to the form used when registering the graph.\n\n\n\n\n\ncloud.taskgraphs.builder.Node.to_registration_json(existing_names)\nConverts this node to the form used when registering the graph.\nThis is the form of the Node that will be used to represent it in the RegisteredTaskGraph object, i.e. a RegisteredTaskGraphNode.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexisting_names\nSet[str]\nThe set of names that have already been used, so that we don’t generate a duplicate node name.\nrequired\n\n\n\n\n\n\n\n\n\ncloud.taskgraphs.builder.TaskGraphBuilder(name=None)\nThe thing you use to build a task graph.\nThis class only builds task graphs. The graphs it builds are static and only represent the steps to run (the recipe). The actual execution will be later performed by the executor.\n\n\n\n\n\nName\nDescription\n\n\n\n\nname\nA name for this graph.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nadd_dep\nManually requires that the parent must happen before child.\n\n\narray_read\nCreates a Node that will read data from a TileDB array.\n\n\ninput\nCreates a Node that can be used as an input to the graph.\n\n\nsql\nCreates a Node that executes an SQL query.\n\n\nudf\nCreates a Node which executes a UDF.\n\n\n\n\n\ncloud.taskgraphs.builder.TaskGraphBuilder.add_dep(parent, child)\nManually requires that the parent must happen before child.\nThis should rarely be necessary; including a parent node within the parameter list of a child node automatically adds a dependency.\n\n\n\ncloud.taskgraphs.builder.TaskGraphBuilder.array_read(\n    uri,\n    *,\n    raw_ranges=None,\n    buffers=None,\n    layout=None,\n    name=None,\n)\nCreates a Node that will read data from a TileDB array.\nThis Node is not executed immediately; instead, it is used in the same way as the array input to an Array UDF works: when an actual UDF is executed, the array is queried server-side and is passed as a parameter to the user code.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nValOrNode[str]\nThe URI to query against. This must be a tiledb:// URI. May be provided either as the URI itself, or as the output of an upstream node.\nrequired\n\n\nraw_ranges\nOptional[ValOrNodeSeq[Any]]\nThe ranges to query against. This is called “raw” because we accept the format that is passed to the server:: [ [startDim1A, endDim1A, startDim1B, endDim1B, …], [startDim2A, endDim2A, startDim2B, endDim2B, …], ] This may also be provided as either a value or a Node output.\nNone\n\n\nbuffers\nOptional[ValOrNodeSeq[str]]\nOptionally, the buffers to query against. May be either a raw value or the Node output.\nNone\n\n\nname\nOptional[str]\nAn optional name for this Node.\nNone\n\n\n\n\n\n\n\ncloud.taskgraphs.builder.TaskGraphBuilder.input(name, default_value=_NOTHING)\nCreates a Node that can be used as an input to the graph.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nThe name of this input. Required, since it is used when executing to match the input to the Node.\nrequired\n\n\ndefault_value\n_T\nAn optional default value to use when executing. If not provided, the caller is required to set this input when running the task graph.\n_NOTHING\n\n\n\n\n\n\n\ncloud.taskgraphs.builder.TaskGraphBuilder.sql(\n    query,\n    init_commands=(),\n    parameters=(),\n    *,\n    result_format='arrow',\n    resource_class=None,\n    download_results=None,\n    namespace=None,\n    name=None,\n)\nCreates a Node that executes an SQL query.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nquery\nstr\nThe query to execute. This must be a string, and cannot be the output of a previous node.\nrequired\n\n\ninit_commands\nIterable[str]\nA list of SQL commands to execute in the session before running query.\n()\n\n\nparameters\nValOrNodeSeq\nA sequence of objects to provide as parameters for the ? placeholders in the query. These may be provided either as values or as the output of earlier Nodes.\n()\n\n\nresult_format\nstr\nThe format to provide results in. Either json or arrow.\n'arrow'\n\n\nresource_class\nOptional[str]\nIf specified, the container resource class that this UDF will be executed in.\nNone\n\n\ndownload_results\nOptional[bool]\nIf True, download results eagerly (i.e., immediately when the function returns). If False, download results lazily (i.e., only when you call .result() on an execution). If unset (the default), automatically choose whether to download results: eagerly if it’s a terminal node, or if it has a local dependent; lazily if it’s an internal node.\nNone\n\n\n\n\n\n\n\ncloud.taskgraphs.builder.TaskGraphBuilder.udf(\n    func,\n    args=types.Arguments(),\n    *,\n    result_format='tiledb_json',\n    include_source=True,\n    image_name=None,\n    timeout=None,\n    resource_class=None,\n    namespace=None,\n    name=None,\n    local=False,\n    download_results=None,\n)\nCreates a Node which executes a UDF.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\nfunctions.Funcable[_T]\nThe function to call; either a Python callable or a registered UDF name.\nrequired\n\n\nargs\ntypes.Arguments\nThe arguments to pass to this function. These may contain values or Nodes.\ntypes.Arguments()\n\n\nresult_format\nOptional[str]\nThe format to return results in.\n'tiledb_json'\n\n\ninclude_source\nbool\nTrue (the default) to include the function source in the request. This is useful for debugging and logging, but does not have any impact on the UDF’s execution. False to omit source.\nTrue\n\n\nimage_name\nOptional[str]\nIf specified, will execute the UDF within the specified image rather than the default image for its language.\nNone\n\n\ntimeout\nUnion[datetime.timedelta, int, None]\nIf specified, the length of time after which the UDF will be terminated on the server side. If specified as a number, a number of seconds. If zero or unset, the UDF will run until the server’s configured maximum. Unlike the timeout parameter to Future-like objects, this sets a limit on actual execution time, rather than just a limit on how long to wait.\nNone\n\n\nresource_class\nOptional[str]\nIf specified, the container resource class that this UDF will be executed in.\nNone\n\n\nnamespace\nOptional[str]\nIf specified, the non-default namespace that the UDF will be executed under. This will also be the namespace used for reading any array nodes used in this UDF’s input.\nNone\n\n\nlocal\nbool\nIf True, will attempt to run the UDF on the client machine. If this is not possible, the UDF will fail.\nFalse\n\n\ndownload_results\nOptional[bool]\nIf True, download results eagerly (i.e., immediately when the function returns). If False, download results lazily (i.e., only when you call .result() on an execution). If unset (the default), automatically choose whether to download results: eagerly if it’s a terminal node, or if it has a local dependent; lazily if it’s an internal node.\nNone",
    "crumbs": [
      "Get Started",
      "Scale",
      "taskgraphs.builder"
    ]
  },
  {
    "objectID": "reference/taskgraphs.builder.html#attributes",
    "href": "reference/taskgraphs.builder.html#attributes",
    "title": "taskgraphs.builder",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nValOrNode\nType indicating that you can pass either a direct value or an input node.\n\n\nValOrNodeSeq\nEither a Node that yields a sequence or a sequence that may contain nodes.",
    "crumbs": [
      "Get Started",
      "Scale",
      "taskgraphs.builder"
    ]
  },
  {
    "objectID": "reference/taskgraphs.builder.html#classes",
    "href": "reference/taskgraphs.builder.html#classes",
    "title": "taskgraphs.builder",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nNode\nThe root type of a Node when building a task graph.\n\n\nTaskGraphBuilder\nThe thing you use to build a task graph.\n\n\n\n\n\ncloud.taskgraphs.builder.Node(owner, name, deps, *, fallback_name=None)\nThe root type of a Node when building a task graph.\nThe basic building block of a task graph. Nodes represent the data and execution steps within a TileDB task graph.\nbuilder.Nodes themselves are inert; they only represent the steps that will be taken by an Executor implementation to run the task graph. They should be treated as opaque and immutable; the Executor’s node objects are the ones that can be interacted with to get status and results.\n\n\n\n\n\nName\nDescription\n\n\n\n\ndisplay_name\nA friendly name for the Node.\n\n\nid\nA unique ID for this node.\n\n\nname\nThe name of the node. If absent, the node is unnamed.\n\n\nowner\nThe Builder this node comes from.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nto_registration_json\nConverts this node to the form used when registering the graph.\n\n\n\n\n\ncloud.taskgraphs.builder.Node.to_registration_json(existing_names)\nConverts this node to the form used when registering the graph.\nThis is the form of the Node that will be used to represent it in the RegisteredTaskGraph object, i.e. a RegisteredTaskGraphNode.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexisting_names\nSet[str]\nThe set of names that have already been used, so that we don’t generate a duplicate node name.\nrequired\n\n\n\n\n\n\n\n\n\ncloud.taskgraphs.builder.TaskGraphBuilder(name=None)\nThe thing you use to build a task graph.\nThis class only builds task graphs. The graphs it builds are static and only represent the steps to run (the recipe). The actual execution will be later performed by the executor.\n\n\n\n\n\nName\nDescription\n\n\n\n\nname\nA name for this graph.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nadd_dep\nManually requires that the parent must happen before child.\n\n\narray_read\nCreates a Node that will read data from a TileDB array.\n\n\ninput\nCreates a Node that can be used as an input to the graph.\n\n\nsql\nCreates a Node that executes an SQL query.\n\n\nudf\nCreates a Node which executes a UDF.\n\n\n\n\n\ncloud.taskgraphs.builder.TaskGraphBuilder.add_dep(parent, child)\nManually requires that the parent must happen before child.\nThis should rarely be necessary; including a parent node within the parameter list of a child node automatically adds a dependency.\n\n\n\ncloud.taskgraphs.builder.TaskGraphBuilder.array_read(\n    uri,\n    *,\n    raw_ranges=None,\n    buffers=None,\n    layout=None,\n    name=None,\n)\nCreates a Node that will read data from a TileDB array.\nThis Node is not executed immediately; instead, it is used in the same way as the array input to an Array UDF works: when an actual UDF is executed, the array is queried server-side and is passed as a parameter to the user code.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nValOrNode[str]\nThe URI to query against. This must be a tiledb:// URI. May be provided either as the URI itself, or as the output of an upstream node.\nrequired\n\n\nraw_ranges\nOptional[ValOrNodeSeq[Any]]\nThe ranges to query against. This is called “raw” because we accept the format that is passed to the server:: [ [startDim1A, endDim1A, startDim1B, endDim1B, …], [startDim2A, endDim2A, startDim2B, endDim2B, …], ] This may also be provided as either a value or a Node output.\nNone\n\n\nbuffers\nOptional[ValOrNodeSeq[str]]\nOptionally, the buffers to query against. May be either a raw value or the Node output.\nNone\n\n\nname\nOptional[str]\nAn optional name for this Node.\nNone\n\n\n\n\n\n\n\ncloud.taskgraphs.builder.TaskGraphBuilder.input(name, default_value=_NOTHING)\nCreates a Node that can be used as an input to the graph.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nThe name of this input. Required, since it is used when executing to match the input to the Node.\nrequired\n\n\ndefault_value\n_T\nAn optional default value to use when executing. If not provided, the caller is required to set this input when running the task graph.\n_NOTHING\n\n\n\n\n\n\n\ncloud.taskgraphs.builder.TaskGraphBuilder.sql(\n    query,\n    init_commands=(),\n    parameters=(),\n    *,\n    result_format='arrow',\n    resource_class=None,\n    download_results=None,\n    namespace=None,\n    name=None,\n)\nCreates a Node that executes an SQL query.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nquery\nstr\nThe query to execute. This must be a string, and cannot be the output of a previous node.\nrequired\n\n\ninit_commands\nIterable[str]\nA list of SQL commands to execute in the session before running query.\n()\n\n\nparameters\nValOrNodeSeq\nA sequence of objects to provide as parameters for the ? placeholders in the query. These may be provided either as values or as the output of earlier Nodes.\n()\n\n\nresult_format\nstr\nThe format to provide results in. Either json or arrow.\n'arrow'\n\n\nresource_class\nOptional[str]\nIf specified, the container resource class that this UDF will be executed in.\nNone\n\n\ndownload_results\nOptional[bool]\nIf True, download results eagerly (i.e., immediately when the function returns). If False, download results lazily (i.e., only when you call .result() on an execution). If unset (the default), automatically choose whether to download results: eagerly if it’s a terminal node, or if it has a local dependent; lazily if it’s an internal node.\nNone\n\n\n\n\n\n\n\ncloud.taskgraphs.builder.TaskGraphBuilder.udf(\n    func,\n    args=types.Arguments(),\n    *,\n    result_format='tiledb_json',\n    include_source=True,\n    image_name=None,\n    timeout=None,\n    resource_class=None,\n    namespace=None,\n    name=None,\n    local=False,\n    download_results=None,\n)\nCreates a Node which executes a UDF.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\nfunctions.Funcable[_T]\nThe function to call; either a Python callable or a registered UDF name.\nrequired\n\n\nargs\ntypes.Arguments\nThe arguments to pass to this function. These may contain values or Nodes.\ntypes.Arguments()\n\n\nresult_format\nOptional[str]\nThe format to return results in.\n'tiledb_json'\n\n\ninclude_source\nbool\nTrue (the default) to include the function source in the request. This is useful for debugging and logging, but does not have any impact on the UDF’s execution. False to omit source.\nTrue\n\n\nimage_name\nOptional[str]\nIf specified, will execute the UDF within the specified image rather than the default image for its language.\nNone\n\n\ntimeout\nUnion[datetime.timedelta, int, None]\nIf specified, the length of time after which the UDF will be terminated on the server side. If specified as a number, a number of seconds. If zero or unset, the UDF will run until the server’s configured maximum. Unlike the timeout parameter to Future-like objects, this sets a limit on actual execution time, rather than just a limit on how long to wait.\nNone\n\n\nresource_class\nOptional[str]\nIf specified, the container resource class that this UDF will be executed in.\nNone\n\n\nnamespace\nOptional[str]\nIf specified, the non-default namespace that the UDF will be executed under. This will also be the namespace used for reading any array nodes used in this UDF’s input.\nNone\n\n\nlocal\nbool\nIf True, will attempt to run the UDF on the client machine. If this is not possible, the UDF will fail.\nFalse\n\n\ndownload_results\nOptional[bool]\nIf True, download results eagerly (i.e., immediately when the function returns). If False, download results lazily (i.e., only when you call .result() on an execution). If unset (the default), automatically choose whether to download results: eagerly if it’s a terminal node, or if it has a local dependent; lazily if it’s an internal node.\nNone",
    "crumbs": [
      "Get Started",
      "Scale",
      "taskgraphs.builder"
    ]
  },
  {
    "objectID": "reference/vcf.allele_frequency.html",
    "href": "reference/vcf.allele_frequency.html",
    "title": "vcf.allele_frequency",
    "section": "",
    "text": "cloud.vcf.allele_frequency\n\n\n\n\n\nName\nDescription\n\n\n\n\ncalc_af\nConsolidate AC and compute AN, AF\n\n\nread_allele_frequency\nRead variant status\n\n\n\n\n\ncloud.vcf.allele_frequency.calc_af(df)\nConsolidate AC and compute AN, AF\n\n\n\ncloud.vcf.allele_frequency.read_allele_frequency(dataset_uri, region)\nRead variant status\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nregion\nstr\ngenomics region to read\nrequired",
    "crumbs": [
      "Get Started",
      "Analyze",
      "vcf.allele_frequency"
    ]
  },
  {
    "objectID": "reference/vcf.allele_frequency.html#functions",
    "href": "reference/vcf.allele_frequency.html#functions",
    "title": "vcf.allele_frequency",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncalc_af\nConsolidate AC and compute AN, AF\n\n\nread_allele_frequency\nRead variant status\n\n\n\n\n\ncloud.vcf.allele_frequency.calc_af(df)\nConsolidate AC and compute AN, AF\n\n\n\ncloud.vcf.allele_frequency.read_allele_frequency(dataset_uri, region)\nRead variant status\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nregion\nstr\ngenomics region to read\nrequired",
    "crumbs": [
      "Get Started",
      "Analyze",
      "vcf.allele_frequency"
    ]
  },
  {
    "objectID": "reference/invites.html",
    "href": "reference/invites.html",
    "title": "invites",
    "section": "",
    "text": "cloud.invites\n\n\n\n\n\nName\nDescription\n\n\n\n\naccept_invitation\nAccept an invitation.\n\n\ncancel_invite_to_array\nCancels array sharing invitation.\n\n\ncancel_invite_to_group\nCancels group sharing invitation.\n\n\ncancel_invite_to_organization\nCancels join organization invitation.\n\n\nfetch_invitations\nFetches a paginated list of invitations.\n\n\ninvite_to_array\nShare array by email invite.\n\n\ninvite_to_group\nSends email to multiple recipients with sharing information regarding a group.\n\n\ninvite_to_organization\nSends email to multiple recipients with joining information\n\n\n\n\n\ncloud.invites.accept_invitation(invitation_id)\nAccept an invitation.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninvitation_id\nstr\nthe ID of invitation about to be accepted.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone\n\n\n\n\n\n\n\ncloud.invites.cancel_invite_to_array(uri, *, invitation_id)\nCancels array sharing invitation.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nURI of array in the form ‘tiledb:///’\nrequired\n\n\ninvitation_id\nstr\nthe ID of invitation about to be canceled.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone\n\n\n\n\n\n\n\ncloud.invites.cancel_invite_to_group(uri, *, invitation_id)\nCancels group sharing invitation.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nURI of group in the form ‘tiledb:///’\nrequired\n\n\ninvitation_id\nstr\nthe ID of invitation about to be canceled.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone\n\n\n\n\n\n\n\ncloud.invites.cancel_invite_to_organization(organization, *, invitation_id)\nCancels join organization invitation.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\norganization\nstr\nname or UUID of organization.\nrequired\n\n\ninvitation_id\nstr\nthe ID of invitation about to be canceled.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone\n\n\n\n\n\n\n\ncloud.invites.fetch_invitations(**filters)\nFetches a paginated list of invitations.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\norganization\nstr\nname or ID of organization to filter\nrequired\n\n\narray\nstr\nname/uri of array that is url-encoded to filter\nrequired\n\n\ngroup\nstr\nname or ID of group to filter\nrequired\n\n\nstart\nint\nstart time for tasks to filter by\nrequired\n\n\nend\nint\nend time for tasks to filter by\nrequired\n\n\npage\nint\npagination offset\nrequired\n\n\nper_page\nint\npagination limit\nrequired\n\n\ntype\nstr\ninvitation type, “ARRAY_SHARE”, “JOIN_ORGANIZATION”\nrequired\n\n\nstatus\nstr\nFilter to only return “PENDING”, “ACCEPTED”\nrequired\n\n\norderby\nstr\nsort by which field valid values include timestamp, array_name, organization_name\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nInvitations and pagination metadata.\n\n\n\n\n\n\n\ncloud.invites.invite_to_array(uri, *, recipients, actions)\nShare array by email invite.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nURI of array in the form ‘tiledb:///’\nrequired\n\n\nrecipients\nSequence[str]\nlist of recipient emails/usernames.\nrequired\n\n\nactions\nSequence[str]\nlist of ArrayActions allowed to the recipient.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone\n\n\n\n\n\n\n\ncloud.invites.invite_to_group(uri, *, recipients, array_actions, group_actions)\nSends email to multiple recipients with sharing information regarding a group.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nURI of group in the form ‘tiledb:///’\nrequired\n\n\nrecipients\nSequence[str]\nlist of recipient emails/usernames.\nrequired\n\n\narray_actions\nSequence[str]\nlist of ArrayActions allowed to the recipient.\nrequired\n\n\ngroup_actions\nSequence[str]\nlist of GroupActions allowed to the recipient.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone\n\n\n\n\n\n\n\ncloud.invites.invite_to_organization(\n    organization,\n    *,\n    recipients,\n    role,\n    actions=None,\n)\nSends email to multiple recipients with joining information regarding an organization.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\norganization\nstr\nname or UUID of organization.\nrequired\n\n\nrecipients\nSequence[str]\nlist of recipient emails/usernames.\nrequired\n\n\nrole\nstr\nrole assigned to the recipient.\nrequired\n\n\nactions\nOptional[Sequence[str]]\nan optional sequence of actions.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone",
    "crumbs": [
      "Get Started",
      "Collaborate",
      "invites"
    ]
  },
  {
    "objectID": "reference/invites.html#functions",
    "href": "reference/invites.html#functions",
    "title": "invites",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\naccept_invitation\nAccept an invitation.\n\n\ncancel_invite_to_array\nCancels array sharing invitation.\n\n\ncancel_invite_to_group\nCancels group sharing invitation.\n\n\ncancel_invite_to_organization\nCancels join organization invitation.\n\n\nfetch_invitations\nFetches a paginated list of invitations.\n\n\ninvite_to_array\nShare array by email invite.\n\n\ninvite_to_group\nSends email to multiple recipients with sharing information regarding a group.\n\n\ninvite_to_organization\nSends email to multiple recipients with joining information\n\n\n\n\n\ncloud.invites.accept_invitation(invitation_id)\nAccept an invitation.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninvitation_id\nstr\nthe ID of invitation about to be accepted.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone\n\n\n\n\n\n\n\ncloud.invites.cancel_invite_to_array(uri, *, invitation_id)\nCancels array sharing invitation.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nURI of array in the form ‘tiledb:///’\nrequired\n\n\ninvitation_id\nstr\nthe ID of invitation about to be canceled.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone\n\n\n\n\n\n\n\ncloud.invites.cancel_invite_to_group(uri, *, invitation_id)\nCancels group sharing invitation.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nURI of group in the form ‘tiledb:///’\nrequired\n\n\ninvitation_id\nstr\nthe ID of invitation about to be canceled.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone\n\n\n\n\n\n\n\ncloud.invites.cancel_invite_to_organization(organization, *, invitation_id)\nCancels join organization invitation.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\norganization\nstr\nname or UUID of organization.\nrequired\n\n\ninvitation_id\nstr\nthe ID of invitation about to be canceled.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone\n\n\n\n\n\n\n\ncloud.invites.fetch_invitations(**filters)\nFetches a paginated list of invitations.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\norganization\nstr\nname or ID of organization to filter\nrequired\n\n\narray\nstr\nname/uri of array that is url-encoded to filter\nrequired\n\n\ngroup\nstr\nname or ID of group to filter\nrequired\n\n\nstart\nint\nstart time for tasks to filter by\nrequired\n\n\nend\nint\nend time for tasks to filter by\nrequired\n\n\npage\nint\npagination offset\nrequired\n\n\nper_page\nint\npagination limit\nrequired\n\n\ntype\nstr\ninvitation type, “ARRAY_SHARE”, “JOIN_ORGANIZATION”\nrequired\n\n\nstatus\nstr\nFilter to only return “PENDING”, “ACCEPTED”\nrequired\n\n\norderby\nstr\nsort by which field valid values include timestamp, array_name, organization_name\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nInvitations and pagination metadata.\n\n\n\n\n\n\n\ncloud.invites.invite_to_array(uri, *, recipients, actions)\nShare array by email invite.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nURI of array in the form ‘tiledb:///’\nrequired\n\n\nrecipients\nSequence[str]\nlist of recipient emails/usernames.\nrequired\n\n\nactions\nSequence[str]\nlist of ArrayActions allowed to the recipient.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone\n\n\n\n\n\n\n\ncloud.invites.invite_to_group(uri, *, recipients, array_actions, group_actions)\nSends email to multiple recipients with sharing information regarding a group.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nURI of group in the form ‘tiledb:///’\nrequired\n\n\nrecipients\nSequence[str]\nlist of recipient emails/usernames.\nrequired\n\n\narray_actions\nSequence[str]\nlist of ArrayActions allowed to the recipient.\nrequired\n\n\ngroup_actions\nSequence[str]\nlist of GroupActions allowed to the recipient.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone\n\n\n\n\n\n\n\ncloud.invites.invite_to_organization(\n    organization,\n    *,\n    recipients,\n    role,\n    actions=None,\n)\nSends email to multiple recipients with joining information regarding an organization.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\norganization\nstr\nname or UUID of organization.\nrequired\n\n\nrecipients\nSequence[str]\nlist of recipient emails/usernames.\nrequired\n\n\nrole\nstr\nrole assigned to the recipient.\nrequired\n\n\nactions\nOptional[Sequence[str]]\nan optional sequence of actions.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone",
    "crumbs": [
      "Get Started",
      "Collaborate",
      "invites"
    ]
  },
  {
    "objectID": "reference/tasks.html",
    "href": "reference/tasks.html",
    "title": "tasks",
    "section": "",
    "text": "cloud.tasks\n\n\n\n\n\nName\nDescription\n\n\n\n\nfetch_results\nFetches the results of a previously-executed UDF or SQL query.\n\n\nfetch_results_pandas\nFetches the results of a previously-executed UDF or SQL query.\n\n\nfetch_tasks\nFetch all tasks a user has access too.\n\n\nlast_sql_task\nFetch the last run sql array task\n\n\nlast_udf_task\nFetch the last run udf task\n\n\ntask\nFetch a single array task\n\n\n\n\n\ncloud.tasks.fetch_results(task_id, *, result_format=None)\nFetches the results of a previously-executed UDF or SQL query.\n\n\n\ncloud.tasks.fetch_results_pandas(\n    task_id,\n    *,\n    result_format=models.ResultFormat.NATIVE,\n)\nFetches the results of a previously-executed UDF or SQL query.\n\n\n\ncloud.tasks.fetch_tasks(\n    namespace=None,\n    array=None,\n    start=None,\n    end=datetime.datetime.now(datetime.timezone.utc),\n    status=None,\n    page=None,\n    per_page=None,\n    async_req=False,\n)\nFetch all tasks a user has access too.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnamespace\nstr\noptional filter by namespace\nNone\n\n\narray\nstr\noptional limit tasks to specific array\nNone\n\n\nstart\ndatetime\noptional start time for listing of tasks, defaults to 7 days ago\nNone\n\n\nend\ndatetime\noptional end time for listing of tasks defaults to now\ndatetime.datetime.now(datetime.timezone.utc)\n\n\nstatus\nstr\noptional filter on status can be one of [‘FAILED’, ‘RUNNING’, ‘COMPLETED’]\nNone\n\n\npage\nint\noptional page for pagenating results\nNone\n\n\nper_page\nint\noptional records to return per page\nNone\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDict[str, Sequence[Dict[str, Any]]]\nMapping of task data organized by task type.\n\n\n\n\n\n\n\ncloud.tasks.last_sql_task()\nFetch the last run sql array task\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nobject with task details\n\n\n\n\n\n\n\ncloud.tasks.last_udf_task()\nFetch the last run udf task\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nobject with task details\n\n\n\n\n\n\n\ncloud.tasks.task(id, async_req=False)\nFetch a single array task\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nid\nstr\nid to lookup\nrequired\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nobject with task details",
    "crumbs": [
      "Get Started",
      "Scale",
      "tasks"
    ]
  },
  {
    "objectID": "reference/tasks.html#functions",
    "href": "reference/tasks.html#functions",
    "title": "tasks",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nfetch_results\nFetches the results of a previously-executed UDF or SQL query.\n\n\nfetch_results_pandas\nFetches the results of a previously-executed UDF or SQL query.\n\n\nfetch_tasks\nFetch all tasks a user has access too.\n\n\nlast_sql_task\nFetch the last run sql array task\n\n\nlast_udf_task\nFetch the last run udf task\n\n\ntask\nFetch a single array task\n\n\n\n\n\ncloud.tasks.fetch_results(task_id, *, result_format=None)\nFetches the results of a previously-executed UDF or SQL query.\n\n\n\ncloud.tasks.fetch_results_pandas(\n    task_id,\n    *,\n    result_format=models.ResultFormat.NATIVE,\n)\nFetches the results of a previously-executed UDF or SQL query.\n\n\n\ncloud.tasks.fetch_tasks(\n    namespace=None,\n    array=None,\n    start=None,\n    end=datetime.datetime.now(datetime.timezone.utc),\n    status=None,\n    page=None,\n    per_page=None,\n    async_req=False,\n)\nFetch all tasks a user has access too.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnamespace\nstr\noptional filter by namespace\nNone\n\n\narray\nstr\noptional limit tasks to specific array\nNone\n\n\nstart\ndatetime\noptional start time for listing of tasks, defaults to 7 days ago\nNone\n\n\nend\ndatetime\noptional end time for listing of tasks defaults to now\ndatetime.datetime.now(datetime.timezone.utc)\n\n\nstatus\nstr\noptional filter on status can be one of [‘FAILED’, ‘RUNNING’, ‘COMPLETED’]\nNone\n\n\npage\nint\noptional page for pagenating results\nNone\n\n\nper_page\nint\noptional records to return per page\nNone\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDict[str, Sequence[Dict[str, Any]]]\nMapping of task data organized by task type.\n\n\n\n\n\n\n\ncloud.tasks.last_sql_task()\nFetch the last run sql array task\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nobject with task details\n\n\n\n\n\n\n\ncloud.tasks.last_udf_task()\nFetch the last run udf task\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nobject with task details\n\n\n\n\n\n\n\ncloud.tasks.task(id, async_req=False)\nFetch a single array task\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nid\nstr\nid to lookup\nrequired\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nobject with task details",
    "crumbs": [
      "Get Started",
      "Scale",
      "tasks"
    ]
  },
  {
    "objectID": "reference/dashboard.html",
    "href": "reference/dashboard.html",
    "title": "dashboard",
    "section": "",
    "text": "cloud.dashboard\n\n\n\n\n\nName\nDescription\n\n\n\n\npanel_dashboard\nStart a Panel server with the provided app and return an iframe that enables\n\n\n\n\n\ncloud.dashboard.panel_dashboard(\n    app,\n    *,\n    full_screen_link=True,\n    title='Dashboard - TileDB',\n    verbose=False,\n    vh_delta=70,\n)\nStart a Panel server with the provided app and return an iframe that enables viewing the app in Voila. This wrapper avoids issues with reactive Panel apps in Voila.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\napp\nAny\nThe Panel app to serve.\nrequired\n\n\nfull_screen_link\nbool\nInclude a link to the full screen view, defaults to True.\nTrue\n\n\ntitle\n\nTitle for the Panel server, defaults to “Dashboard - TileDB”.\n'Dashboard - TileDB'\n\n\nverbose\nbool\nEnable verbose logging, defaults to False.\nFalse\n\n\nvh_delta\nint\nThe vertical height delta for the iframe in px. This value is subtracted from 100vh to set the height of the iframe.\n70\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nAny\nA Panel HTML pane containing an iframe that displays the app.",
    "crumbs": [
      "Get Started",
      "Catalog",
      "dashboard"
    ]
  },
  {
    "objectID": "reference/dashboard.html#functions",
    "href": "reference/dashboard.html#functions",
    "title": "dashboard",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\npanel_dashboard\nStart a Panel server with the provided app and return an iframe that enables\n\n\n\n\n\ncloud.dashboard.panel_dashboard(\n    app,\n    *,\n    full_screen_link=True,\n    title='Dashboard - TileDB',\n    verbose=False,\n    vh_delta=70,\n)\nStart a Panel server with the provided app and return an iframe that enables viewing the app in Voila. This wrapper avoids issues with reactive Panel apps in Voila.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\napp\nAny\nThe Panel app to serve.\nrequired\n\n\nfull_screen_link\nbool\nInclude a link to the full screen view, defaults to True.\nTrue\n\n\ntitle\n\nTitle for the Panel server, defaults to “Dashboard - TileDB”.\n'Dashboard - TileDB'\n\n\nverbose\nbool\nEnable verbose logging, defaults to False.\nFalse\n\n\nvh_delta\nint\nThe vertical height delta for the iframe in px. This value is subtracted from 100vh to set the height of the iframe.\n70\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nAny\nA Panel HTML pane containing an iframe that displays the app.",
    "crumbs": [
      "Get Started",
      "Catalog",
      "dashboard"
    ]
  },
  {
    "objectID": "reference/udf.html",
    "href": "reference/udf.html",
    "title": "udf",
    "section": "",
    "text": "cloud.udf\n\n\n\n\n\nName\nDescription\n\n\n\n\ndelete\nDeletes a registered udf\n\n\nderegister\nDe-registers a registered udf, by de-registering the array that it\n\n\nexec\nRun a user defined function, synchronously, returning only the result.\n\n\nexec_async\nRun a user defined function, asynchronously.\n\n\nexec_base\nRun a user defined function, returning the result and metadata.\n\n\ninfo\nFetch info on a registered udf\n\n\nregister_generic_udf\n\n\n\nregister_multi_array_udf\n\n\n\nregister_single_array_udf\n\n\n\nregister_udf\n\n\n\nshare\nShare a registered udf\n\n\nunshare\nShare a registered udf\n\n\nupdate_generic_udf\n\n\n\nupdate_single_array_udf\n\n\n\nupdate_udf\n\n\n\n\n\n\ncloud.udf.delete(uri, *, async_req=False)\nDeletes a registered udf\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nTileDB URI of the udf, defaults to None.\nrequired\n\n\nasync_req\nbool\nReturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\ndeleted udf details\n\n\n\n\n\n\n\ncloud.udf.deregister(uri, *, async_req=False)\nDe-registers a registered udf, by de-registering the array that it is registered on. This does not physically delete the array, it will remain in your bucket. All access to the array and cloud metadata will be removed.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nTileDB URI of the array.\nrequired\n\n\nasync_req\nbool\nReturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\ncloud.udf.exec(*args, **kwargs)\nRun a user defined function, synchronously, returning only the result.\nArguments are exactly as in :func:exec_base.\n\n\n\ncloud.udf.exec_async(*args, **kwargs)\nRun a user defined function, asynchronously.\nArguments are exactly as in :func:exec_base.\n\n\n\ncloud.udf.exec_base(\n    func,\n    *args,\n    name=None,\n    namespace=None,\n    image_name='default',\n    http_compressor='deflate',\n    include_source_lines=True,\n    task_name=None,\n    result_format='tiledb_json',\n    result_format_version=None,\n    store_results=False,\n    stored_param_uuids=(),\n    timeout=None,\n    resource_class=None,\n    _download_results=True,\n    _server_graph_uuid=None,\n    _client_node_uuid=None,\n    access_credentials_name=None,\n    **kwargs,\n)\nRun a user defined function, returning the result and metadata.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\nUnion[str, Callable, Any]\nThe function to call, either as a callable function, or as the name of a registered user-defined function. (If name is set, this is instead prepended to args for backwards compatibility.)\nrequired\n\n\nargs\nAny\nThe arguments to pass to the function.\n()\n\n\nname\nOptional[str]\nDEPRECATED. If present, the name of the user-defined function to run.\nNone\n\n\nnamespace\nOptional[str]\nnamespace to run udf under\nNone\n\n\nimage_name\nstr\nudf image name to use, useful for testing beta features\n'default'\n\n\nhttp_compressor\nOptional[str]\nset http compressor for results\n'deflate'\n\n\ninclude_source_lines\nbool\nTrue to send the source code of your UDF to the server with your request. (This means it can be shown to you in stack traces if an error occurs.) False to send only compiled Python bytecode.\nTrue\n\n\ntask_name\nstr\noptional name to assign the task for logging and audit purposes\nNone\n\n\nresult_format\nResultFormat\nresult serialization format\n'tiledb_json'\n\n\nresult_format_version\nstr\nDeprecated and ignored.\nNone\n\n\nstore_results\nbool\nTrue to temporarily store results on the server side for later retrieval (in addition to downloading them).\nFalse\n\n\ntimeout\nint\nTimeout for UDF in seconds\nNone\n\n\nresource_class\nOptional[str]\nThe name of the resource class to use. Resource classes define maximum limits for cpu and memory usage.\nNone\n\n\n_download_results\nbool\nTrue to download and parse results eagerly. False to not download results by default and only do so lazily (e.g. for an intermediate node in a graph).\nTrue\n\n\n_server_graph_uuid\nOptional[uuid.UUID]\nIf this function is being executed within a DAG, the server-generated ID of the graph’s log. Otherwise, None.\nNone\n\n\n_client_node_uuid\nOptional[uuid.UUID]\nIf this function is being executed within a DAG, the ID of this function’s node within the graph. Otherwise, None.\nNone\n\n\nkwargs\n\nnamed arguments to pass to function\n{}\n\n\n\n\n\n\n\ncloud.udf.info(namespace=None, name=None, async_req=False)\nFetch info on a registered udf\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnamespace\n\nnamespace to filter to\nNone\n\n\nname\n\nname of udf to get info\nNone\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nregistered udf details\n\n\n\n\n\n\n\ncloud.udf.register_generic_udf(\n    func,\n    name,\n    namespace=None,\n    image_name=None,\n    include_source_lines=True,\n    async_req=False,\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\n\nfunction to register\nrequired\n\n\nname\n\nname of udf to register\nrequired\n\n\nnamespace\n\nnamespace to register in\nNone\n\n\nimage_name\n\noptional image name\nNone\n\n\ninclude_source_lines\n\ndisables sending sources lines of function along with udf\nTrue\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncloud.udf.register_multi_array_udf(\n    func,\n    name,\n    namespace=None,\n    image_name=None,\n    include_source_lines=True,\n    async_req=False,\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\n\nfunction to register\nrequired\n\n\nname\n\nname of udf to register\nrequired\n\n\nnamespace\n\nnamespace to register in\nNone\n\n\nimage_name\n\noptional image name\nNone\n\n\ninclude_source_lines\n\ndisables sending sources lines of function along with udf\nTrue\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncloud.udf.register_single_array_udf(\n    func,\n    name,\n    namespace=None,\n    image_name=None,\n    include_source_lines=True,\n    async_req=False,\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\n\nfunction to register\nrequired\n\n\nname\n\nname of udf to register\nrequired\n\n\nnamespace\n\nnamespace to register in\nNone\n\n\nimage_name\n\noptional image name\nNone\n\n\ninclude_source_lines\n\ndisables sending sources lines of function along with udf\nTrue\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncloud.udf.register_udf(\n    func,\n    name,\n    namespace=None,\n    image_name=None,\n    type=None,\n    include_source_lines=True,\n    async_req=False,\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\n\nfunction to register\nrequired\n\n\nname\n\nname of udf to register\nrequired\n\n\nnamespace\n\nnamespace to register in\nNone\n\n\nimage_name\n\noptional image name\nNone\n\n\ntype\n\ntype of udf, generic or single_array\nNone\n\n\ninclude_source_lines\n\ndisables sending sources lines of function along with udf\nTrue\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncloud.udf.share(name=None, namespace=None, async_req=False)\nShare a registered udf\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\n\nname of udf in “namespace/name” format\nNone\n\n\nnamespace\n\nnamespace to share array with\nNone\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nregistered udf details\n\n\n\n\n\n\n\ncloud.udf.unshare(name=None, namespace=None, async_req=False)\nShare a registered udf\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\n\nname of udf in “namespace/name” format\nNone\n\n\nnamespace\n\nnamespace to share array with\nNone\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nregistered udf details\n\n\n\n\n\n\n\ncloud.udf.update_generic_udf(\n    func,\n    name,\n    namespace=None,\n    image_name=None,\n    include_source_lines=True,\n    async_req=False,\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\n\nfunction to register\nrequired\n\n\nname\n\nname of udf to register\nrequired\n\n\nnamespace\n\nnamespace to register in\nNone\n\n\nimage_name\n\noptional image name\nNone\n\n\ninclude_source_lines\n\ndisables sending sources lines of function along with udf\nTrue\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncloud.udf.update_single_array_udf(\n    func,\n    name,\n    namespace=None,\n    image_name=None,\n    include_source_lines=True,\n    async_req=False,\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\n\nfunction to register\nrequired\n\n\nname\n\nname of udf to register\nrequired\n\n\nnamespace\n\nnamespace to register in\nNone\n\n\nimage_name\n\noptional image name\nNone\n\n\ninclude_source_lines\n\ndisables sending sources lines of function along with udf\nTrue\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncloud.udf.update_udf(\n    func,\n    name,\n    namespace=None,\n    update_name=None,\n    image_name=None,\n    type=None,\n    license_id=None,\n    license_text=None,\n    readme=None,\n    include_source_lines=True,\n    async_req=False,\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\n\nfunction to update register\nrequired\n\n\nname\n\nname of udf to register\nrequired\n\n\nnamespace\n\nnamespace to register in\nNone\n\n\nupdate_name\n\nnew name for udf - physical folder will not be renamed, just the registered array name\nNone\n\n\nimage_name\n\noptional image name\nNone\n\n\ntype\n\ntype of udf, generic or single_array\nNone\n\n\nlicense_id\n\nlicense id for udf according to https://spdx.org/licenses/\nNone\n\n\nlicense_text\n\ntext of license for udf\nNone\n\n\nreadme\n\nreadme of udf\nNone\n\n\ninclude_source_lines\n\ndisables sending sources lines of function along with udf\nTrue\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription",
    "crumbs": [
      "Get Started",
      "Catalog",
      "udf"
    ]
  },
  {
    "objectID": "reference/udf.html#functions",
    "href": "reference/udf.html#functions",
    "title": "udf",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndelete\nDeletes a registered udf\n\n\nderegister\nDe-registers a registered udf, by de-registering the array that it\n\n\nexec\nRun a user defined function, synchronously, returning only the result.\n\n\nexec_async\nRun a user defined function, asynchronously.\n\n\nexec_base\nRun a user defined function, returning the result and metadata.\n\n\ninfo\nFetch info on a registered udf\n\n\nregister_generic_udf\n\n\n\nregister_multi_array_udf\n\n\n\nregister_single_array_udf\n\n\n\nregister_udf\n\n\n\nshare\nShare a registered udf\n\n\nunshare\nShare a registered udf\n\n\nupdate_generic_udf\n\n\n\nupdate_single_array_udf\n\n\n\nupdate_udf\n\n\n\n\n\n\ncloud.udf.delete(uri, *, async_req=False)\nDeletes a registered udf\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nTileDB URI of the udf, defaults to None.\nrequired\n\n\nasync_req\nbool\nReturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\ndeleted udf details\n\n\n\n\n\n\n\ncloud.udf.deregister(uri, *, async_req=False)\nDe-registers a registered udf, by de-registering the array that it is registered on. This does not physically delete the array, it will remain in your bucket. All access to the array and cloud metadata will be removed.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nTileDB URI of the array.\nrequired\n\n\nasync_req\nbool\nReturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\ncloud.udf.exec(*args, **kwargs)\nRun a user defined function, synchronously, returning only the result.\nArguments are exactly as in :func:exec_base.\n\n\n\ncloud.udf.exec_async(*args, **kwargs)\nRun a user defined function, asynchronously.\nArguments are exactly as in :func:exec_base.\n\n\n\ncloud.udf.exec_base(\n    func,\n    *args,\n    name=None,\n    namespace=None,\n    image_name='default',\n    http_compressor='deflate',\n    include_source_lines=True,\n    task_name=None,\n    result_format='tiledb_json',\n    result_format_version=None,\n    store_results=False,\n    stored_param_uuids=(),\n    timeout=None,\n    resource_class=None,\n    _download_results=True,\n    _server_graph_uuid=None,\n    _client_node_uuid=None,\n    access_credentials_name=None,\n    **kwargs,\n)\nRun a user defined function, returning the result and metadata.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\nUnion[str, Callable, Any]\nThe function to call, either as a callable function, or as the name of a registered user-defined function. (If name is set, this is instead prepended to args for backwards compatibility.)\nrequired\n\n\nargs\nAny\nThe arguments to pass to the function.\n()\n\n\nname\nOptional[str]\nDEPRECATED. If present, the name of the user-defined function to run.\nNone\n\n\nnamespace\nOptional[str]\nnamespace to run udf under\nNone\n\n\nimage_name\nstr\nudf image name to use, useful for testing beta features\n'default'\n\n\nhttp_compressor\nOptional[str]\nset http compressor for results\n'deflate'\n\n\ninclude_source_lines\nbool\nTrue to send the source code of your UDF to the server with your request. (This means it can be shown to you in stack traces if an error occurs.) False to send only compiled Python bytecode.\nTrue\n\n\ntask_name\nstr\noptional name to assign the task for logging and audit purposes\nNone\n\n\nresult_format\nResultFormat\nresult serialization format\n'tiledb_json'\n\n\nresult_format_version\nstr\nDeprecated and ignored.\nNone\n\n\nstore_results\nbool\nTrue to temporarily store results on the server side for later retrieval (in addition to downloading them).\nFalse\n\n\ntimeout\nint\nTimeout for UDF in seconds\nNone\n\n\nresource_class\nOptional[str]\nThe name of the resource class to use. Resource classes define maximum limits for cpu and memory usage.\nNone\n\n\n_download_results\nbool\nTrue to download and parse results eagerly. False to not download results by default and only do so lazily (e.g. for an intermediate node in a graph).\nTrue\n\n\n_server_graph_uuid\nOptional[uuid.UUID]\nIf this function is being executed within a DAG, the server-generated ID of the graph’s log. Otherwise, None.\nNone\n\n\n_client_node_uuid\nOptional[uuid.UUID]\nIf this function is being executed within a DAG, the ID of this function’s node within the graph. Otherwise, None.\nNone\n\n\nkwargs\n\nnamed arguments to pass to function\n{}\n\n\n\n\n\n\n\ncloud.udf.info(namespace=None, name=None, async_req=False)\nFetch info on a registered udf\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnamespace\n\nnamespace to filter to\nNone\n\n\nname\n\nname of udf to get info\nNone\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nregistered udf details\n\n\n\n\n\n\n\ncloud.udf.register_generic_udf(\n    func,\n    name,\n    namespace=None,\n    image_name=None,\n    include_source_lines=True,\n    async_req=False,\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\n\nfunction to register\nrequired\n\n\nname\n\nname of udf to register\nrequired\n\n\nnamespace\n\nnamespace to register in\nNone\n\n\nimage_name\n\noptional image name\nNone\n\n\ninclude_source_lines\n\ndisables sending sources lines of function along with udf\nTrue\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncloud.udf.register_multi_array_udf(\n    func,\n    name,\n    namespace=None,\n    image_name=None,\n    include_source_lines=True,\n    async_req=False,\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\n\nfunction to register\nrequired\n\n\nname\n\nname of udf to register\nrequired\n\n\nnamespace\n\nnamespace to register in\nNone\n\n\nimage_name\n\noptional image name\nNone\n\n\ninclude_source_lines\n\ndisables sending sources lines of function along with udf\nTrue\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncloud.udf.register_single_array_udf(\n    func,\n    name,\n    namespace=None,\n    image_name=None,\n    include_source_lines=True,\n    async_req=False,\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\n\nfunction to register\nrequired\n\n\nname\n\nname of udf to register\nrequired\n\n\nnamespace\n\nnamespace to register in\nNone\n\n\nimage_name\n\noptional image name\nNone\n\n\ninclude_source_lines\n\ndisables sending sources lines of function along with udf\nTrue\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncloud.udf.register_udf(\n    func,\n    name,\n    namespace=None,\n    image_name=None,\n    type=None,\n    include_source_lines=True,\n    async_req=False,\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\n\nfunction to register\nrequired\n\n\nname\n\nname of udf to register\nrequired\n\n\nnamespace\n\nnamespace to register in\nNone\n\n\nimage_name\n\noptional image name\nNone\n\n\ntype\n\ntype of udf, generic or single_array\nNone\n\n\ninclude_source_lines\n\ndisables sending sources lines of function along with udf\nTrue\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncloud.udf.share(name=None, namespace=None, async_req=False)\nShare a registered udf\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\n\nname of udf in “namespace/name” format\nNone\n\n\nnamespace\n\nnamespace to share array with\nNone\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nregistered udf details\n\n\n\n\n\n\n\ncloud.udf.unshare(name=None, namespace=None, async_req=False)\nShare a registered udf\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\n\nname of udf in “namespace/name” format\nNone\n\n\nnamespace\n\nnamespace to share array with\nNone\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nregistered udf details\n\n\n\n\n\n\n\ncloud.udf.update_generic_udf(\n    func,\n    name,\n    namespace=None,\n    image_name=None,\n    include_source_lines=True,\n    async_req=False,\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\n\nfunction to register\nrequired\n\n\nname\n\nname of udf to register\nrequired\n\n\nnamespace\n\nnamespace to register in\nNone\n\n\nimage_name\n\noptional image name\nNone\n\n\ninclude_source_lines\n\ndisables sending sources lines of function along with udf\nTrue\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncloud.udf.update_single_array_udf(\n    func,\n    name,\n    namespace=None,\n    image_name=None,\n    include_source_lines=True,\n    async_req=False,\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\n\nfunction to register\nrequired\n\n\nname\n\nname of udf to register\nrequired\n\n\nnamespace\n\nnamespace to register in\nNone\n\n\nimage_name\n\noptional image name\nNone\n\n\ninclude_source_lines\n\ndisables sending sources lines of function along with udf\nTrue\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncloud.udf.update_udf(\n    func,\n    name,\n    namespace=None,\n    update_name=None,\n    image_name=None,\n    type=None,\n    license_id=None,\n    license_text=None,\n    readme=None,\n    include_source_lines=True,\n    async_req=False,\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\n\nfunction to update register\nrequired\n\n\nname\n\nname of udf to register\nrequired\n\n\nnamespace\n\nnamespace to register in\nNone\n\n\nupdate_name\n\nnew name for udf - physical folder will not be renamed, just the registered array name\nNone\n\n\nimage_name\n\noptional image name\nNone\n\n\ntype\n\ntype of udf, generic or single_array\nNone\n\n\nlicense_id\n\nlicense id for udf according to https://spdx.org/licenses/\nNone\n\n\nlicense_text\n\ntext of license for udf\nNone\n\n\nreadme\n\nreadme of udf\nNone\n\n\ninclude_source_lines\n\ndisables sending sources lines of function along with udf\nTrue\n\n\nasync_req\n\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription",
    "crumbs": [
      "Get Started",
      "Catalog",
      "udf"
    ]
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "API Reference",
    "section": "",
    "text": "Find and register private and public assets.\n\n\n\ngroups\nFunctions for managing TileDB Cloud groups.\n\n\narray\nRegister, search, and manage arrays with TileDB.\n\n\nasset\nAn asset may be an array or a group.\n\n\nnotebook\nPython support for notebook I/O on Tiledb Cloud. All notebook JSON content\n\n\ndashboard\n\n\n\nudf\n\n\n\n\n\n\n\nShare private assets with other users and organizations, or make them public.\n\n\n\ngroups\nFunctions for managing TileDB Cloud groups.\n\n\narray\nRegister, search, and manage arrays with TileDB.\n\n\nasset\nAn asset may be an array or a group.\n\n\ninvites\n\n\n\n\n\n\n\nGain understanding of data.\n\n\n\n\n\nbioimg.exportation\n\n\n\nbioimg.ingestion\n\n\n\n\n\n\n\n\n\n\nfiles.indexing\n\n\n\nfiles.ingestion\n\n\n\nfiles.udfs\n\n\n\nfiles.utils\n\n\n\n\n\n\n\n\n\n\ngeospatial.ingestion\n\n\n\n\n\n\n\n\n\n\nsoma.ingest\n\n\n\nsoma.mapper\n\n\n\n\n\n\n\n\n\n\nvcf.allele_frequency\n\n\n\nvcf.ingestion\n\n\n\nvcf.query\n\n\n\nvcf.split\nSplit samples from multi-sample VCF.\n\n\nvcf.utils\n\n\n\n\n\n\n\n\nIngest and analyze data that is too large for a single computer.\n\n\n\ncloudarray\n\n\n\ncompute.delayed\n\n\n\ndag.dag\nDirected acyclic graphs as TileDB task graphs.\n\n\ndag.mode\n\n\n\ndag.visualization\n\n\n\ntasks\n\n\n\ntaskgraphs.builder\nThe code to build task graphs for later registration and execution.\n\n\ntaskgraphs.executor\nGeneric interfaces for task graph executors.\n\n\ntaskgraphs.registration\n\n\n\ntaskgraphs.types\nUser-facing types used in task graphs.\n\n\nudf\n\n\n\nutilities.consolidate\n\n\n\nutilities.profiler\n\n\n\n\n\n\n\nProfile, settings, and usage.\n\n\n\nclient\n\n\n\nconfig"
  },
  {
    "objectID": "reference/index.html#catalog",
    "href": "reference/index.html#catalog",
    "title": "API Reference",
    "section": "",
    "text": "Find and register private and public assets.\n\n\n\ngroups\nFunctions for managing TileDB Cloud groups.\n\n\narray\nRegister, search, and manage arrays with TileDB.\n\n\nasset\nAn asset may be an array or a group.\n\n\nnotebook\nPython support for notebook I/O on Tiledb Cloud. All notebook JSON content\n\n\ndashboard\n\n\n\nudf"
  },
  {
    "objectID": "reference/index.html#collaborate",
    "href": "reference/index.html#collaborate",
    "title": "API Reference",
    "section": "",
    "text": "Share private assets with other users and organizations, or make them public.\n\n\n\ngroups\nFunctions for managing TileDB Cloud groups.\n\n\narray\nRegister, search, and manage arrays with TileDB.\n\n\nasset\nAn asset may be an array or a group.\n\n\ninvites"
  },
  {
    "objectID": "reference/index.html#analyze",
    "href": "reference/index.html#analyze",
    "title": "API Reference",
    "section": "",
    "text": "Gain understanding of data.\n\n\n\n\n\nbioimg.exportation\n\n\n\nbioimg.ingestion\n\n\n\n\n\n\n\n\n\n\nfiles.indexing\n\n\n\nfiles.ingestion\n\n\n\nfiles.udfs\n\n\n\nfiles.utils\n\n\n\n\n\n\n\n\n\n\ngeospatial.ingestion\n\n\n\n\n\n\n\n\n\n\nsoma.ingest\n\n\n\nsoma.mapper\n\n\n\n\n\n\n\n\n\n\nvcf.allele_frequency\n\n\n\nvcf.ingestion\n\n\n\nvcf.query\n\n\n\nvcf.split\nSplit samples from multi-sample VCF.\n\n\nvcf.utils"
  },
  {
    "objectID": "reference/index.html#scale",
    "href": "reference/index.html#scale",
    "title": "API Reference",
    "section": "",
    "text": "Ingest and analyze data that is too large for a single computer.\n\n\n\ncloudarray\n\n\n\ncompute.delayed\n\n\n\ndag.dag\nDirected acyclic graphs as TileDB task graphs.\n\n\ndag.mode\n\n\n\ndag.visualization\n\n\n\ntasks\n\n\n\ntaskgraphs.builder\nThe code to build task graphs for later registration and execution.\n\n\ntaskgraphs.executor\nGeneric interfaces for task graph executors.\n\n\ntaskgraphs.registration\n\n\n\ntaskgraphs.types\nUser-facing types used in task graphs.\n\n\nudf\n\n\n\nutilities.consolidate\n\n\n\nutilities.profiler"
  },
  {
    "objectID": "reference/index.html#account",
    "href": "reference/index.html#account",
    "title": "API Reference",
    "section": "",
    "text": "Profile, settings, and usage.\n\n\n\nclient\n\n\n\nconfig"
  },
  {
    "objectID": "reference/files.utils.html",
    "href": "reference/files.utils.html",
    "title": "files.utils",
    "section": "",
    "text": "cloud.files.utils\n\n\n\n\n\nName\nDescription\n\n\n\n\nbasename_match\nChecks if the basename of a given file uri matches\n\n\ncreate_file\nCreates a TileDB file at the specified location\n\n\nexport_file\nExports a TileDB File back to its original file format\n\n\nexport_file_local\nExports a TileDB File back to its original file format\n\n\nsanitize_filename\nSanitizes a filename by removing invalid characters.\n\n\nupload_file\nUploads a file to TileDB Cloud.\n\n\n\n\n\ncloud.files.utils.basename_match(file_uri, pattern=None)\nChecks if the basename of a given file uri matches the given UNIX shell style pattern.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfile_uri\nstr\nA file URI.\nrequired\n\n\npattern\nOptional[str]\nA UNIX shell style pattern, defaults to None\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nbool\nPattern matches the file basename or not.\n\n\n\n\n\n\n\ncloud.files.utils.create_file(\n    namespace,\n    input_uri,\n    output_uri,\n    name=None,\n    access_credentials_name=None,\n    access_credential_name=None,\n    async_req=False,\n)\nCreates a TileDB file at the specified location\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnamespace\nstr\nnamespace the create file operation belongs to\nrequired\n\n\nname\nOptional[str]\nname to use for registration in TileDB Cloud\nNone\n\n\ninput_uri\nstr\ninput file uri\nrequired\n\n\noutput_uri\nstr\noutput array uri\nrequired\n\n\naccess_credential_name\nOptional[str]\nDEPRECATED. Use access_credential_name instead.\nNone\n\n\naccess_credentials_name\nOptional[str]\noptional access credentials to use\nNone\n\n\nasync_req\nbool\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nmodels.FileCreated\nFileCreated details, including file_uuid and output_uri\n\n\n\n\n\n\n\ncloud.files.utils.export_file(uri, output_uri, async_req=False)\nExports a TileDB File back to its original file format\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nThe tiledb://... URI of the file to export\nrequired\n\n\noutput_uri\nstr\noutput file uri\nrequired\n\n\nasync_req\nbool\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nmodels.FileExported\nFileExported details, including output_uri\n\n\n\n\n\n\n\ncloud.files.utils.export_file_local(\n    uri,\n    output_uri,\n    timestamp=None,\n    async_req=False,\n)\nExports a TileDB File back to its original file format\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nThe tiledb://... URI of the file to export\nrequired\n\n\noutput_uri\nstr\noutput file uri\nrequired\n\n\ntimestamp\ntuple\n(default None) If int, open the array at a given TileDB timestamp. If tuple, open at the given start and end TileDB timestamps.\nNone\n\n\nasync_req\nbool\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nmodels.FileExported\nFileExported details, including output_uri\n\n\n\n\n\n\n\ncloud.files.utils.sanitize_filename(fname)\nSanitizes a filename by removing invalid characters.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfname\nstr\nA filename to sanitize\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nThe sanitized string\n\n\n\n\n\n\n\ncloud.files.utils.upload_file(\n    input_uri,\n    output_uri,\n    *,\n    filename=None,\n    content_type='application/octet-stream',\n    access_credentials_name=None,\n)\nUploads a file to TileDB Cloud.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninput_uri\nstr\nThe URI or path of the input file. May be an ordinary path or any URI accessible via TileDB VFS.\nrequired\n\n\noutput_uri\nstr\nThe TileDB URI to write the file to.\nrequired\n\n\nfilename\nOptional[str]\nIf present, the value to store as the original filename.\nNone\n\n\ncontent_type\nstr\nThe MIME type of the file.\n'application/octet-stream'\n\n\naccess_credentials_name\nOptional[str]\nIf present, the name of the credentials to use when writing the uploaded file to backend storage instead of the defaults.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nThe tiledb:// URI of the uploaded file.",
    "crumbs": [
      "Get Started",
      "Analyze",
      "files.utils"
    ]
  },
  {
    "objectID": "reference/files.utils.html#functions",
    "href": "reference/files.utils.html#functions",
    "title": "files.utils",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nbasename_match\nChecks if the basename of a given file uri matches\n\n\ncreate_file\nCreates a TileDB file at the specified location\n\n\nexport_file\nExports a TileDB File back to its original file format\n\n\nexport_file_local\nExports a TileDB File back to its original file format\n\n\nsanitize_filename\nSanitizes a filename by removing invalid characters.\n\n\nupload_file\nUploads a file to TileDB Cloud.\n\n\n\n\n\ncloud.files.utils.basename_match(file_uri, pattern=None)\nChecks if the basename of a given file uri matches the given UNIX shell style pattern.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfile_uri\nstr\nA file URI.\nrequired\n\n\npattern\nOptional[str]\nA UNIX shell style pattern, defaults to None\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nbool\nPattern matches the file basename or not.\n\n\n\n\n\n\n\ncloud.files.utils.create_file(\n    namespace,\n    input_uri,\n    output_uri,\n    name=None,\n    access_credentials_name=None,\n    access_credential_name=None,\n    async_req=False,\n)\nCreates a TileDB file at the specified location\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnamespace\nstr\nnamespace the create file operation belongs to\nrequired\n\n\nname\nOptional[str]\nname to use for registration in TileDB Cloud\nNone\n\n\ninput_uri\nstr\ninput file uri\nrequired\n\n\noutput_uri\nstr\noutput array uri\nrequired\n\n\naccess_credential_name\nOptional[str]\nDEPRECATED. Use access_credential_name instead.\nNone\n\n\naccess_credentials_name\nOptional[str]\noptional access credentials to use\nNone\n\n\nasync_req\nbool\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nmodels.FileCreated\nFileCreated details, including file_uuid and output_uri\n\n\n\n\n\n\n\ncloud.files.utils.export_file(uri, output_uri, async_req=False)\nExports a TileDB File back to its original file format\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nThe tiledb://... URI of the file to export\nrequired\n\n\noutput_uri\nstr\noutput file uri\nrequired\n\n\nasync_req\nbool\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nmodels.FileExported\nFileExported details, including output_uri\n\n\n\n\n\n\n\ncloud.files.utils.export_file_local(\n    uri,\n    output_uri,\n    timestamp=None,\n    async_req=False,\n)\nExports a TileDB File back to its original file format\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nThe tiledb://... URI of the file to export\nrequired\n\n\noutput_uri\nstr\noutput file uri\nrequired\n\n\ntimestamp\ntuple\n(default None) If int, open the array at a given TileDB timestamp. If tuple, open at the given start and end TileDB timestamps.\nNone\n\n\nasync_req\nbool\nreturn future instead of results for async support\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nmodels.FileExported\nFileExported details, including output_uri\n\n\n\n\n\n\n\ncloud.files.utils.sanitize_filename(fname)\nSanitizes a filename by removing invalid characters.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfname\nstr\nA filename to sanitize\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nThe sanitized string\n\n\n\n\n\n\n\ncloud.files.utils.upload_file(\n    input_uri,\n    output_uri,\n    *,\n    filename=None,\n    content_type='application/octet-stream',\n    access_credentials_name=None,\n)\nUploads a file to TileDB Cloud.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninput_uri\nstr\nThe URI or path of the input file. May be an ordinary path or any URI accessible via TileDB VFS.\nrequired\n\n\noutput_uri\nstr\nThe TileDB URI to write the file to.\nrequired\n\n\nfilename\nOptional[str]\nIf present, the value to store as the original filename.\nNone\n\n\ncontent_type\nstr\nThe MIME type of the file.\n'application/octet-stream'\n\n\naccess_credentials_name\nOptional[str]\nIf present, the name of the credentials to use when writing the uploaded file to backend storage instead of the defaults.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nThe tiledb:// URI of the uploaded file.",
    "crumbs": [
      "Get Started",
      "Analyze",
      "files.utils"
    ]
  },
  {
    "objectID": "reference/soma.ingest.html",
    "href": "reference/soma.ingest.html",
    "title": "soma.ingest",
    "section": "",
    "text": "cloud.soma.ingest\n\n\n\n\n\nName\nDescription\n\n\n\n\ningest_h5ad\nPerforms the actual work of ingesting H5AD data into TileDB.\n\n\nregister_dataset_udf\nRegister the dataset on TileDB Cloud.\n\n\nrun_ingest_workflow\nStarts a workflow to ingest H5AD data into SOMA.\n\n\nrun_ingest_workflow_udf\nThis is the highest-level ingestor component that runs on-node. Only here\n\n\n\n\n\ncloud.soma.ingest.ingest_h5ad(\n    output_uri,\n    input_uri,\n    measurement_name,\n    extra_tiledb_config=None,\n    platform_config=None,\n    ingest_mode='write',\n    logging_level=logging.INFO,\n    dry_run=False,\n)\nPerforms the actual work of ingesting H5AD data into TileDB.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\noutput_uri\nstr\nThe output URI to write to. This will probably look like tiledb://namespace/some://storage/uri.\nrequired\n\n\ninput_uri\nstr\nThe URI of the H5AD file to read from. This file is read using TileDB VFS, so any path supported (and accessible) will work.\nrequired\n\n\nmeasurement_name\nstr\nThe name of the Measurement within the Experiment to store the data.\nrequired\n\n\nextra_tiledb_config\nOptional[Dict[str, object]]\nExtra configuration for TileDB.\nNone\n\n\nplatform_config\nOptional[Dict[str, object]]\nThe SOMA platform_config value to pass in, if any.\nNone\n\n\ningest_mode\nstr\nOne of the ingest modes supported by tiledbsoma.io.read_h5ad.\n'write'\n\n\ndry_run\nbool\nIf provided and set to True, does the input-path traversals without ingesting data.\nFalse\n\n\n\n\n\n\n\ncloud.soma.ingest.register_dataset_udf(\n    dataset_uri,\n    *,\n    register_name,\n    acn,\n    namespace=None,\n    config=None,\n    verbose=False,\n    logging_level=logging.INFO,\n)\nRegister the dataset on TileDB Cloud.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nregister_name\nstr\nname to register the dataset with on TileDB Cloud\nrequired\n\n\nnamespace\nOptional[str]\nTileDB Cloud namespace, defaults to the user’s default namespace\nNone\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\ncloud.soma.ingest.run_ingest_workflow(\n    output_uri,\n    input_uri,\n    measurement_name,\n    pattern=None,\n    extra_tiledb_config=None,\n    platform_config=None,\n    ingest_mode='write',\n    ingest_resources=None,\n    namespace=None,\n    register_name=None,\n    acn=None,\n    logging_level=logging.INFO,\n    dry_run=False,\n    **kwargs,\n)\nStarts a workflow to ingest H5AD data into SOMA.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\noutput_uri\nstr\nThe output URI to write to. This will probably look like tiledb://namespace/some://storage/uri.\nrequired\n\n\ninput_uri\nstr\nThe URI of the H5AD file(s) to read from. These are read using TileDB VFS, so any path supported (and accessible) will work. If the input_uri passes vfs.is_file, it’s ingested. If the input_uri passes vfs.is_dir, then all first-level entries are ingested . In the latter, directory case, an input file is skipped if pattern is provided and doesn’t match the input file. As well, in the directory case, each entry’s basename is appended to the output_uri to form the entry’s output URI. For example, if a.h5ad` andb.h5adare present withininput_uriofs3://bucket/h5ads/andoutput_uriistiledb://namespace/s3://bucket/somas, thentiledb://namespace/s3://bucket/somas/aandtiledb://namespace/s3://bucket/somas/bare written. | _required_ | | measurement_name    | str                             | The name of the Measurement within the Experiment to store the data.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | _required_ | | pattern             | Optional\\[str\\]                 | As described forinput_uri.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | `None`     | | extra_tiledb_config | Optional\\[Dict\\[str, object\\]\\] | Extra configuration for TileDB.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | `None`     | | platform_config     | Optional\\[Dict\\[str, object\\]\\] | The SOMAplatform_configvalue to pass in, if any.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | `None`     | | ingest_mode         | str                             | One of the ingest modes supported bytiledbsoma.io.read_h5ad.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | `'write'`  | | ingest_resources    | Optional\\[Dict\\[str, object\\]\\] | A specification for the amount of resources to provide to the UDF executing the ingestion process, to override the default.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | `None`     | | namespace           | Optional\\[str\\]                 | An alternate namespace to run the ingestion process under.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | `None`     | | register_name       | Optional\\[str\\]                 | name to register the dataset with on TileDB Cloud.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | `None`     | | acn                 | Optional\\[str\\]                 | The name of the credentials to pass to the executing UDF.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | `None`     | | dry_run             | bool                            | If provided and set toTrue`, does the input-path traversals without ingesting data.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |False`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDict[str, str]\nA dictionary of {\"status\": \"started\", \"graph_id\": ...}, with the UUID of the graph on the server side, which can be used to manage execution and monitor progress.\n\n\n\n\n\n\n\ncloud.soma.ingest.run_ingest_workflow_udf(\n    output_uri,\n    input_uri,\n    measurement_name,\n    pattern=None,\n    extra_tiledb_config=None,\n    platform_config=None,\n    ingest_mode='write',\n    ingest_resources=None,\n    namespace=None,\n    register_name=None,\n    acn=None,\n    logging_level=logging.INFO,\n    dry_run=False,\n    **kwargs,\n)\nThis is the highest-level ingestor component that runs on-node. Only here can we do VFS with access_credentials_name – that does not work correctly on the client.",
    "crumbs": [
      "Get Started",
      "Analyze",
      "soma.ingest"
    ]
  },
  {
    "objectID": "reference/soma.ingest.html#functions",
    "href": "reference/soma.ingest.html#functions",
    "title": "soma.ingest",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ningest_h5ad\nPerforms the actual work of ingesting H5AD data into TileDB.\n\n\nregister_dataset_udf\nRegister the dataset on TileDB Cloud.\n\n\nrun_ingest_workflow\nStarts a workflow to ingest H5AD data into SOMA.\n\n\nrun_ingest_workflow_udf\nThis is the highest-level ingestor component that runs on-node. Only here\n\n\n\n\n\ncloud.soma.ingest.ingest_h5ad(\n    output_uri,\n    input_uri,\n    measurement_name,\n    extra_tiledb_config=None,\n    platform_config=None,\n    ingest_mode='write',\n    logging_level=logging.INFO,\n    dry_run=False,\n)\nPerforms the actual work of ingesting H5AD data into TileDB.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\noutput_uri\nstr\nThe output URI to write to. This will probably look like tiledb://namespace/some://storage/uri.\nrequired\n\n\ninput_uri\nstr\nThe URI of the H5AD file to read from. This file is read using TileDB VFS, so any path supported (and accessible) will work.\nrequired\n\n\nmeasurement_name\nstr\nThe name of the Measurement within the Experiment to store the data.\nrequired\n\n\nextra_tiledb_config\nOptional[Dict[str, object]]\nExtra configuration for TileDB.\nNone\n\n\nplatform_config\nOptional[Dict[str, object]]\nThe SOMA platform_config value to pass in, if any.\nNone\n\n\ningest_mode\nstr\nOne of the ingest modes supported by tiledbsoma.io.read_h5ad.\n'write'\n\n\ndry_run\nbool\nIf provided and set to True, does the input-path traversals without ingesting data.\nFalse\n\n\n\n\n\n\n\ncloud.soma.ingest.register_dataset_udf(\n    dataset_uri,\n    *,\n    register_name,\n    acn,\n    namespace=None,\n    config=None,\n    verbose=False,\n    logging_level=logging.INFO,\n)\nRegister the dataset on TileDB Cloud.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nregister_name\nstr\nname to register the dataset with on TileDB Cloud\nrequired\n\n\nnamespace\nOptional[str]\nTileDB Cloud namespace, defaults to the user’s default namespace\nNone\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\ncloud.soma.ingest.run_ingest_workflow(\n    output_uri,\n    input_uri,\n    measurement_name,\n    pattern=None,\n    extra_tiledb_config=None,\n    platform_config=None,\n    ingest_mode='write',\n    ingest_resources=None,\n    namespace=None,\n    register_name=None,\n    acn=None,\n    logging_level=logging.INFO,\n    dry_run=False,\n    **kwargs,\n)\nStarts a workflow to ingest H5AD data into SOMA.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\noutput_uri\nstr\nThe output URI to write to. This will probably look like tiledb://namespace/some://storage/uri.\nrequired\n\n\ninput_uri\nstr\nThe URI of the H5AD file(s) to read from. These are read using TileDB VFS, so any path supported (and accessible) will work. If the input_uri passes vfs.is_file, it’s ingested. If the input_uri passes vfs.is_dir, then all first-level entries are ingested . In the latter, directory case, an input file is skipped if pattern is provided and doesn’t match the input file. As well, in the directory case, each entry’s basename is appended to the output_uri to form the entry’s output URI. For example, if a.h5ad` andb.h5adare present withininput_uriofs3://bucket/h5ads/andoutput_uriistiledb://namespace/s3://bucket/somas, thentiledb://namespace/s3://bucket/somas/aandtiledb://namespace/s3://bucket/somas/bare written. | _required_ | | measurement_name    | str                             | The name of the Measurement within the Experiment to store the data.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | _required_ | | pattern             | Optional\\[str\\]                 | As described forinput_uri.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | `None`     | | extra_tiledb_config | Optional\\[Dict\\[str, object\\]\\] | Extra configuration for TileDB.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | `None`     | | platform_config     | Optional\\[Dict\\[str, object\\]\\] | The SOMAplatform_configvalue to pass in, if any.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | `None`     | | ingest_mode         | str                             | One of the ingest modes supported bytiledbsoma.io.read_h5ad.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | `'write'`  | | ingest_resources    | Optional\\[Dict\\[str, object\\]\\] | A specification for the amount of resources to provide to the UDF executing the ingestion process, to override the default.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | `None`     | | namespace           | Optional\\[str\\]                 | An alternate namespace to run the ingestion process under.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | `None`     | | register_name       | Optional\\[str\\]                 | name to register the dataset with on TileDB Cloud.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | `None`     | | acn                 | Optional\\[str\\]                 | The name of the credentials to pass to the executing UDF.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | `None`     | | dry_run             | bool                            | If provided and set toTrue`, does the input-path traversals without ingesting data.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |False`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDict[str, str]\nA dictionary of {\"status\": \"started\", \"graph_id\": ...}, with the UUID of the graph on the server side, which can be used to manage execution and monitor progress.\n\n\n\n\n\n\n\ncloud.soma.ingest.run_ingest_workflow_udf(\n    output_uri,\n    input_uri,\n    measurement_name,\n    pattern=None,\n    extra_tiledb_config=None,\n    platform_config=None,\n    ingest_mode='write',\n    ingest_resources=None,\n    namespace=None,\n    register_name=None,\n    acn=None,\n    logging_level=logging.INFO,\n    dry_run=False,\n    **kwargs,\n)\nThis is the highest-level ingestor component that runs on-node. Only here can we do VFS with access_credentials_name – that does not work correctly on the client.",
    "crumbs": [
      "Get Started",
      "Analyze",
      "soma.ingest"
    ]
  },
  {
    "objectID": "reference/files.indexing.html",
    "href": "reference/files.indexing.html",
    "title": "files.indexing",
    "section": "",
    "text": "cloud.files.indexing\n\n\n\n\n\nName\nDescription\n\n\n\n\ncreate_dataset_udf\nCreate a TileDB vector search dataset.\n\n\nindex_files_udf\nIngest files into a vector search text index.\n\n\ningest_files\nIngest files into a vector search text index.\n\n\n\n\n\ncloud.files.indexing.create_dataset_udf(\n    search_uri,\n    index_uri,\n    *,\n    config=None,\n    environment_variables=None,\n    verbose=False,\n    index_type=IndexTypes.IVF_FLAT,\n    index_creation_kwargs=None,\n    pattern='*',\n    ignore=('[.]*', '*/[.]*'),\n    suffixes=None,\n    max_files=None,\n    text_splitter='RecursiveCharacterTextSplitter',\n    text_splitter_kwargs=None,\n    embedding_class='LangChainEmbedding',\n    embedding_kwargs=None,\n)\nCreate a TileDB vector search dataset.\n\n\n\ncloud.files.indexing.index_files_udf(\n    index_uri,\n    *,\n    acn=None,\n    config=None,\n    environment_variables=None,\n    openai_key=None,\n    namespace=None,\n    verbose=False,\n    trace_id=None,\n    index_timestamp=None,\n    workers=-1,\n    worker_resources=None,\n    worker_image=None,\n    extra_worker_modules=None,\n    driver_resources=None,\n    driver_image=None,\n    extra_driver_modules=None,\n    max_tasks_per_stage=-1,\n    embeddings_generation_mode=dag.Mode.BATCH,\n    embeddings_generation_driver_mode=dag.Mode.BATCH,\n    vector_indexing_mode=dag.Mode.BATCH,\n    index_update_kwargs=None,\n)\nIngest files into a vector search text index.\n\n\n\ncloud.files.indexing.ingest_files(\n    search_uri,\n    index_uri,\n    *,\n    acn=None,\n    config=None,\n    environment_variables=None,\n    namespace=None,\n    verbose=False,\n    trace_id=None,\n    index_type=IndexTypes.IVF_FLAT,\n    index_creation_kwargs=None,\n    index_dag_resources=dag.MIN_BATCH_RESOURCES,\n    include='*',\n    exclude=('[.]*', '*/[.]*'),\n    suffixes=None,\n    max_files=None,\n    text_splitter='RecursiveCharacterTextSplitter',\n    text_splitter_kwargs=None,\n    embedding_class='LangChainEmbedding',\n    embedding_kwargs=None,\n    openai_key=None,\n    index_timestamp=None,\n    workers=-1,\n    worker_resources=None,\n    worker_image=None,\n    extra_worker_modules=None,\n    driver_resources=None,\n    driver_image=None,\n    extra_driver_modules=None,\n    max_tasks_per_stage=-1,\n    embeddings_generation_mode=dag.Mode.BATCH,\n    embeddings_generation_driver_mode=dag.Mode.BATCH,\n    vector_indexing_mode=dag.Mode.BATCH,\n    index_update_kwargs=None,\n    threads='16',\n    ingest_resources=None,\n    consolidate_partition_resources=None,\n    copy_centroids_resources=None,\n    random_sample_resources=None,\n    kmeans_resources=None,\n    compute_new_centroids_resources=None,\n    assign_points_and_partial_new_centroids_resources=None,\n    write_centroids_resources=None,\n    partial_index_resources=None,\n)\nIngest files into a vector search text index.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsearch_uri\nstr\nUri to load files from. This can be a directory URI or a FileStore file URI.\nrequired\n\n\nindex_uri\nstr\nURI of the vector index to load files to.\nrequired\n\n\nacn\nOptional[str]\nAccess Credentials Name (ACN) registered in TileDB Cloud (ARN type), defaults to None.\nNone\n\n\nconfig\nOptional[dict]\nconfig dictionary, defaults to None.\nNone\n\n\nenvironment_variables\nOptional[Mapping[str, str]]\nEnvironment variables to use during ingestion.\nNone\n\n\nnamespace\nOptional[str]\nTileDB-Cloud namespace, defaults to None.\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False.\nFalse\n\n\ntrace_id\nOptional[str]\ntrace ID for logging, defaults to None. # Vector Index params\nNone\n\n\nindex_type\nIndexTypes\nVector search index type (“FLAT”, “IVF_FLAT”).\nIndexTypes.IVF_FLAT\n\n\nindex_creation_kwargs\nOptional[Dict]\nArguments to be passed to the index creation method.\nNone\n\n\nindex_dag_resources\nOptional[Mapping[str, Any]]\nIndex creation Node Specs configuration. # DirectoryTextReader params.\ndag.MIN_BATCH_RESOURCES\n\n\ninclude\nstr\nFile pattern to include relative to search_uri. By default set to include all files.\n'*'\n\n\nexclude\nOptional[Sequence[str]]\nFile patterns to exclude relative to search_uri. By default set to ignore all hidden files.\n('[.]*', '*/[.]*')\n\n\nsuffixes\nOptional[Sequence[str]]\nProvide to keep only files with these suffixes Useful when wanting to keep files with different suffixes Suffixes must include the dot, e.g. “.txt”\nNone\n\n\nmax_files\nOptional[int]\nMaximum number of files to include.\nNone\n\n\ntext_splitter_kwargs\nOptional[Dict]\nArguments for the splitter class. # Index update params.\nNone\n\n\nindex_timestamp\nOptional[int]\nTimestamp to add index updates at.\nNone\n\n\nworkers\nint\nIf embeddings_generation_mode=BATCH this is the number of distributed workers to be used.\n-1\n\n\nworker_resources\nOptional[Dict]\nIf embeddings_generation_mode=BATCH this can be used to specify the worker resources.\nNone\n\n\nworker_image\nOptional[str]\nIf embeddings_generation_mode=BATCH this can be used to specify the worker Docker image.\nNone\n\n\nextra_worker_modules\nOptional[List[str]]\nIf embeddings_generation_mode=BATCH this can be used to install extra pip package to the image.\nNone\n\n\ndriver_resources\nOptional[Dict]\nIf embeddings_generation_driver_mode=BATCH this can be used to specify the driver resources.\nNone\n\n\ndriver_image\nOptional[str]\nIf embeddings_generation_driver_mode=BATCH this can be used to specify the driver Docker image.\nNone\n\n\nextra_driver_modules\nOptional[List[str]]\nIf embeddings_generation_driver_mode=BATCH this can be used to install extra pip package to the image.\nNone\n\n\nmax_tasks_per_stage\nint\nNumber of maximum udf tasks per computation stage.\n-1\n\n\nembeddings_generation_mode\ndag.Mode\nTaskGraph execution mode for embeddings generation.\ndag.Mode.BATCH\n\n\nembeddings_generation_driver_mode\ndag.Mode\nTaskGraph execution mode for the ingestion driver.\ndag.Mode.BATCH\n\n\nvector_indexing_mode\ndag.Mode\nTaskGraph execution mode for the vector indexing.\ndag.Mode.BATCH\n\n\nindex_update_kwargs\nOptional[Dict]\nExtra arguments to pass to the index update job. These can be any of the documented tiledb.vector_search.ingest method with the exception of BATCH Embedding Resources (see next params): https://tiledb-inc.github.io/TileDB-Vector-Search/documentation/reference/ingestion.html#tiledb.vector_search.ingestion.ingest Also files_per_partition: int can be included (defaults to -1) ## Vector Search BATCH Embedding Resources ## These are only applicable if indexing update is executed in BATCH mode.\nNone\n\n\nthreads\nstr\nThreads to be used in the Nodes, defaults to 16.\n'16'\n\n\ningest_resources\nOptional[Dict]\nResources to request when performing vector ingestion.\nNone\n\n\nconsolidate_partition_resources\nOptional[Dict]\nResources to request when performing consolidation of a partition.\nNone\n\n\ncopy_centroids_resources\nOptional[Dict]\nResources to request when performing copy of centroids from input array to output array.\nNone\n\n\nrandom_sample_resources\nOptional[Dict]\nResources to request when performing random sample selection.\nNone\n\n\nkmeans_resources\nOptional[Dict]\nResources to request when performing kmeans task.\nNone\n\n\ncompute_new_centroids_resources\nOptional[Dict]\nResources to request when performing centroid computation.\nNone\n\n\nassign_points_and_partial_new_centroids_resources\nOptional[Dict]\nResources to request when performing the computation of partial centroids.\nNone\n\n\nwrite_centroids_resources\nOptional[Dict]\nResources to request when performing the write of centroids.\nNone\n\n\npartial_index_resources\nOptional[Dict]\nResources to request when performing the computation of partial indexing.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nThe resulting TaskGraph’s server UUID.",
    "crumbs": [
      "Get Started",
      "Analyze",
      "files.indexing"
    ]
  },
  {
    "objectID": "reference/files.indexing.html#functions",
    "href": "reference/files.indexing.html#functions",
    "title": "files.indexing",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncreate_dataset_udf\nCreate a TileDB vector search dataset.\n\n\nindex_files_udf\nIngest files into a vector search text index.\n\n\ningest_files\nIngest files into a vector search text index.\n\n\n\n\n\ncloud.files.indexing.create_dataset_udf(\n    search_uri,\n    index_uri,\n    *,\n    config=None,\n    environment_variables=None,\n    verbose=False,\n    index_type=IndexTypes.IVF_FLAT,\n    index_creation_kwargs=None,\n    pattern='*',\n    ignore=('[.]*', '*/[.]*'),\n    suffixes=None,\n    max_files=None,\n    text_splitter='RecursiveCharacterTextSplitter',\n    text_splitter_kwargs=None,\n    embedding_class='LangChainEmbedding',\n    embedding_kwargs=None,\n)\nCreate a TileDB vector search dataset.\n\n\n\ncloud.files.indexing.index_files_udf(\n    index_uri,\n    *,\n    acn=None,\n    config=None,\n    environment_variables=None,\n    openai_key=None,\n    namespace=None,\n    verbose=False,\n    trace_id=None,\n    index_timestamp=None,\n    workers=-1,\n    worker_resources=None,\n    worker_image=None,\n    extra_worker_modules=None,\n    driver_resources=None,\n    driver_image=None,\n    extra_driver_modules=None,\n    max_tasks_per_stage=-1,\n    embeddings_generation_mode=dag.Mode.BATCH,\n    embeddings_generation_driver_mode=dag.Mode.BATCH,\n    vector_indexing_mode=dag.Mode.BATCH,\n    index_update_kwargs=None,\n)\nIngest files into a vector search text index.\n\n\n\ncloud.files.indexing.ingest_files(\n    search_uri,\n    index_uri,\n    *,\n    acn=None,\n    config=None,\n    environment_variables=None,\n    namespace=None,\n    verbose=False,\n    trace_id=None,\n    index_type=IndexTypes.IVF_FLAT,\n    index_creation_kwargs=None,\n    index_dag_resources=dag.MIN_BATCH_RESOURCES,\n    include='*',\n    exclude=('[.]*', '*/[.]*'),\n    suffixes=None,\n    max_files=None,\n    text_splitter='RecursiveCharacterTextSplitter',\n    text_splitter_kwargs=None,\n    embedding_class='LangChainEmbedding',\n    embedding_kwargs=None,\n    openai_key=None,\n    index_timestamp=None,\n    workers=-1,\n    worker_resources=None,\n    worker_image=None,\n    extra_worker_modules=None,\n    driver_resources=None,\n    driver_image=None,\n    extra_driver_modules=None,\n    max_tasks_per_stage=-1,\n    embeddings_generation_mode=dag.Mode.BATCH,\n    embeddings_generation_driver_mode=dag.Mode.BATCH,\n    vector_indexing_mode=dag.Mode.BATCH,\n    index_update_kwargs=None,\n    threads='16',\n    ingest_resources=None,\n    consolidate_partition_resources=None,\n    copy_centroids_resources=None,\n    random_sample_resources=None,\n    kmeans_resources=None,\n    compute_new_centroids_resources=None,\n    assign_points_and_partial_new_centroids_resources=None,\n    write_centroids_resources=None,\n    partial_index_resources=None,\n)\nIngest files into a vector search text index.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsearch_uri\nstr\nUri to load files from. This can be a directory URI or a FileStore file URI.\nrequired\n\n\nindex_uri\nstr\nURI of the vector index to load files to.\nrequired\n\n\nacn\nOptional[str]\nAccess Credentials Name (ACN) registered in TileDB Cloud (ARN type), defaults to None.\nNone\n\n\nconfig\nOptional[dict]\nconfig dictionary, defaults to None.\nNone\n\n\nenvironment_variables\nOptional[Mapping[str, str]]\nEnvironment variables to use during ingestion.\nNone\n\n\nnamespace\nOptional[str]\nTileDB-Cloud namespace, defaults to None.\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False.\nFalse\n\n\ntrace_id\nOptional[str]\ntrace ID for logging, defaults to None. # Vector Index params\nNone\n\n\nindex_type\nIndexTypes\nVector search index type (“FLAT”, “IVF_FLAT”).\nIndexTypes.IVF_FLAT\n\n\nindex_creation_kwargs\nOptional[Dict]\nArguments to be passed to the index creation method.\nNone\n\n\nindex_dag_resources\nOptional[Mapping[str, Any]]\nIndex creation Node Specs configuration. # DirectoryTextReader params.\ndag.MIN_BATCH_RESOURCES\n\n\ninclude\nstr\nFile pattern to include relative to search_uri. By default set to include all files.\n'*'\n\n\nexclude\nOptional[Sequence[str]]\nFile patterns to exclude relative to search_uri. By default set to ignore all hidden files.\n('[.]*', '*/[.]*')\n\n\nsuffixes\nOptional[Sequence[str]]\nProvide to keep only files with these suffixes Useful when wanting to keep files with different suffixes Suffixes must include the dot, e.g. “.txt”\nNone\n\n\nmax_files\nOptional[int]\nMaximum number of files to include.\nNone\n\n\ntext_splitter_kwargs\nOptional[Dict]\nArguments for the splitter class. # Index update params.\nNone\n\n\nindex_timestamp\nOptional[int]\nTimestamp to add index updates at.\nNone\n\n\nworkers\nint\nIf embeddings_generation_mode=BATCH this is the number of distributed workers to be used.\n-1\n\n\nworker_resources\nOptional[Dict]\nIf embeddings_generation_mode=BATCH this can be used to specify the worker resources.\nNone\n\n\nworker_image\nOptional[str]\nIf embeddings_generation_mode=BATCH this can be used to specify the worker Docker image.\nNone\n\n\nextra_worker_modules\nOptional[List[str]]\nIf embeddings_generation_mode=BATCH this can be used to install extra pip package to the image.\nNone\n\n\ndriver_resources\nOptional[Dict]\nIf embeddings_generation_driver_mode=BATCH this can be used to specify the driver resources.\nNone\n\n\ndriver_image\nOptional[str]\nIf embeddings_generation_driver_mode=BATCH this can be used to specify the driver Docker image.\nNone\n\n\nextra_driver_modules\nOptional[List[str]]\nIf embeddings_generation_driver_mode=BATCH this can be used to install extra pip package to the image.\nNone\n\n\nmax_tasks_per_stage\nint\nNumber of maximum udf tasks per computation stage.\n-1\n\n\nembeddings_generation_mode\ndag.Mode\nTaskGraph execution mode for embeddings generation.\ndag.Mode.BATCH\n\n\nembeddings_generation_driver_mode\ndag.Mode\nTaskGraph execution mode for the ingestion driver.\ndag.Mode.BATCH\n\n\nvector_indexing_mode\ndag.Mode\nTaskGraph execution mode for the vector indexing.\ndag.Mode.BATCH\n\n\nindex_update_kwargs\nOptional[Dict]\nExtra arguments to pass to the index update job. These can be any of the documented tiledb.vector_search.ingest method with the exception of BATCH Embedding Resources (see next params): https://tiledb-inc.github.io/TileDB-Vector-Search/documentation/reference/ingestion.html#tiledb.vector_search.ingestion.ingest Also files_per_partition: int can be included (defaults to -1) ## Vector Search BATCH Embedding Resources ## These are only applicable if indexing update is executed in BATCH mode.\nNone\n\n\nthreads\nstr\nThreads to be used in the Nodes, defaults to 16.\n'16'\n\n\ningest_resources\nOptional[Dict]\nResources to request when performing vector ingestion.\nNone\n\n\nconsolidate_partition_resources\nOptional[Dict]\nResources to request when performing consolidation of a partition.\nNone\n\n\ncopy_centroids_resources\nOptional[Dict]\nResources to request when performing copy of centroids from input array to output array.\nNone\n\n\nrandom_sample_resources\nOptional[Dict]\nResources to request when performing random sample selection.\nNone\n\n\nkmeans_resources\nOptional[Dict]\nResources to request when performing kmeans task.\nNone\n\n\ncompute_new_centroids_resources\nOptional[Dict]\nResources to request when performing centroid computation.\nNone\n\n\nassign_points_and_partial_new_centroids_resources\nOptional[Dict]\nResources to request when performing the computation of partial centroids.\nNone\n\n\nwrite_centroids_resources\nOptional[Dict]\nResources to request when performing the write of centroids.\nNone\n\n\npartial_index_resources\nOptional[Dict]\nResources to request when performing the computation of partial indexing.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nThe resulting TaskGraph’s server UUID.",
    "crumbs": [
      "Get Started",
      "Analyze",
      "files.indexing"
    ]
  },
  {
    "objectID": "reference/vcf.query.html",
    "href": "reference/vcf.query.html",
    "title": "vcf.query",
    "section": "",
    "text": "cloud.vcf.query\n\n\n\n\n\nName\nDescription\n\n\n\n\nbuild_read_dag\nBuild the DAG for a distributed read on a TileDB-VCF dataset.\n\n\nconcat_tables_udf\nConcatenate a list of Arrow tables.\n\n\nread\nRun a distributed read on a TileDB-VCF dataset.\n\n\nsetup\nSet the default TileDB context, OS environment variables for AWS,\n\n\nvcf_query_udf\nRun a query on a TileDB-VCF dataset.\n\n\n\n\n\ncloud.vcf.query.build_read_dag(\n    dataset_uri,\n    *,\n    config=None,\n    attrs=None,\n    regions=None,\n    bed_file=None,\n    num_region_partitions=1,\n    dag_name='VCF-Distributed-Query',\n    max_workers=MAX_WORKERS,\n    samples=None,\n    memory_budget_mb=1024,\n    af_filter=None,\n    transform_result=None,\n    promote_null=False,\n    max_sample_batch_size=MAX_SAMPLE_BATCH_SIZE,\n    log_uri=None,\n    namespace=None,\n    resource_class=None,\n    verbose=False,\n    batch_mode=False,\n)\nBuild the DAG for a distributed read on a TileDB-VCF dataset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\nattrs\nOptional[Union[Sequence[str], str]]\nattribute names to read, defaults to None\nNone\n\n\nregions\nOptional[Union[Sequence[str], str, Delayed, DelayedArrayUDF, DelayedMultiArrayUDF, DelayedSQL]]\ngenomics regions to read, defaults to None\nNone\n\n\nbed_file\nOptional[str]\nURI of a BED file containing genomics regions to read, defaults to None\nNone\n\n\nnum_region_partitions\nint\nnumber of region partitions, defaults to 1\n1\n\n\ndag_name\nstr\nthe name of the built DAG, defaults to “VCF-Distributed-Query”,\n'VCF-Distributed-Query'\n\n\nmax_workers\nint\nmaximum number of workers, defaults to 40\nMAX_WORKERS\n\n\nsamples\nOptional[Union[Sequence[str], str, Delayed, DelayedArrayUDF, DelayedMultiArrayUDF, DelayedSQL]]\nsample names to read, defaults to None\nNone\n\n\nmemory_budget_mb\nint\nVCF memory budget in MiB, defaults to 1024\n1024\n\n\naf_filter\nOptional[str]\nallele frequency filter, defaults to None\nNone\n\n\ntransform_result\nOptional[Callable[[pa.Table], pa.Table]]\nfunction to apply to each partition; by default, does not transform the result\nNone\n\n\npromote_null\nbool\nFor all cols with null dtype, cast each as dtype of joining col when dtypes are different\nFalse\n\n\nmax_sample_batch_size\nint\nmaximum number of samples to read in a single node, defaults to 500\nMAX_SAMPLE_BATCH_SIZE\n\n\nlog_uri\nOptional[str]\nlog array URI for profiling, defaults to None\nNone\n\n\nnamespace\nOptional[str]\nTileDB-Cloud namespace, defaults to None\nNone\n\n\nresource_class\nOptional[str]\nTileDB-Cloud resource class for UDFs, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\nbatch_mode\nbool\nrun the query with batch UDFs, defaults to False\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTuple[tiledb.cloud.dag.DAG, tiledb.cloud.dag.Node]\nDAG and result Node\n\n\n\n\n\n\n\ncloud.vcf.query.concat_tables_udf(\n    tables,\n    *,\n    config=None,\n    promote_null=False,\n    log_uri=None,\n    verbose=False,\n)\nConcatenate a list of Arrow tables.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntables\nList[pa.Table]\nArrow tables\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\npromote_null\nbool\nFor all cols with null dtype, cast each as dtype of joining col when dtypes are different\nFalse\n\n\nlog_uri\nOptional[str]\nlog URI for profiling, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npa.table\nconcatenated Arrow table\n\n\n\n\n\n\n\ncloud.vcf.query.read(\n    dataset_uri,\n    *,\n    config=None,\n    attrs=None,\n    regions=None,\n    bed_file=None,\n    num_region_partitions=1,\n    dag_name='VCF-Distributed-Query',\n    max_workers=MAX_WORKERS,\n    samples=None,\n    memory_budget_mb=1024,\n    af_filter=None,\n    transform_result=None,\n    promote_null=False,\n    max_sample_batch_size=MAX_SAMPLE_BATCH_SIZE,\n    log_uri=None,\n    namespace=None,\n    resource_class=None,\n    verbose=False,\n    batch_mode=False,\n)\nRun a distributed read on a TileDB-VCF dataset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\nattrs\nOptional[Union[Sequence[str], str]]\nattribute names to read, defaults to None\nNone\n\n\nregions\nOptional[Union[Sequence[str], str, Delayed, DelayedArrayUDF, DelayedMultiArrayUDF, DelayedSQL]]\ngenomics regions to read, defaults to None\nNone\n\n\nbed_file\nOptional[str]\nURI of a BED file containing genomics regions to read, defaults to None\nNone\n\n\nnum_region_partitions\nint\nnumber of region partitions, defaults to 1\n1\n\n\ndag_name\nstr\nthe name of the read DAG, defaults to “VCF-Distributed-Query”,\n'VCF-Distributed-Query'\n\n\nmax_workers\nint\nmaximum number of workers, defaults to 40\nMAX_WORKERS\n\n\nsamples\nOptional[Union[Sequence[str], str, Delayed, DelayedArrayUDF, DelayedMultiArrayUDF, DelayedSQL]]\nsample names to read, defaults to None\nNone\n\n\nmemory_budget_mb\nint\nVCF memory budget in MiB, defaults to 1024\n1024\n\n\naf_filter\nOptional[str]\nallele frequency filter, defaults to None\nNone\n\n\ntransform_result\nOptional[Callable[[pa.Table], pa.Table]]\nfunction to apply to each partition; by default, does not transform the result\nNone\n\n\npromote_null\nbool\nFor all cols with null dtype, cast each as dtype of joining col when dtypes are different\nFalse\n\n\nmax_sample_batch_size\nint\nmaximum number of samples to read in a single node, defaults to 500\nMAX_SAMPLE_BATCH_SIZE\n\n\nlog_uri\nOptional[str]\nlog array URI for profiling, defaults to None\nNone\n\n\nnamespace\nOptional[str]\nTileDB-Cloud namespace, defaults to None\nNone\n\n\nresource_class\nOptional[str]\nTileDB-Cloud resource class for UDFs, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\nbatch_mode\nbool\nrun the query with batch UDFs, defaults to False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npa.Table\nArrow table containing the query results\n\n\n\n\n\n\n\ncloud.vcf.query.setup(config=None, verbose=False)\nSet the default TileDB context, OS environment variables for AWS, and return a logger instance.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlogging.Logger\nlogger instance\n\n\n\n\n\n\n\ncloud.vcf.query.vcf_query_udf(\n    dataset_uri,\n    *,\n    config=None,\n    attrs=None,\n    regions=None,\n    bed_file=None,\n    samples=None,\n    region_partition=None,\n    sample_partition=None,\n    memory_budget_mb=1024,\n    af_filter=None,\n    transform_result=None,\n    promote_null=False,\n    log_uri=None,\n    log_id='query',\n    verbose=False,\n)\nRun a query on a TileDB-VCF dataset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\nattrs\nOptional[Union[Sequence[str], str]]\nattribute names to read, defaults to None\nNone\n\n\nregions\nOptional[Union[Sequence[str], str, pd.DataFrame]]\ngenomics regions to read, defaults to None\nNone\n\n\nbed_file\nOptional[str]\nURI of a BED file containing genomics regions to read, defaults to None\nNone\n\n\nsamples\nOptional[Union[Sequence[str], str]]\nsample names to read, defaults to None\nNone\n\n\nregion_partition\nOptional[Tuple[int, int]]\nregion partition tuple (0-based indexed, num_partitions), defaults to None\nNone\n\n\nsample_partition\nOptional[Tuple[int, int]]\nsample partition tuple (0-based indexed, num_partitions), defaults to None\nNone\n\n\nmemory_budget_mb\nint\nVCF memory budget in MiB, defaults to 1024\n1024\n\n\naf_filter\nOptional[str]\nallele frequency filter, defaults to None\nNone\n\n\ntransform_result\nOptional[Callable[[pa.Table], pa.Table]]\nfunction to apply to the result table; by default, does not transform the result\nNone\n\n\npromote_null\nbool\nFor all cols with null dtype, cast each as dtype of joining col when dtypes are different\nFalse\n\n\nlog_uri\nOptional[str]\nlog array URI for profiling, defaults to None\nNone\n\n\nlog_id\nstr\nprofiler event ID, defaults to “query”\n'query'\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npa.table\nArrow table containing the query results",
    "crumbs": [
      "Get Started",
      "Analyze",
      "vcf.query"
    ]
  },
  {
    "objectID": "reference/vcf.query.html#functions",
    "href": "reference/vcf.query.html#functions",
    "title": "vcf.query",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nbuild_read_dag\nBuild the DAG for a distributed read on a TileDB-VCF dataset.\n\n\nconcat_tables_udf\nConcatenate a list of Arrow tables.\n\n\nread\nRun a distributed read on a TileDB-VCF dataset.\n\n\nsetup\nSet the default TileDB context, OS environment variables for AWS,\n\n\nvcf_query_udf\nRun a query on a TileDB-VCF dataset.\n\n\n\n\n\ncloud.vcf.query.build_read_dag(\n    dataset_uri,\n    *,\n    config=None,\n    attrs=None,\n    regions=None,\n    bed_file=None,\n    num_region_partitions=1,\n    dag_name='VCF-Distributed-Query',\n    max_workers=MAX_WORKERS,\n    samples=None,\n    memory_budget_mb=1024,\n    af_filter=None,\n    transform_result=None,\n    promote_null=False,\n    max_sample_batch_size=MAX_SAMPLE_BATCH_SIZE,\n    log_uri=None,\n    namespace=None,\n    resource_class=None,\n    verbose=False,\n    batch_mode=False,\n)\nBuild the DAG for a distributed read on a TileDB-VCF dataset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\nattrs\nOptional[Union[Sequence[str], str]]\nattribute names to read, defaults to None\nNone\n\n\nregions\nOptional[Union[Sequence[str], str, Delayed, DelayedArrayUDF, DelayedMultiArrayUDF, DelayedSQL]]\ngenomics regions to read, defaults to None\nNone\n\n\nbed_file\nOptional[str]\nURI of a BED file containing genomics regions to read, defaults to None\nNone\n\n\nnum_region_partitions\nint\nnumber of region partitions, defaults to 1\n1\n\n\ndag_name\nstr\nthe name of the built DAG, defaults to “VCF-Distributed-Query”,\n'VCF-Distributed-Query'\n\n\nmax_workers\nint\nmaximum number of workers, defaults to 40\nMAX_WORKERS\n\n\nsamples\nOptional[Union[Sequence[str], str, Delayed, DelayedArrayUDF, DelayedMultiArrayUDF, DelayedSQL]]\nsample names to read, defaults to None\nNone\n\n\nmemory_budget_mb\nint\nVCF memory budget in MiB, defaults to 1024\n1024\n\n\naf_filter\nOptional[str]\nallele frequency filter, defaults to None\nNone\n\n\ntransform_result\nOptional[Callable[[pa.Table], pa.Table]]\nfunction to apply to each partition; by default, does not transform the result\nNone\n\n\npromote_null\nbool\nFor all cols with null dtype, cast each as dtype of joining col when dtypes are different\nFalse\n\n\nmax_sample_batch_size\nint\nmaximum number of samples to read in a single node, defaults to 500\nMAX_SAMPLE_BATCH_SIZE\n\n\nlog_uri\nOptional[str]\nlog array URI for profiling, defaults to None\nNone\n\n\nnamespace\nOptional[str]\nTileDB-Cloud namespace, defaults to None\nNone\n\n\nresource_class\nOptional[str]\nTileDB-Cloud resource class for UDFs, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\nbatch_mode\nbool\nrun the query with batch UDFs, defaults to False\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTuple[tiledb.cloud.dag.DAG, tiledb.cloud.dag.Node]\nDAG and result Node\n\n\n\n\n\n\n\ncloud.vcf.query.concat_tables_udf(\n    tables,\n    *,\n    config=None,\n    promote_null=False,\n    log_uri=None,\n    verbose=False,\n)\nConcatenate a list of Arrow tables.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntables\nList[pa.Table]\nArrow tables\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\npromote_null\nbool\nFor all cols with null dtype, cast each as dtype of joining col when dtypes are different\nFalse\n\n\nlog_uri\nOptional[str]\nlog URI for profiling, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npa.table\nconcatenated Arrow table\n\n\n\n\n\n\n\ncloud.vcf.query.read(\n    dataset_uri,\n    *,\n    config=None,\n    attrs=None,\n    regions=None,\n    bed_file=None,\n    num_region_partitions=1,\n    dag_name='VCF-Distributed-Query',\n    max_workers=MAX_WORKERS,\n    samples=None,\n    memory_budget_mb=1024,\n    af_filter=None,\n    transform_result=None,\n    promote_null=False,\n    max_sample_batch_size=MAX_SAMPLE_BATCH_SIZE,\n    log_uri=None,\n    namespace=None,\n    resource_class=None,\n    verbose=False,\n    batch_mode=False,\n)\nRun a distributed read on a TileDB-VCF dataset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\nattrs\nOptional[Union[Sequence[str], str]]\nattribute names to read, defaults to None\nNone\n\n\nregions\nOptional[Union[Sequence[str], str, Delayed, DelayedArrayUDF, DelayedMultiArrayUDF, DelayedSQL]]\ngenomics regions to read, defaults to None\nNone\n\n\nbed_file\nOptional[str]\nURI of a BED file containing genomics regions to read, defaults to None\nNone\n\n\nnum_region_partitions\nint\nnumber of region partitions, defaults to 1\n1\n\n\ndag_name\nstr\nthe name of the read DAG, defaults to “VCF-Distributed-Query”,\n'VCF-Distributed-Query'\n\n\nmax_workers\nint\nmaximum number of workers, defaults to 40\nMAX_WORKERS\n\n\nsamples\nOptional[Union[Sequence[str], str, Delayed, DelayedArrayUDF, DelayedMultiArrayUDF, DelayedSQL]]\nsample names to read, defaults to None\nNone\n\n\nmemory_budget_mb\nint\nVCF memory budget in MiB, defaults to 1024\n1024\n\n\naf_filter\nOptional[str]\nallele frequency filter, defaults to None\nNone\n\n\ntransform_result\nOptional[Callable[[pa.Table], pa.Table]]\nfunction to apply to each partition; by default, does not transform the result\nNone\n\n\npromote_null\nbool\nFor all cols with null dtype, cast each as dtype of joining col when dtypes are different\nFalse\n\n\nmax_sample_batch_size\nint\nmaximum number of samples to read in a single node, defaults to 500\nMAX_SAMPLE_BATCH_SIZE\n\n\nlog_uri\nOptional[str]\nlog array URI for profiling, defaults to None\nNone\n\n\nnamespace\nOptional[str]\nTileDB-Cloud namespace, defaults to None\nNone\n\n\nresource_class\nOptional[str]\nTileDB-Cloud resource class for UDFs, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\nbatch_mode\nbool\nrun the query with batch UDFs, defaults to False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npa.Table\nArrow table containing the query results\n\n\n\n\n\n\n\ncloud.vcf.query.setup(config=None, verbose=False)\nSet the default TileDB context, OS environment variables for AWS, and return a logger instance.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlogging.Logger\nlogger instance\n\n\n\n\n\n\n\ncloud.vcf.query.vcf_query_udf(\n    dataset_uri,\n    *,\n    config=None,\n    attrs=None,\n    regions=None,\n    bed_file=None,\n    samples=None,\n    region_partition=None,\n    sample_partition=None,\n    memory_budget_mb=1024,\n    af_filter=None,\n    transform_result=None,\n    promote_null=False,\n    log_uri=None,\n    log_id='query',\n    verbose=False,\n)\nRun a query on a TileDB-VCF dataset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_uri\nstr\ndataset URI\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\nattrs\nOptional[Union[Sequence[str], str]]\nattribute names to read, defaults to None\nNone\n\n\nregions\nOptional[Union[Sequence[str], str, pd.DataFrame]]\ngenomics regions to read, defaults to None\nNone\n\n\nbed_file\nOptional[str]\nURI of a BED file containing genomics regions to read, defaults to None\nNone\n\n\nsamples\nOptional[Union[Sequence[str], str]]\nsample names to read, defaults to None\nNone\n\n\nregion_partition\nOptional[Tuple[int, int]]\nregion partition tuple (0-based indexed, num_partitions), defaults to None\nNone\n\n\nsample_partition\nOptional[Tuple[int, int]]\nsample partition tuple (0-based indexed, num_partitions), defaults to None\nNone\n\n\nmemory_budget_mb\nint\nVCF memory budget in MiB, defaults to 1024\n1024\n\n\naf_filter\nOptional[str]\nallele frequency filter, defaults to None\nNone\n\n\ntransform_result\nOptional[Callable[[pa.Table], pa.Table]]\nfunction to apply to the result table; by default, does not transform the result\nNone\n\n\npromote_null\nbool\nFor all cols with null dtype, cast each as dtype of joining col when dtypes are different\nFalse\n\n\nlog_uri\nOptional[str]\nlog array URI for profiling, defaults to None\nNone\n\n\nlog_id\nstr\nprofiler event ID, defaults to “query”\n'query'\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npa.table\nArrow table containing the query results",
    "crumbs": [
      "Get Started",
      "Analyze",
      "vcf.query"
    ]
  },
  {
    "objectID": "reference/vcf.split.html",
    "href": "reference/vcf.split.html",
    "title": "vcf.split",
    "section": "",
    "text": "cloud.vcf.split\nSplit samples from multi-sample VCF.\n\n\n\n\n\nName\nDescription\n\n\n\n\nls_samples\nList samples in an aggregate VCF.\n\n\nsplit_one_sample\nSplit one sample from multi-sample VCF.\n\n\nsplit_vcf\nSplit individual sample VCFs from an aggreate VCF.\n\n\n\n\n\ncloud.vcf.split.ls_samples(vcf_uri, config=None)\nList samples in an aggregate VCF.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvcf_uri\nstr\nS3 path to aggregate VCF.\nrequired\n\n\nconfig\nOptional[Mapping[str, str]]\nTileDB config params.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[str]\nSamples included in VCF.\n\n\n\n\n\n\n\ncloud.vcf.split.split_one_sample(vcf_uri, sample, output_uri, config=None)\nSplit one sample from multi-sample VCF.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvcf_uri\nstr\nURI of VCF to isolate from.\nrequired\n\n\nsample\nstr\nSample name to isolate.\nrequired\n\n\noutput_uri\nstr\nURI to deposit isolated VCF.\nrequired\n\n\nconfig\nOptional[Mapping[str, str]]\nTileDB config object.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nURI of isolated sample.\n\n\n\n\n\n\n\ncloud.vcf.split.split_vcf(\n    vcf_uri,\n    output_uri,\n    namespace,\n    acn,\n    resources={'cpu': '2', 'memory': '30Gi'},\n    compute=True,\n    verbose=False,\n    samples=None,\n    retry_count=1,\n    max_workers=100,\n    config=None,\n)\nSplit individual sample VCFs from an aggreate VCF.\nGiven an aggregate VCF file containing multiple samples, split all samples into isolated VCFs, one per sample. Alternatively, specify sample(s) to split apart from VCF if not all isolated VCFs are needed.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvcf_uri\nstr\nAggregate VCF URI.\nrequired\n\n\noutput_uri\nstr\nOutput URI to write isolated VCFs.\nrequired\n\n\nnamespace\nstr\nTileDB Cloud namespace to process task graph.\nrequired\n\n\nacn\nstr\nAccess credential friendly name to auth storage i/o.\nrequired\n\n\nresources\nMapping[str, str]\nResources applied to splitting UDF (start with default).\n{'cpu': '2', 'memory': '30Gi'}\n\n\ncompute\nbool\nWhether to execute DAG.\nTrue\n\n\nverbose\nbool\nLogging verbosity.\nFalse\n\n\nsamples\nOptional[Sequence[str]]\nIndicate a batch of sample names within vcf_uri to isolate if it is undesired to isolate all samples (default).\nNone\n\n\nretry_count\nint\nNumber of Node retries.\n1\n\n\nmax_workers\nint\nMax workers to engage simultaneously.\n100\n\n\nconfig\nOptional[Mapping[str, int]]\nTileDB configuration parameters used to configure virtual filesystem handler.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDAG\nDAG instantiated as specified.",
    "crumbs": [
      "Get Started",
      "Analyze",
      "vcf.split"
    ]
  },
  {
    "objectID": "reference/vcf.split.html#functions",
    "href": "reference/vcf.split.html#functions",
    "title": "vcf.split",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nls_samples\nList samples in an aggregate VCF.\n\n\nsplit_one_sample\nSplit one sample from multi-sample VCF.\n\n\nsplit_vcf\nSplit individual sample VCFs from an aggreate VCF.\n\n\n\n\n\ncloud.vcf.split.ls_samples(vcf_uri, config=None)\nList samples in an aggregate VCF.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvcf_uri\nstr\nS3 path to aggregate VCF.\nrequired\n\n\nconfig\nOptional[Mapping[str, str]]\nTileDB config params.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[str]\nSamples included in VCF.\n\n\n\n\n\n\n\ncloud.vcf.split.split_one_sample(vcf_uri, sample, output_uri, config=None)\nSplit one sample from multi-sample VCF.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvcf_uri\nstr\nURI of VCF to isolate from.\nrequired\n\n\nsample\nstr\nSample name to isolate.\nrequired\n\n\noutput_uri\nstr\nURI to deposit isolated VCF.\nrequired\n\n\nconfig\nOptional[Mapping[str, str]]\nTileDB config object.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nURI of isolated sample.\n\n\n\n\n\n\n\ncloud.vcf.split.split_vcf(\n    vcf_uri,\n    output_uri,\n    namespace,\n    acn,\n    resources={'cpu': '2', 'memory': '30Gi'},\n    compute=True,\n    verbose=False,\n    samples=None,\n    retry_count=1,\n    max_workers=100,\n    config=None,\n)\nSplit individual sample VCFs from an aggreate VCF.\nGiven an aggregate VCF file containing multiple samples, split all samples into isolated VCFs, one per sample. Alternatively, specify sample(s) to split apart from VCF if not all isolated VCFs are needed.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvcf_uri\nstr\nAggregate VCF URI.\nrequired\n\n\noutput_uri\nstr\nOutput URI to write isolated VCFs.\nrequired\n\n\nnamespace\nstr\nTileDB Cloud namespace to process task graph.\nrequired\n\n\nacn\nstr\nAccess credential friendly name to auth storage i/o.\nrequired\n\n\nresources\nMapping[str, str]\nResources applied to splitting UDF (start with default).\n{'cpu': '2', 'memory': '30Gi'}\n\n\ncompute\nbool\nWhether to execute DAG.\nTrue\n\n\nverbose\nbool\nLogging verbosity.\nFalse\n\n\nsamples\nOptional[Sequence[str]]\nIndicate a batch of sample names within vcf_uri to isolate if it is undesired to isolate all samples (default).\nNone\n\n\nretry_count\nint\nNumber of Node retries.\n1\n\n\nmax_workers\nint\nMax workers to engage simultaneously.\n100\n\n\nconfig\nOptional[Mapping[str, int]]\nTileDB configuration parameters used to configure virtual filesystem handler.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDAG\nDAG instantiated as specified.",
    "crumbs": [
      "Get Started",
      "Analyze",
      "vcf.split"
    ]
  },
  {
    "objectID": "reference/vcf.utils.html",
    "href": "reference/vcf.utils.html",
    "title": "vcf.utils",
    "section": "",
    "text": "cloud.vcf.utils\n\n\n\n\n\nName\nDescription\n\n\n\n\ncreate_index_file\nCreate a VCF index file in the current working directory.\n\n\nfind_index\nFind the index file for a VCF file or None if not found.\n\n\nget_record_count\nReturn the record count in a VCF file.\n\n\nget_sample_name\nReturns the sample name in a VCF file.\n\n\nis_bgzipped\nReturns True if the VCF file is bgzipped.\n\n\nsort_and_bgzip\nSort and bgzip a VCF file storing the result in the tmp space.\n\n\n\n\n\ncloud.vcf.utils.create_index_file(vcf_uri)\nCreate a VCF index file in the current working directory.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvcf_uri\nstr\nURI of the VCF file\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nindex file name\n\n\n\n\n\n\n\ncloud.vcf.utils.find_index(vcf_uri)\nFind the index file for a VCF file or None if not found.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvcf_uri\nstr\nURI of the VCF file\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nOptional[str]\nURI of the index file\n\n\n\n\n\n\n\ncloud.vcf.utils.get_record_count(vcf_uri, index_uri)\nReturn the record count in a VCF file.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvcf_uri\nstr\nURI of the VCF file\nrequired\n\n\nindex_uri\nstr\nURI of the VCF index file\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nOptional[int]\nrecord count or None if there is an error\n\n\n\n\n\n\n\ncloud.vcf.utils.get_sample_name(vcf_uri)\nReturns the sample name in a VCF file.\nIf there are multiple samples, return a comma-separated list of sample names.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvcf_uri\nstr\nURI of the VCF file\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nsample name\n\n\n\n\n\n\n\ncloud.vcf.utils.is_bgzipped(vcf_uri)\nReturns True if the VCF file is bgzipped.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvcf_uri\nstr\nURI of the VCF file\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nbool\nTrue if the VCF file is bgzipped\n\n\n\n\n\n\n\ncloud.vcf.utils.sort_and_bgzip(vcf_uri, *, tmp_space='.')\nSort and bgzip a VCF file storing the result in the tmp space.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvcf_uri\nstr\nURI of the VCF file\nrequired\n\n\ntmp_space\nstr\ntmp space URI, defaults to the current directory\n'.'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nURI of bgzipped VCF",
    "crumbs": [
      "Get Started",
      "Analyze",
      "vcf.utils"
    ]
  },
  {
    "objectID": "reference/vcf.utils.html#functions",
    "href": "reference/vcf.utils.html#functions",
    "title": "vcf.utils",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncreate_index_file\nCreate a VCF index file in the current working directory.\n\n\nfind_index\nFind the index file for a VCF file or None if not found.\n\n\nget_record_count\nReturn the record count in a VCF file.\n\n\nget_sample_name\nReturns the sample name in a VCF file.\n\n\nis_bgzipped\nReturns True if the VCF file is bgzipped.\n\n\nsort_and_bgzip\nSort and bgzip a VCF file storing the result in the tmp space.\n\n\n\n\n\ncloud.vcf.utils.create_index_file(vcf_uri)\nCreate a VCF index file in the current working directory.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvcf_uri\nstr\nURI of the VCF file\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nindex file name\n\n\n\n\n\n\n\ncloud.vcf.utils.find_index(vcf_uri)\nFind the index file for a VCF file or None if not found.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvcf_uri\nstr\nURI of the VCF file\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nOptional[str]\nURI of the index file\n\n\n\n\n\n\n\ncloud.vcf.utils.get_record_count(vcf_uri, index_uri)\nReturn the record count in a VCF file.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvcf_uri\nstr\nURI of the VCF file\nrequired\n\n\nindex_uri\nstr\nURI of the VCF index file\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nOptional[int]\nrecord count or None if there is an error\n\n\n\n\n\n\n\ncloud.vcf.utils.get_sample_name(vcf_uri)\nReturns the sample name in a VCF file.\nIf there are multiple samples, return a comma-separated list of sample names.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvcf_uri\nstr\nURI of the VCF file\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nsample name\n\n\n\n\n\n\n\ncloud.vcf.utils.is_bgzipped(vcf_uri)\nReturns True if the VCF file is bgzipped.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvcf_uri\nstr\nURI of the VCF file\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nbool\nTrue if the VCF file is bgzipped\n\n\n\n\n\n\n\ncloud.vcf.utils.sort_and_bgzip(vcf_uri, *, tmp_space='.')\nSort and bgzip a VCF file storing the result in the tmp space.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvcf_uri\nstr\nURI of the VCF file\nrequired\n\n\ntmp_space\nstr\ntmp space URI, defaults to the current directory\n'.'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nURI of bgzipped VCF",
    "crumbs": [
      "Get Started",
      "Analyze",
      "vcf.utils"
    ]
  },
  {
    "objectID": "reference/files.udfs.html",
    "href": "reference/files.udfs.html",
    "title": "files.udfs",
    "section": "",
    "text": "cloud.files.udfs\n\n\n\n\n\nName\nDescription\n\n\n\n\nchunk_udf\nFlatten and break an iterable into batches of a specified size.\n\n\nfind_uris_udf\nFind URIs matching a pattern in the search_uri path.\n\n\n\n\n\ncloud.files.udfs.chunk_udf(\n    items,\n    batch_size=None,\n    flatten_items=False,\n    verbose=False,\n)\nFlatten and break an iterable into batches of a specified size.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nitems\nSequence[_T]\nAn iterable to be split into chunks.\nrequired\n\n\nbatch_size\nOptional[int]\nResulting chunk size, defaults to None.\nNone\n\n\nflatten_items\nbool\nIf set to True, it will flatten the items iterable, defaults to False\nFalse\n\n\nverbose\nbool\nVerbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nList[List[str]]\nA list of chunks as lists.\n\n\n\n\n\n\n\ncloud.files.udfs.find_uris_udf(\n    search_uri,\n    *,\n    config=None,\n    include=None,\n    exclude=None,\n    max_files=None,\n    verbose=False,\n)\nFind URIs matching a pattern in the search_uri path.\ninclude and exclude patterns are Unix shell style (see fnmatch module).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsearch_uri\nstr\nURI to search for files\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\ninclude\nOptional[str]\ninclude pattern used in the search, defaults to None\nNone\n\n\nexclude\nOptional[str]\nexclude pattern applied to the search results, defaults to None\nNone\n\n\nmax_files\nOptional[int]\nmaximum number of URIs returned, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nSequence[str]\nlist of URIs",
    "crumbs": [
      "Get Started",
      "Analyze",
      "files.udfs"
    ]
  },
  {
    "objectID": "reference/files.udfs.html#functions",
    "href": "reference/files.udfs.html#functions",
    "title": "files.udfs",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nchunk_udf\nFlatten and break an iterable into batches of a specified size.\n\n\nfind_uris_udf\nFind URIs matching a pattern in the search_uri path.\n\n\n\n\n\ncloud.files.udfs.chunk_udf(\n    items,\n    batch_size=None,\n    flatten_items=False,\n    verbose=False,\n)\nFlatten and break an iterable into batches of a specified size.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nitems\nSequence[_T]\nAn iterable to be split into chunks.\nrequired\n\n\nbatch_size\nOptional[int]\nResulting chunk size, defaults to None.\nNone\n\n\nflatten_items\nbool\nIf set to True, it will flatten the items iterable, defaults to False\nFalse\n\n\nverbose\nbool\nVerbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nList[List[str]]\nA list of chunks as lists.\n\n\n\n\n\n\n\ncloud.files.udfs.find_uris_udf(\n    search_uri,\n    *,\n    config=None,\n    include=None,\n    exclude=None,\n    max_files=None,\n    verbose=False,\n)\nFind URIs matching a pattern in the search_uri path.\ninclude and exclude patterns are Unix shell style (see fnmatch module).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsearch_uri\nstr\nURI to search for files\nrequired\n\n\nconfig\nOptional[Mapping[str, Any]]\nconfig dictionary, defaults to None\nNone\n\n\ninclude\nOptional[str]\ninclude pattern used in the search, defaults to None\nNone\n\n\nexclude\nOptional[str]\nexclude pattern applied to the search results, defaults to None\nNone\n\n\nmax_files\nOptional[int]\nmaximum number of URIs returned, defaults to None\nNone\n\n\nverbose\nbool\nverbose logging, defaults to False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nSequence[str]\nlist of URIs",
    "crumbs": [
      "Get Started",
      "Analyze",
      "files.udfs"
    ]
  },
  {
    "objectID": "reference/utilities.profiler.html",
    "href": "reference/utilities.profiler.html",
    "title": "utilities.profiler",
    "section": "",
    "text": "cloud.utilities.profiler\n\n\n\n\n\nName\nDescription\n\n\n\n\nProfiler\nA context manager–based profiler to log events and CPU and memory usage\n\n\n\n\n\ncloud.utilities.profiler.Profiler(\n    array_uri=None,\n    group_uri=None,\n    group_member=None,\n    id=None,\n    period_sec=5,\n    trace=False,\n)\nA context manager–based profiler to log events and CPU and memory usage to a TileDB array.\nIf the trace parameter is True, CPU and memory usage will be logged to the array every period_sec seconds. This is useful for profiling jobs that are OOM killed.\nExamples:\n# Basic usage\nwith Profiler(array_uri=\"tiledb://array-uri...\"):\n    # code to profile\n\n# Write custom events\nwith Profiler(group_uri=\"tiledb://group-uri...\", group_member=\"log\") as prof:\n    # code to profile\n\n    # write custom event\n    prof.write(\"my-op\", \"my-data\", \"my-extra-data\")\n\n    # more code to profile\n\n\n\n\n\nName\nDescription\n\n\n\n\nwrite\nWrite an event to the log array.\n\n\n\n\n\ncloud.utilities.profiler.Profiler.write(op='', data='', extra='')\nWrite an event to the log array.\nWhen writing large amounts of data, store the data in the extra parameter to improve query performance when the extra data is not needed.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nop\nstr\nevent op, defaults to “”\n''\n\n\ndata\nstr\nevent data, defaults to “”\n''\n\n\nextra\nstr\nevent extra data, defaults to “”\n''\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncreate_log_array\nCreate an array to hold log events.\n\n\nwrite_log_event\nWrite an event to the log array.\n\n\n\n\n\ncloud.utilities.profiler.create_log_array(uri)\nCreate an array to hold log events.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\narray URI\nrequired\n\n\n\n\n\n\n\ncloud.utilities.profiler.write_log_event(uri, id, op='', data='', extra='')\nWrite an event to the log array.\nWhen writing large amounts of data, store the data in the extra parameter to improve query performance when the extra data is not needed.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\narray URI\nrequired\n\n\nid\nstr\nevent id\nrequired\n\n\nop\nOptional[str]\nevent operation, defaults to “”\n''\n\n\ndata\nOptional[str]\nevent data, defaults to “”\n''\n\n\nextra\nOptional[str]\nevent extra data, defaults to “”\n''",
    "crumbs": [
      "Get Started",
      "Scale",
      "utilities.profiler"
    ]
  },
  {
    "objectID": "reference/utilities.profiler.html#classes",
    "href": "reference/utilities.profiler.html#classes",
    "title": "utilities.profiler",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nProfiler\nA context manager–based profiler to log events and CPU and memory usage\n\n\n\n\n\ncloud.utilities.profiler.Profiler(\n    array_uri=None,\n    group_uri=None,\n    group_member=None,\n    id=None,\n    period_sec=5,\n    trace=False,\n)\nA context manager–based profiler to log events and CPU and memory usage to a TileDB array.\nIf the trace parameter is True, CPU and memory usage will be logged to the array every period_sec seconds. This is useful for profiling jobs that are OOM killed.\nExamples:\n# Basic usage\nwith Profiler(array_uri=\"tiledb://array-uri...\"):\n    # code to profile\n\n# Write custom events\nwith Profiler(group_uri=\"tiledb://group-uri...\", group_member=\"log\") as prof:\n    # code to profile\n\n    # write custom event\n    prof.write(\"my-op\", \"my-data\", \"my-extra-data\")\n\n    # more code to profile\n\n\n\n\n\nName\nDescription\n\n\n\n\nwrite\nWrite an event to the log array.\n\n\n\n\n\ncloud.utilities.profiler.Profiler.write(op='', data='', extra='')\nWrite an event to the log array.\nWhen writing large amounts of data, store the data in the extra parameter to improve query performance when the extra data is not needed.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nop\nstr\nevent op, defaults to “”\n''\n\n\ndata\nstr\nevent data, defaults to “”\n''\n\n\nextra\nstr\nevent extra data, defaults to “”\n''",
    "crumbs": [
      "Get Started",
      "Scale",
      "utilities.profiler"
    ]
  },
  {
    "objectID": "reference/utilities.profiler.html#functions",
    "href": "reference/utilities.profiler.html#functions",
    "title": "utilities.profiler",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncreate_log_array\nCreate an array to hold log events.\n\n\nwrite_log_event\nWrite an event to the log array.\n\n\n\n\n\ncloud.utilities.profiler.create_log_array(uri)\nCreate an array to hold log events.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\narray URI\nrequired\n\n\n\n\n\n\n\ncloud.utilities.profiler.write_log_event(uri, id, op='', data='', extra='')\nWrite an event to the log array.\nWhen writing large amounts of data, store the data in the extra parameter to improve query performance when the extra data is not needed.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\narray URI\nrequired\n\n\nid\nstr\nevent id\nrequired\n\n\nop\nOptional[str]\nevent operation, defaults to “”\n''\n\n\ndata\nOptional[str]\nevent data, defaults to “”\n''\n\n\nextra\nOptional[str]\nevent extra data, defaults to “”\n''",
    "crumbs": [
      "Get Started",
      "Scale",
      "utilities.profiler"
    ]
  }
]